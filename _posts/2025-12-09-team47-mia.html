<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Junguk Yoon">
<meta name="dcterms.date" content="2025-12-13">

<title>Membership Inference Attacks against Vision Deep Learning Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="2025-12-09-team47-mia_files/libs/clipboard/clipboard.min.js"></script>
<script src="2025-12-09-team47-mia_files/libs/quarto-html/quarto.js"></script>
<script src="2025-12-09-team47-mia_files/libs/quarto-html/popper.min.js"></script>
<script src="2025-12-09-team47-mia_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="2025-12-09-team47-mia_files/libs/quarto-html/anchor.min.js"></script>
<link href="2025-12-09-team47-mia_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2025-12-09-team47-mia_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="2025-12-09-team47-mia_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="2025-12-09-team47-mia_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="2025-12-09-team47-mia_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Membership Inference Attacks against Vision Deep Learning Models</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Junguk Yoon </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 13, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>Memberhip Inference Attacks (MIA) are privacy attacks on machine learning models meant to predict whether or nota a data point was used to train a model. This blog looks at three different MIA methods against three different types of models.</p>
</blockquote>
<!--more-->
<p>{: class=“table-of-content”} * TOC {:toc}</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>With the widespread commercialization and use of machine learning and deep learning models, privacy risks have consequently become a rising concern. A Membership Inference Attack (MIA) is a form of privacy attack performed against a model to determine if a data point was used in training a specific model. In the context of computer vision and deep learning, these attacks are primarily concerned with whether or not a specific image was included in the training dataset. The basis for most MIA methods relies on the fact that machine learning models typically overfit to the training data. This discrepency between the model’s performance on the training data and unseen data exposes the model to such a privacy attack. Therefore, a successful MIA can expose vulnerabilities of a model, such as training data memorization, overfitting, and most importantly, data leakage. However, as models become more complex, robust, and diverse, various new methods are constantly being researched to improve the performance of these MIA attacks. This report aims to explore several Membership Inference Attack methods against three different types of models. Specifically, MIAs for Convolutional Neural Networks (CNN), Contrastive Language-Image Pretraining model (CLIP), and diffusion models will be discussed.</p>
<p>Membership is defined as being a part of, or a “member” of the training data. Essentially, if an image was used in the training data, that image is a member; on the other hand, if the image was not used for training, then the image is a non-member. Futhermore, a target model is defined as the model that is being attacked, while an attack model is a binary classification model used for predicting membership.</p>
</section>
<section id="shokri-supervised-model" class="level1">
<h1>Shokri (Supervised Model)</h1>
<p>“Membership Inference Attacks Against Machine Learning Models” by Shokri et al.&nbsp;(2017) is one of the most prominent and pioneering papers in this form of privacy attacks. Shokri et al.&nbsp;(2017) proposes an approach using shadow models to predict membership of images used to train convolutional neural networks (CNN).</p>
<section id="target-model" class="level2">
<h2 class="anchored" data-anchor-id="target-model">Target Model</h2>
<p>Although the paper utilizes various datasets, including tabular datasets, to keep it relevant to the course, this report will fous on the CIFAR datasets. For both the CIFAR-10 and CIFAR-100, a standard convolutional neural network (CNN) with two convolution and max pooling layers, a fully connected layer of size 128, a softmax layer, and Tanh activation was designed as the target model.</p>
</section>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>The approach is based on the black-box assumption. This means that the attacker does not have access to any parameters of the target model, nor the actual training data itself. The only access provided is an auxiliary dataset (a dataset that has a similar statistical distribution to the real training data) and the output vector from the target model. This is crucial in cases where an attacker may only rely on a model’s API, rather than having access to the model itself, to query the output from an input image. In this specific paper, the attacker only relies on the softmax probabilities outputted from the target model, and nothing more.</p>
</section>
<section id="method" class="level2">
<h2 class="anchored" data-anchor-id="method">Method</h2>
<p>First, the auxiliiary dataset is obtained, whether this is through synthetic data or through real data. This data is assumed to be competely separate from the actual training data of the target model. The data is then bootstrapped into k different datasets. Each dataset is then split into a training and testing dataset. Using this data, k models of the same architecture as the target model are individually trained using their respective training data. These models are referred to as shadow models. After training, the ouputs from each shadow for their respective training and testing data are collected and concatenated with a corresponding label (1 if the data is a member and 0 if the data is a non-member). All the results are then split into separate datasets depending on the class label. For example, all data points with class label ‘airplane’ are combined into one dataset, all data points with class ‘bird’ are combined into another dataset, etc. For each dataset, the softmax probabilities are the features and its membership is the feature. Using this dataset, a completely new model–an attack model, is trained for each class label. The attack model takes the softmax probabilities as inputs to perform a binary classification task, determining whether the data is a member or a non-member.</p>
<p>![Shokri]({{ ‘/assets/images/team47/shokri3.png’ | relative_url }}) {: style=“width: 400px; max-width: 100%;”} <em>Fig 1. Auxiliary data is used to train Shadow Models, which are used to train an Attack Model</em> [1].</p>
<p>Finally, once the attack model is trained, the target model is queried with the data in question to obtain its softmax probabilities. These probabilities are then fed into the corresponding attack model based on the predicted class label to classify membership.</p>
<p>![Shokri]({{ ‘/assets/images/team47/shokri1.png’ | relative_url }}) {: style=“width: 400px; max-width: 100%;”} <em>Fig 2. The Attack model uses the Target model outputs to predict Membership</em> [1].</p>
<p>This attack method utilizes the over confidence of models on already seen data. Models are more likely to have higher confidence scores when predicting the class of an image they have been trained on, whereas they are more likely to be less confident in their predictions on unseen data. This discrepancy allows for a successful MIA.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>![Shokri]({{ ‘/assets/images/team47/shokri2.png’ | relative_url }}) {: style=“width: 400px; max-width: 100%;”} <em>Fig 3. Precision for both CIFAR-10 and CIFAR-100 against the training data size</em> [1].</p>
<p>Precision was chosen as the metric to evaluate the performance of the MIA. Essentially, the propotion of actual members from the total number of images predicted as members were assessed. Looking at the figure above, it is evident that the precision was in fact very high for both the CIFAR-10 and CIFAR-100 datasets. The paper also mentions that recall was nearly 1.0 for both datasets.</p>
<p>The above results also demonstrate that the attack performs better on the CIFAR-100 dataset than the CIFAR-10 dataset. Based on this, it can be inferred that more classes contribute to more leakage. Because the attack model relies on the probability vector of the target model, more classes allows the attack model to have more information about the target model. This in turn helps the attack model perform better.</p>
</section>
</section>
<section id="ko-et-al.-self-supervised" class="level1">
<h1>Ko et al.&nbsp;(Self-Supervised)</h1>
<p>“Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study” by Ko et al.&nbsp;(2023) dives into attacking self-supervised vision language models, specifically the CLIP model.</p>
<section id="target-model-1" class="level2">
<h2 class="anchored" data-anchor-id="target-model-1">Target Model</h2>
<p>The CLIP model utilizes two different encoders: one for images and one for text. Using these images, the CLIP model learns to connect matching images with the corresponding text. The model uses contrastive learning, which utilizes cosine similarity to maximize the similarity between correct image-text pairs while minimizing the similarity between incorrect image-text pairs.</p>
<p>![Ko]({{ ‘/assets/images/team47/clip.svg’ | relative_url }}) {: style=“width: 400px; max-width: 100%;”} <em>Fig 4. CLIP model</em> [1].</p>
<p>For this specific research, ViT-B/32, ViT-B/16, and ViT-L/14 are used for the LAION dataset while ResNet50, Resnet101, and ViT-B/32 are used for the CC12M dataset.</p>
</section>
<section id="assumptions-1" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-1">Assumptions</h2>
<p>Similar to the first paper, black-box access is assumed in this research as well. The attacker is able to query the model with a pair of image and text, and obtain the corresponding encoded features. It is assumed that the attacker is unaware of the model architecture, parameters, etc. The first two attack methods also assume that the attacker has no knowledge on the real training dataset.</p>
</section>
<section id="method-1" class="level2">
<h2 class="anchored" data-anchor-id="method-1">Method</h2>
<p>The paper proposes three different methods in this paper.</p>
<section id="cosine-similarity-attack-csa" class="level3">
<h3 class="anchored" data-anchor-id="cosine-similarity-attack-csa">Cosine-Similarity Attack (CSA)</h3>
<p>This attack relies on the basis that models trained with constrastive learning focus on maximizing cosine similarity. In this simple attack, the cosine similarity scores are compared using a threshold. The cosine similarity between the image and its respective text is calculated. If this score is above a certain, predefined threshold, then the data is predicted to be a member, and vice versa.</p>
</section>
<section id="augmentation-enhanced-attack-aea" class="level3">
<h3 class="anchored" data-anchor-id="augmentation-enhanced-attack-aea">Augmentation-Enhanced Attack (AEA)</h3>
<p>In this second attack, K number of transformations are performed on each input image. Then, the cosine similarity between the text and the transformed image are calculated, and the cosine similarity gap, or the difference in cosine similiarity from the original image and the text are collected. Finally, these scores are aggregated and compared to a threshold to determine membership. Similar to the previous attack, if the scores exceed the threshold, they are considered a member, and vice versa.</p>
</section>
<section id="weakly-supervised-attack-wsa" class="level3">
<h3 class="anchored" data-anchor-id="weakly-supervised-attack-wsa">Weakly Supervised Attack (WSA)</h3>
<p>The Weakly Supervised Attack is the most promising attack proposed in this paper. However, this method relies on a different assumption regarding the data compared to the last two attacks. When attacking models trained on public, internet data, it can be easily assumed that data uploaded after the publication of the model are definitely non-members. Using this knowledge, the mean and standard deviation of the cosine similarities of the model outputs on these guaranteed non-members are calculated. Then, from a very large dataset containing both non-members and members, data whose cosine similarities are greater than <span class="math inline">\(\mu_{no} + \lambda \sigma_{no}\)</span>, where <span class="math inline">\(\mu_{no}\)</span> is the mean of the non-member data and <span class="math inline">\(\sigma_{no}\)</span> is the standard deviation of the non-member data, are collected as pseudo-members. It is assumed that image-text pairs with a much higher cosine similarity are outliers, and therefore can be excluded from being a non-member. The encoded features for the images and texts for both psuedo-members and non-members, along with their corresponding binary labels (1 for member and 0 for non-member) are used to create an attack dataset. An attack model, similar to Shokri et al., is then trained using this data to predict binary membership.</p>
<p>![Ko]({{ ‘/assets/images/team47/ko1.png’ | relative_url }}) {: style=“width: 400px; max-width: 100%;”} <em>Fig 5. An Overview of the proposed Weakly Supervised Attack</em> [1].</p>
</section>
</section>
<section id="results-1" class="level2">
<h2 class="anchored" data-anchor-id="results-1">Results</h2>
<p>From Table 1, we find that simple CSA already shows relatively high AUC scores (around 76% on LAION and 68% on CC12M). In addition, AEA consistently demon- strates slightly improved performance across all settings by 0.74% to 7%. While CSA and AEA demonstrate rel- atively high average performance, they fail in the low false- positive regime: the true positive rate is below 10% at the false positive rate of 1%. In contrast, WSA surpasses the performance of CSA and AEA in terms of all eval- uation metrics by a significant margin. For instance, on the LAION dataset, WSA achieves an AUC of 93.2% and TPR@1%FPR of 74.01%, improving over CSA by 17% in terms of AUC and 64% in terms of TPR@1%FPR. Fig- ure 2 illustrates examples that CSA fails to attack yet WSA succeeds. The effectiveness of WSA shows that there is a strong privacy implication of the one-sided non-member in- formation available online. Overall, our results indicate a well-generalized multi-modal model trained with a large- scale dataset can still compromise privacy.</p>
<p>The table below shows the AUC for each attack method on each model for the LAION dataset:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ViT-B/32</th>
<th style="text-align: right;">ViT-B/16</th>
<th style="text-align: right;">ViT-L/4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CSA</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: right;">0.76</td>
<td style="text-align: right;">0.79</td>
</tr>
<tr class="even">
<td style="text-align: left;">AEA</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: right;">0.79</td>
<td style="text-align: right;">0.80</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WSA</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: right;">0.93</td>
<td style="text-align: right;">0.94</td>
</tr>
</tbody>
</table>
<p>WSA trains an attack model, which can be regarded as a more complex transformation, thereby more successfully extracting the membership-indicative informa- tion from the image-text features of the target point. As shown in Figure 4, it is difficult to achieve a clear separation between members and non-members based on the CS score (the metric of CSA) or the aggregate CS score (the metric of AEA). Yet, they are separable in some latent space as shown in Figure 4 (c) which projects the member and non-member features onto a two-dimensional embedding space found by t-SNE [34]. The efficacy of WSA is primarily attributed to its active discovery of a latent space where member and non-member samples can be effectively separated.</p>
</section>
</section>
<section id="li-et-al.-diffusion" class="level1">
<h1>Li et al.&nbsp;(Diffusion)</h1>
<p>“Towards Black-Box Membership Inference Attack for Diffusion Models” proposes a novel method for performing MIAs on various forms of diffusion models. In particular, the proposed method is compatible with just black-box access by not relying on the U-net architecture like other pre-existing approaches.</p>
<section id="target-model-2" class="level2">
<h2 class="anchored" data-anchor-id="target-model-2">Target Model</h2>
<p>This paper chooses to attack several different variations of diffusion models. In particular, the DDIM model, diffusion transformer model, stable diffusion model are tested. In addition to these three models, the DALL-E 2 is also attacked using its API.</p>
</section>
<section id="assumptions-2" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-2">Assumptions</h2>
<p>This research also assumes black-box access. The attack proposed relies only on the target model’s API and does not have access to any internal structure or parameters.</p>
</section>
<section id="method-2" class="level2">
<h2 class="anchored" data-anchor-id="method-2">Method</h2>
<p>This attack method is designed on the basis that images used in training are more likely to undergo minimal changes when inputted through the diffusion process, while unseen, non-member images are more likely to undergo larger changes.</p>
<p>For every image, the attacker inputs the image to the selected model and repeats this several times. Each time, the model takes in the same input image, adds noise to the image, then denoises the image, and finally outputs a newly generated image. The attacker then takes the newly generated images (of the same input image) and averages the images together. The difference between this average and the original image is calculated, and if this difference is below a certain threshold, the input image is determined to be a member.</p>
<p>![Li]({{ ‘/assets/images/team47/li.png’ | relative_url }}) {: style=“width: 400px; max-width: 100%;”} <em>Fig 6. An Overview of the proposed ReDiffuse algorithm</em> [1].</p>
</section>
<section id="results-2" class="level2">
<h2 class="anchored" data-anchor-id="results-2">Results</h2>
<p>The results for each model is shown below:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 23%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">DDIM</th>
<th style="text-align: right;">Diffusion Transformer</th>
<th style="text-align: right;">Stable Diffusion</th>
<th style="text-align: right;">DALLE-2 (with L1 distance)</th>
<th style="text-align: right;">DALLE-2 (with L2 distance)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">AUC</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: right;">0.98</td>
<td style="text-align: right;">0.81</td>
<td style="text-align: right;">76.2</td>
<td style="text-align: right;">88.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">ASR</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: right;">0.95</td>
<td style="text-align: right;">0.75</td>
<td style="text-align: right;">74.5</td>
<td style="text-align: right;">81.4</td>
</tr>
</tbody>
</table>
<p>As shown above, the proposed attack demonstrates very high performance for all the diffusion models, validating an effective MIA. The attack’s performance on the DALLE-2 provides important insight that even popular, commercialized models can be vulnerable to privacy attacks and data leakage.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>This blog explored three different methods for performing MIA against three different models. This demonstrates how custom methods for attacking different types of models are necessary for good performance. Overall, it is evident that the proposed methods are all very successful in performing strong attacks against their respective target models. These results leave great concerns regarding privacy risks in vision deep learning models, along with great potential for further research into defenses against such privacy attacks.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>