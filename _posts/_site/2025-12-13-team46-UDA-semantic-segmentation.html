<h2 id="unsupervised-domain-adaptation-for-semantic-segmentation">Unsupervised Domain Adaptation for Semantic Segmentation</h2>
<p><em>{date} by Brandon Wu</em></p>

<p>.</p>

<hr />
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#unsupervised-domain-adaptation-uda">Unsupervised Domain Adaptation (UDA)</a></li>
  <li><a href="#approaches">Approaches</a>
    <ul>
      <li><a href="#daformer">DAFormer</a></li>
      <li><a href="#hrda">HRDA</a></li>
      <li><a href="#mic">MIC</a></li>
    </ul>
  </li>
  <li><a href="#evaluation">Evaluation</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#references">References</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>Semantic segmentation is the computer vision task of assigning a class label to every pixel in an image in the pursuit of understanding the image’s content with pixel-level precision. Due to its impact in popular research fields such as autonomous vehicles and medical imaging, it has been a prominent area of study within deep learning. The introduction of CNNs a decade ago allowed major breakthroughs in semantic segmentation, and in recent years, after revolutioning natural language processing, Transformers have quickly adapted to many computer vision tasks such as this one. However, Transformers were not so quick to supplant CNNs in Unsupervised Domain Adaptation (UDA) approaches to semantic segmentation. It was only three years ago when such an attempt was systematically made. We will explore this novel architecture (DAFormer) and its influence on UDA performance of semantic segmentation, as well as state-of-the-art upgrades (HRDA and MIC).</p>

<h2 id="unsupervised-domain-adaptation-uda">Unsupervised Domain Adaptation (UDA)</h2>
<p>UDA is a learning paradigm that addresses the challenge of domain shift, a phenomenon where a model overfits on a labeled source domain and performs poorly on a different, unlabled, target domain. In the context of semantic segmentation, the motivation for UDA arises because in many settings, pixel-level annotations for a target dataset can be expensive to obtain or even unavailable. UDA is necessary to transfer knowledge from the source dataset, which is much cheaper to label, to the target dataset. Common domain gaps include lighting, textures, resolution, and small objects. UDA methods primarily focus on learning domain-invariate representations that capture semantic similarities in both domains, enabling models to generalize well to novel conditions and environments. Ultimately, progression in this field reduces the need for expensive, manual annotations of the target domain.</p>

<h1 id="approaches">Approaches</h1>
<h2 id="daformer">DAFormer</h2>

<h3 id="architecture">Architecture</h3>
<p>Previous UDA methods mostly used CNN network architectures, which were beginning to become outdated. Researchers hypothesized that the dynamic self-attention mechanism in Transformers would improve robustness and produce more adaptive models. In particular, the design of Mix Transformers (MiT) was tailored for semantic segmentation, and was chosen for DAFormer. MiT uses smaller image patches than Vision Transformers (ViT) to capture finer details. The self-attention blocks use sequence reduction to compensate the increased feature resolution. Additionally, MiT has an encoder-decoder architecture. The encoder produces multi-level feature maps defined as</p>

\[F_i \in \R^{\frac{H}{2^{i+1}} \times \frac{W}{2^{i+1}} \times C_i},  \hspace{1mm} i \in \{1,2,3,4\}\]

<p>The decoder, in addition to exploiting local information from bottleneck features like a typical decoder, also uses context across features from different encoder levels. The earlier features provide insightful low-level understandings for semantic segmentation at a high resolution. First, each feature map $F_i$ is aligned to the same number of channels and to the same size of $F_1$, and then they are stacked together. The context-aware feature fusion then fuses the respective outputs from applying multiple parallel $3 \times 3$ depthwise separable convolutions with varying dilation rates on the stack.</p>

<h3 id="self-training-for-uda">Self-Training for UDA</h3>

<p>Due to the lack of labels for images in the target domain, naively training a neural network with categorial cross-entropy loss on the source domain causes the network to generalize poorly on the target domain. Self-training has been the most effective strategy for addressing this domain gap.</p>

<p>Self-training uses a teacher network $h_\phi$ to generate pseudo-labels for target domain data. The pseudo-labels are defined as 
\(p_T^{(i,j,c)} = [c = \argmax_{c^\prime} h_\phi(x_T^{(i)})^{(j,c^\prime)} ]\)
where $[\cdot]$ denotes the Iverson bracket.</p>

<p>Additionally. the pseudo-labels are weighted with a confidence estimate defined as 
\(q_T^{(i)} = \frac{\sum_{j=1}^{H \times W} [\max_{c^\prime} h_\phi(x_T^{(i)})^{(j,c^\prime)} &gt; \tau]} {H \cdot W}\)</p>

<p>where $\tau$ is a threshold of the maximum softmax probability.</p>

<p>These two quantities are used to build a new loss function for the network $g_\theta$ that can train on the target domain:</p>

\[\mathcal{L}_T^{(i)} = - \sum^{H \times W}_{j=1} \sum^C_{c=1} q_T^{(i)} p_T^{(i,j,c)} \log g_\theta(x_T^{(i)})^{(j,c)}\]

<p>The weights of the teacher network ($\phi$) are updated based on the weights of the student network ($\theta$) after each training step t as follows:</p>

\[\phi_{t+1} \leftarrow \alpha \phi_t + (1-\alpha)\theta_t\]

<h3 id="additional-training-strategies">Additional Training Strategies</h3>

<h4 id="rare-class-sampling-rcs">Rare Class Sampling (RCS)</h4>

<p>It was observed that rarer classes in the source domain performed inconsistently over different runs. The order in which the data was sampled decided when the classes were learned, and classes that were learned later were more likey to perform worse. To prevent the model from being overly biased towards common classes in later training iterations, rare class sampling samples images with rare classes from the source domain more often so that these classes may be learned better and earlier.</p>

<h4 id="thing-class-imagenet-feature-distance-fd">Thing-Class ImageNet Feature Distance (FD)</h4>
<p>DAFormer is pretrained on the ImageNet-1K classification dataset, which provides information on high-level semantic classes (bus vs. train) that often pose a challenge to UDA. It was observed that such classes were segmentable by DAFormer early on during training, but were forgotten after a few hundred training steps. The learned synthetic source data features began to override the ImageNet features. To preserve these generic features, the model is regularized with the feature distance between bottleneck features of the segmentation model and the bottleneck features of the ImageNet model, but for only thing-classes (well-defined objects) which comprised most of the ImageNet data.</p>

<h4 id="learning-rate-warmup">Learning Rate Warmup</h4>

<p>Warming up the learning rate has commonly been applied to models to improve network generalization by stabilizing initial training. For UDA purposes, a learning rate warmup would prevent distortion of importatn ImageNet features early on in training. Up to iteration $t_{warm}$, the learning rate at iteration t is defined as
\(\eta_t = \eta_{base} \cdot t/t_{warm}\)</p>

<h2 id="hrda">HRDA</h2>

<p>UDA methods require images from multiple domains, additional networks, and more loss functions to train, so due to GPU memory constraints, most previous works, including DAFormer, only use low resolution (LR) inputs. Consequently, excluding high-/multi-resolution (HR) inputs hinders a model’s ability to recognize smaller objects and produce fine segmentation borders. Context-aware high-resolution domain-adaptive semantic segmentation (HRDA) addresses this issue by introducing a large LR context crop and a small HR detail crop. The context crop exploits the benefits of LR by learning long-range context relations while the detail crop focuses on HR to recognize small objects and produce fine segmentation details.</p>

<h3 id="context-crop">Context Crop</h3>

<p>Beginning with an HR image $x_{HR} \in \R^{H \times W \times 3}$, it is cropped such that</p>

\[x_{c, HR} = x_{HR} = [b_{c,1} : b_{c,2}, b_{c,3} : b_{c,4}]\]

<p>where $b_c$ is a bounding box that is randomly sampled from a discrete uniform distribution bounded by the size of the image, or more specifically</p>

\[b_{c,1} \sim \mathcal{U}\{0, (H - sh_c) / k\} \cdot k, \hspace{3mm} b_{c,2} = b_{c,1} + sh_c\]

\[b_{c,3} \sim \mathcal{U}\{0, (W - sw_c) / k\} \cdot k, \hspace{3mm} b_{c,4} = b_{c,3} + sw_c\]

<p>$k$ is defined as $k = o \cdot s$ where $o$ is the output stride of the segmentation network while $s$ is the factor used to biilinear downsample $x_{c, HR}$ to obtain the context crop $x_c \in \R^{h_c \times w_c \times 3}$.</p>

\[x_c = \zeta(x_{c, HR}, 1/s)\]

<p>The intuition behind the choice of $k$ is to ensure exact alignement later on in the network because the coordinates of the bounding box $b_c$ are defined to be divisible by $k$.</p>

<h3 id="detail-crop">Detail Crop</h3>

<p>The context crop is then used to generate the detail crop $x_d \in \R^{h_d \times h_w \times 3}$. It has a similar definition, but without downsampling:</p>

<p>\(x_d = x_{c, HR} = [b_{d,1} : b_{d,2}, b_{d,3} : b_{d,4}]\)
\(b_{d,1} \sim \mathcal{U}\{0, (sh_c - h_d) / k\} \cdot k, \hspace{3mm} b_{d,2} = b_{d,1} + sh_d\)</p>

\[b_{d,3} \sim \mathcal{U}\{0, (sw_c - w_d) / k\} \cdot k, \hspace{3mm} b_{d,4} = b_{d,3} + sw_d\]

<p>To balance resources between both crops and have equal tradeoffs between context-aware and detailed predictions, the researchers set $h_c = h_d$ and $w_c = w_d$. By using a downscale factor of $s=2$, the context crop essentially comprises 4 times more content than the detail crop.</p>

<h3 id="multi-resolution-fusion">Multi-Resolution Fusion</h3>
<p>As mentioned earlier, the HR detail crop and and LR context crop have different strengths and address the other’s weaknesses. To maximize the perfomances of both, their predictions are fused using a learned scale attention which essentially weighs the trustworthiness of their predictions.</p>

<p>Following the architecture of DAFormer, HRDA uses a feature encoder $f^E$ and a semantic decoder $f^S$, which are shared for HR and LR inputs. Hence we have the context semantic segmentation</p>

\[\hat y_c  = f^S(f^E(x_c)) \in \R^{\frac{h_c}{o} \times \frac{w_c}{o} \times C}\]

<p>and the detail semantic segmentation</p>

\[\hat y_d  = f^S(f^E(x_d)) \in \R^{\frac{h_d}{o} \times \frac{w_d}{o} \times C}\]

<p>To predict the scale attention $a_c$, a scale attention decoder $f^A$ is used such that</p>

\[a_c = \sigma(f^A(f^E(x_c))) \in [0,1]^{\frac{h_c}{o} \times \frac{w_c}{o} \times C}\]

<p>The attention is predicted on the context crop,and the sigmoid is used since the scaled attention represents a ratio of focus, where a value closer to 1 means on a focus on the HR detail crop and a value closer to 0 means a focus on the LR context crop. Since the detail crop exists within the context crop, only $a_c$ values within the bounds of the detail crop are kept, and denoted as $a_c’$.</p>

<p>For fusion purposes, the detail crop must be aligned with the context crop by padding it with zeros, and the results is denoted as $y_d’$.</p>

<p>Then the fused multi-scale prediction is constructed as follows:</p>

\[\hat y_{c, F} = \zeta((1-a_c') \odot \hat y_c,s) + \zeta(a_c',s) \odot \hat y_d'\]

<p>With the ground truth $y_{c, HR}^S$ / $ y_d^S$ for the source domain, the encoder, segmentation head, and attention head are trained with the loss function</p>

\[\mathcal{L}^S_{HRDA} = (1-\lambda_d) \mathcal{L_{ce}} (\hat y_{c,F}^S, y_{c,HR}^S, 1)  + \lambda_d \mathcal{L_{ce}}(\hat y_d^S, y_d^S, 1)\]

<h2 id="mic">MIC</h2>

<p>In DAFormer, UDA is addressed using the standard self-training method, in which a teacher network generates pseudo-labels for an unlabaled target domain. However, due to the lack of ground truth labels in the target domain, self-supervised losses are too weak to properly exploit the learning of context relations on the target domain. Consequently, many classes with a similar visual appearance on the target domain are often confused with one another. Masked Image Consistency (MIC), which aims to facilitate learning of context relations on the target domain, is a plug-and-play module that can be applied to existing UDA methods.</p>

<p>MIC randomly masks out patches of the target image as follows:</p>

\[\mathcal{M}_{mb+1:(m+1)b, nb+1:(n+1)b} = [v &gt; r], \hspace{4mm} v \sim \mathcal{U}(0,1)\]

<p>with $[\cdot]$ denoting the Iverson bracket, $b$ as the patch size, $r$ as the mask ratio, and $m, n$ as the patch indices.</p>

<p>With this mask, the masked target image is constructed as</p>

\[X^M = \mathcal{M} \odot x^T\]

<p>where $x^T$ are images from the target domain.</p>

<p>Hence, the prediction of the semantic segmentation network $f_\theta$ on the masked target image is</p>

\[\hat y^M = f_\theta(x^M)\]

<p>The masking process eliminates most local information, making the prediction more difficult and forces the network to use the remaining context clues to segment the masked image. The network is trained on an MIC loss defined as</p>

\[\mathcal{L}^M = q^T\mathcal{L_{ce}}(\hat y^M, p^T)\]

<p>where $p^T$ and $q^T$ are the pseudo-labels and their confidence estimates as described previously for DAFormer.</p>

<h1 id="evaluation">Evaluation</h1>
<p>DAFormer is trained on the Cityscapes street scene dataset for the target domain, and on GTA and Synthia datasets for the source domain. It significantly ourperforms previous methods, achieving 68.3 mIOU on GTA $\rightarrow$ Cityscapes and 60.9 mIOU on Synthia $\rightarrow$ Cityscapes, a 10.8 mIOU and 5.4 mIOU improvement in state-of-the-art performance, respectively.</p>

<p>With the HRDA modification to the DAFormer approach, the resulting model achieves 73.8 mIOU on GTA $\rightarrow$ Cityscapes and 65.8 mIOU on Synthia $\rightarrow$ Cityscapes, a 5.5 mIOU and 4.9 mIOU improvement from before, respectively.</p>

<p>With both the HRDA and MIC additions to the DAFormer approach, the resulting model achieves 75.9 mIOU on GTA $\rightarrow$ Cityscapes and 67.3 mIOU on Synthia $\rightarrow$ Cityscapes, a 2.1 mIOU and 1.5 mIOU improvement from before, respectively.</p>

<p>Additionally, more tests were conducted on different domain transfer pairs. The tests above are synthetic-to-real adaptations, and the researchers also studied clear-to-adverse weather (Cityscapes $\rightarrow$ DarkZurich) and day-to-nighttime adaptations (Cityscapes $\rightarrow$ ACDC). Similarly, DAFormer with HRDA and MIC was the best-performing approach, raising state-of-art-performance by 4.3 mIOU on Cityscapes $\rightarrow$ DarkZurich and 2.4 mIOU on Cityscapes $\rightarrow$ ACDC.</p>

<h1 id="conclusion">Conclusion</h1>

<p>DAFormer shows that Transformer-based architectures, when combined with effective self-training and stabilization methods, sigficantly advances UDA for semantic segmentation. HRDA navigates around the GPU memory bottleneck for UDA methods and allows use of HR inputs for capturing fine details, while MIC strengthens contexual learning on unlabeled target images. Together, these methods achieve state-of-the-art results across multiple domain shifts, and further reinforce UDA as a solution towards costly annotations as well as a means of making semantic segmentation models more robust and adaptable to real-world environments.</p>

<h2 id="basic-syntax">Basic Syntax</h2>
<h3 id="image">Image</h3>
<p>Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.</p>

<p style="width: 400px; max-width: 100%;">You can add an image to your survey like this:
<img src="/assets/images/UCLAdeepvision/object_detection.png" alt="YOLO" /></p>
<p><em>Fig 1. YOLO: An object detection method in computer vision</em> [1].</p>

<p>Please cite the image if it is taken from other people’s work.</p>

<h3 id="table">Table</h3>
<p>Here is an example for creating tables, including alignment syntax.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">column 1</th>
      <th style="text-align: right">column 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">row1</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
    <tr>
      <td style="text-align: left">row2</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
  </tbody>
</table>

<h3 id="more-markdown-syntax">More Markdown Syntax</h3>
<p>You can find more Markdown syntax at <a href="https://www.markdownguide.org/basic-syntax/">this page</a>.</p>

<h2 id="references">References</h2>

<p>[1] Hoyer, Lukas et al. “DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 9914-9925.</p>

<p>[2] Hoyer, Lukas et al. “HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation.” ArXiv abs/2204.13132 (2022): n. pag.</p>

<p>[3] Hoyer, Lukas et al. “MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation.” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 11721-11732.</p>

<hr />
