<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/CS163-Projects-2025Fall/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/CS163-Projects-2025Fall/" rel="alternate" type="text/html" /><updated>2025-12-13T18:23:42-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/feed.xml</id><title type="html">2025F, UCLA CS163 Course Projects</title><subtitle>Course projects for UCLA CS163, Deep Learning in Compuver Vision</subtitle><author><name>UCLAdeepvision</name></author><entry><title type="html">Project Track Project 44 - Streetview Semantics</title><link href="http://localhost:4000/CS163-Projects-2025Fall/2025/12/13/team44-streetview-semantics.html" rel="alternate" type="text/html" title="Project Track Project 44 - Streetview Semantics" /><published>2025-12-13T00:00:00-08:00</published><updated>2025-12-13T00:00:00-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/2025/12/13/team44-streetview-semantics</id><content type="html" xml:base="http://localhost:4000/CS163-Projects-2025Fall/2025/12/13/team44-streetview-semantics.html"><![CDATA[<blockquote>
  <p>Street-level semantic segmentation is a core capability for autonomous driving systems, yet performance is often dominated by severe class imbalance where large categories such as roads and skies overwhelm safety-critical but rare classes like bikes, motorcycles, and poles. Using the BDD100K dataset, this study systematically examines how architectural choices, loss design, and training strategies affect segmentation quality beyond misleading pixel-level accuracy. Starting from a DeepLabV3-ResNet50 baseline, we demonstrate that high pixel accuracy (~ 94%) can coincide with extremely poor mIoU (~ 4%) under imbalance. We then introduce class-weighted and combined Dice-Cross-Entropy/Focal losses, auxiliary supervision, differential learning rates, and gradient clipping, achieving a 10x improvement in mIoU. Finally, we propose a targeted optimization strategy that remaps the task to six safety-critical small classes and leverages higher resolution, aggressive augmentation, and boosted class weights for thin and small objects. This approach significantly improves IoU for bicycles, motorcycles, and poles, highlighting practical trade-offs between accuracy, resolution, and computational cost. However, such increases in resolution resulted in significant increases to training time per epoch, resulting in less training. Overall, the work provides an empirically grounded blueprint for addressing class imbalance and small-object segmentation in urban scene understanding.</p>
</blockquote>

<ul>
  <li><a href="#streetview-semantics">Streetview Semantics</a>
    <ul>
      <li><a href="#what-is-streetview-semantics">What is streetview semantics?</a></li>
      <li><a href="#dataset">Dataset</a></li>
      <li><a href="#initial-baseline-implementation">Initial baseline implementation</a></li>
      <li><a href="#improvements-atop-baseline-model">Improvements atop Baseline Model</a></li>
      <li><a href="#further-model-development">Further Model Development</a></li>
      <li><a href="#model-development-versions">Model Development Versions</a></li>
      <li><a href="#future-uses">Future uses</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </li>
</ul>

<h1 id="streetview-semantics">Streetview Semantics</h1>

<h2 id="what-is-streetview-semantics">What is streetview semantics?</h2>

<p>Streetview semantics involves assigning meaningful labels such as <em>road</em>, <em>sidewalk</em>, <em>building</em>, <em>vehicle</em>, <em>pedestrian</em>, or <em>sky</em> to pixels or regions in images captured from a street-level perspective.</p>

<p>Semantic segmentation of streetview images can be used to transform raw visual input into a structured representation, where each region corresponds to an object or surface with a specific functional role. This semantic understanding enables higher-level reasoning about the environment, such as identifying drivable areas, detecting obstacles, and interpreting relationships between objects in down stream tasks, like autonomus driving.</p>

<h2 id="dataset">Dataset</h2>

<p>The BDD100K dataset [2] includes a diverse collection of streetview images designed for autonomus driving research. The dataset includes images captured by a camera mounted on a vehicle from a variety of locations like New York and San Francisco.</p>

<p>We use 10,000 images and their semantically segmented labeled masks to train and test our model.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/44/bddsample.png" alt="A sample image and mask" />
<em>A sample image and its corresponding mask from the dataset.</em></p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Pixel Count</th>
      <th>Percentage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Road</td>
      <td>14,112,633</td>
      <td>26.43%</td>
    </tr>
    <tr>
      <td>Sidewalk</td>
      <td>1,072,369</td>
      <td>2.01%</td>
    </tr>
    <tr>
      <td>Building</td>
      <td>8,773,136</td>
      <td>16.43%</td>
    </tr>
    <tr>
      <td>Wall</td>
      <td>300,296</td>
      <td>0.56%</td>
    </tr>
    <tr>
      <td>Fence</td>
      <td>520,512</td>
      <td>0.97%</td>
    </tr>
    <tr>
      <td>Pole</td>
      <td>635,225</td>
      <td>1.19%</td>
    </tr>
    <tr>
      <td>Traffic light</td>
      <td>81,342</td>
      <td>0.15%</td>
    </tr>
    <tr>
      <td>Traffic sign</td>
      <td>284,603</td>
      <td>0.53%</td>
    </tr>
    <tr>
      <td>Vegetation</td>
      <td>8,817,392</td>
      <td>16.51%</td>
    </tr>
    <tr>
      <td>Terrain</td>
      <td>725,905</td>
      <td>1.36%</td>
    </tr>
    <tr>
      <td>Sky</td>
      <td>11,134,137</td>
      <td>20.85%</td>
    </tr>
    <tr>
      <td>Person</td>
      <td>187,644</td>
      <td>0.35%</td>
    </tr>
    <tr>
      <td>Rider</td>
      <td>16,331</td>
      <td>0.03%</td>
    </tr>
    <tr>
      <td>Car</td>
      <td>5,684,376</td>
      <td>10.65%</td>
    </tr>
    <tr>
      <td>Truck</td>
      <td>622,560</td>
      <td>1.17%</td>
    </tr>
    <tr>
      <td>Bus</td>
      <td>401,880</td>
      <td>0.75%</td>
    </tr>
    <tr>
      <td>Train</td>
      <td>8,826</td>
      <td>0.02%</td>
    </tr>
    <tr>
      <td>Motorcycle</td>
      <td>7,140</td>
      <td>0.01%</td>
    </tr>
    <tr>
      <td>Bicycle</td>
      <td>10,650</td>
      <td>0.02%</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Pixel distribution across semantic classes in the dataset.</p>

<p>The BDD100K dataset exhibits severe class imbalance, with a small number of large classes; most notably road, vegetation, and sky-dominating pixel coverage, while many semantically and operationally important classes occupy only a minimal fraction of the image area. This imbalance induces a common failure mode in semantic segmentation, whereby models achieve high pixel-level accuracy by overpredicting majority classes while largely ignoring rare objects. Such behavior is particularly problematic in autonomous driving contexts, where small and thin objects such as bicycles and poles are both visually challenging and safety-critical. Accordingly, our objective is not to optimize global accuracy, but to improve the reliable detection and segmentation of minority classes relative to a baseline, even when this entails trade-offs on dominant classes.</p>

<h2 id="initial-baseline-implementation">Initial baseline implementation</h2>

<p>We first started with simply feeding our test set to DeepLabV3 [1] with a ResNet-50 Backbone [5].</p>

<p>We chose this model for its proven performance on urban scene understanding (Cityscapes [6], COCO), its Atrous Spatial Pyramid Pooling (ASPP) module for multi-scale context, pretrained ResNet-50 backbone for strong feature representations, and auxiliary classifier head for improved gradient flow.</p>

<p>Our initial model performed deceptively well (when it was in fact not performant at all) because we used pixel-level accuracy as our metric. When checking model predictions on the test set, we can see the model is not performing well:</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/44/badseg.png" alt="Bad segmentation from the old model" /></p>

<p><strong>High pixel accuracy</strong>: ~ 93%</p>

<p><strong>Very poor mIoU</strong>: ~ 4%</p>

<p>This was likely caused by a severe class imbalance in the BDD100K dataset. The model learns to predict only common classes, causing high pixel accuracy but poor mIoU.</p>

<h2 id="improvements-atop-baseline-model">Improvements atop Baseline Model</h2>

<p>We implemented a variety of additions to our baseline model in hopes of pushing the model to classify low-density classes more.</p>

<h3 id="1-class-weighting">1. <strong>Class Weighting</strong></h3>

<p>We assign higher weight to rarer classes (less pixels labeled in pictures in the dataset) in our loss function in order to get our model to focus on these fine grained details. This prevents the model from straight out ignoring minority classes and simply labeling majority classes to everything. This works as a sort of increased penalty for cheating.</p>

<p>We assign inverse-frequency weight for class \(f_c\) and normalize.</p>

\[w_c = \frac{\frac{1}{f_cC}}{\sum_{i=1}^C w_i}C\]

<h3 id="2-combined-loss-function">2. <strong>Combined Loss Function</strong></h3>

<p>We use a combined <strong>Dice loss</strong> and <strong>Cross Entropy loss</strong> to better handle the class imbalance and improve boundary predictions.</p>

<p>Dice loss is defined as:</p>

\[{L}_{d} =1 - \frac{2\sum_{i=1}^n y_{pred_i}y_{true_i} + \epsilon}{\sum_{i=1}^n y_{pred_i} + \sum_{i=1}^n y_{true_i} + \epsilon}\]

<p>Cross Entropy loss is defined as:</p>

\[{L}_{CE} = -\sum_{c=1}^N y_clog(p_i)\]

<p>The combined loss is simply</p>

\[{L} = {L}_{d} + {L}_{CE}\]

<p>This creates a training objective that balances per-pixel classification accuracy and region level overlap, encouraging accurate semantic prediction.</p>

<h3 id="3-auxiliary-loss-usage">3. <strong>Auxiliary Loss Usage</strong></h3>

<p>We make use of the two predictions the network produces: full-resolution prediction and the intermediate <code class="language-plaintext highlighter-rouge">aux_out</code> prediction from an earlier layer. This helps our deep network’s gradients have a shorter path to reach earlier layers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">main_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">main_out</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span class="n">aux_loss</span> <span class="o">=</span> <span class="n">criterion_aux</span><span class="p">(</span><span class="n">aux_out</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="n">main_loss</span> <span class="o">+</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">aux_loss</span>
</code></pre></div></div>

<h3 id="4-separate-learning-rates">4. <strong>Separate Learning Rates</strong></h3>

<table>
  <thead>
    <tr>
      <th>Parameter Group</th>
      <th>Learning Rate</th>
      <th>Weight Decay</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Backbone Parameters</td>
      <td>1e-4</td>
      <td>1e-4</td>
    </tr>
    <tr>
      <td>Classifier Parameters</td>
      <td>1e-3</td>
      <td>1e-4</td>
    </tr>
  </tbody>
</table>

<p>We reduce the learning rate of the pretrained backbone and increase it for the classifier head to allow for better retainment of high-level vision understanding while better learning our dataset.</p>

<h3 id="5-gradient-clipping">5. <strong>Gradient Clipping</strong></h3>

<p>We clip gradients to a max norm of 1 to prevent exploding gradients. This allows for more stable training.</p>

\[g_i \leftarrow g_i \frac{1}{||\textbf{g}||_2}\]

<h2 id="further-model-development">Further Model Development</h2>

<p>The initial results of running our model through 6400 training samples on 9 epochs produced best model at the 9th epoch with validation mIoU (on 1600 test samples) of <strong>41.56%</strong>.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/44/mv1_training.png" alt="Training" />
<em>Note that the Epoch axies are wrong here; we trained 4 extra epochs after it was already trained for 5.</em></p>

<p>This shows promising improvement over our baseline. Looking at some model sample outputs, we see the model is able to identify large segments but struggles to match a precise boundary.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/44/okseg.png" alt="Some samples" /></p>

<p>This is reflected in our model’s per-class mIoU:</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>IoU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>road</td>
      <td>0.6055</td>
    </tr>
    <tr>
      <td>sidewalk</td>
      <td>0.4564</td>
    </tr>
    <tr>
      <td>building</td>
      <td>0.6682</td>
    </tr>
    <tr>
      <td>wall</td>
      <td>0.2186</td>
    </tr>
    <tr>
      <td>fence</td>
      <td>0.3026</td>
    </tr>
    <tr>
      <td>pole</td>
      <td>0.2435</td>
    </tr>
    <tr>
      <td>traffic_light</td>
      <td>0.3113</td>
    </tr>
    <tr>
      <td>traffic_sign</td>
      <td>0.3164</td>
    </tr>
    <tr>
      <td>vegetation</td>
      <td>0.7488</td>
    </tr>
    <tr>
      <td>terrain</td>
      <td>0.3584</td>
    </tr>
    <tr>
      <td>sky</td>
      <td>0.8786</td>
    </tr>
    <tr>
      <td>person</td>
      <td>0.4281</td>
    </tr>
    <tr>
      <td>rider</td>
      <td>0.2265</td>
    </tr>
    <tr>
      <td>car</td>
      <td>0.7943</td>
    </tr>
    <tr>
      <td>truck</td>
      <td>0.4343</td>
    </tr>
    <tr>
      <td>bus</td>
      <td>0.5171</td>
    </tr>
    <tr>
      <td>train</td>
      <td>0.0010</td>
    </tr>
    <tr>
      <td>motorcycle</td>
      <td>0.2024</td>
    </tr>
    <tr>
      <td>bicycle</td>
      <td>0.1842</td>
    </tr>
  </tbody>
</table>

<p>mIoU: 0.4156</p>

<p>In order to test whether our model can effectively differentiate difficult classes, however, we decided to repeat the same training setup using a modified dataset. In this variant, all classes are assigned a weight of 0 except for <em>pole, vegetation, person, car, motorcycle,</em> and <em>bicycle</em>. These classes are suspected to be especially difficult to segment due to their low frequency in the dataset and, for some, their thin or fine-grained structure.</p>

<p>The resulting model performed <strong>significantly worse overall</strong>, with particularly poor performance on thin objects such as <strong>poles</strong>, <strong>motorcycles</strong>, and <strong>bicycles</strong> with individual IoUs of less than 0.001 and an overall mIoU of 25%.</p>

<p>This collapse in our first 6-class attempt pushed us to design a third version: moving to 1024 x 512 resolution, stronger Focal Loss [3] focus (y = 3.0), heavier class weights (bike/moto 3x, pole 2x), wider scale augmentation (0.5 - 2.0), tighter crops (0.6 - 1.0) to keep small objects in frame, and light random erasing to cut background reliance. The combined Focal Loss [3] and Dice Loss [4] formulation helped target hard examples and improve boundary predictions for thin objects. The goal was to recover and improve thin-object performance, especially for bicycles and motorcycles, while staying focused on the six safety-critical classes. However, these improvements, especially the increase in resolution did lead to a significant increase in training time (one hour per epoch) as we would need to half the batch size.</p>

<p>After 10 epochs with the improved v3 setup, the 6-class evaluation produced the following class-wise weights and IoU scores:</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Weight</th>
      <th>IoU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>pole</td>
      <td>0.233</td>
      <td>0.0105</td>
    </tr>
    <tr>
      <td>vegetation</td>
      <td>0.100</td>
      <td>0.6153</td>
    </tr>
    <tr>
      <td>person</td>
      <td>0.817</td>
      <td>0.4454</td>
    </tr>
    <tr>
      <td>car</td>
      <td>0.100</td>
      <td>0.7371</td>
    </tr>
    <tr>
      <td>motorcycle</td>
      <td>10.000</td>
      <td>0.2537</td>
    </tr>
    <tr>
      <td>bicycle</td>
      <td>7.243</td>
      <td>0.0934</td>
    </tr>
  </tbody>
</table>

<p>mIoU: 0.3592</p>

<p>Although our overall mIoU was still lower than our model for all classes, it still showed significant improvement as not only were pole and bicycle able to break the 0.01 value of IoU, motorcycle improved drastically , reaching an IoU of 0.25. However we do believe that further training could generate even stronger results.</p>

<h2 id="model-development-versions">Model Development Versions</h2>

<h3 id="model-v1---baseline-deeplabv3">Model v1 - Baseline DeepLabV3</h3>

<p><strong>Architecture:</strong> DeepLabV3-ResNet50 pretrained on COCO</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li>Resolution: 512 x 256</li>
  <li>Classes: All 19 BDD100K classes</li>
  <li>Loss: Cross Entropy only</li>
  <li>Batch Size: 8</li>
</ul>

<p><strong>Performance:</strong></p>
<ul>
  <li>Pixel Accuracy: ~ 94%</li>
  <li>mIoU: ~ 4%</li>
</ul>

<p><strong>Key Problem:</strong> Severe class imbalance - road and sky dominate</p>

<hr />

<h3 id="model-v2---class-balancing--loss-improvements">Model v2 - Class Balancing &amp; Loss Improvements</h3>

<p><strong>Improvements over v1:</strong></p>
<ol>
  <li>Class weighting</li>
  <li>Combined Focal Loss</li>
  <li>Auxiliary loss branch</li>
  <li>Separate learning rates</li>
  <li>Gradient clipping</li>
</ol>

<p><strong>Configuration:</strong></p>
<ul>
  <li>Resolution: 512 x 256</li>
  <li>Classes: All 19 BDD100K classes</li>
  <li>Loss: Focal (y = 2.0) + Dice (weight = 0.3)</li>
  <li>Batch Size: 8</li>
  <li>Epochs: 10</li>
</ul>

<p><strong>Performance on all 19 classes:</strong></p>
<ul>
  <li>Pixel Accuracy: ~ 88 - 92%</li>
  <li>mIoU: ~ 40%</li>
  <li>Common:  ~ 80%</li>
  <li>Medium:  ~ 50%</li>
  <li>Rare:    ~ 20%</li>
</ul>

<p><strong>Performance on just 6 classes:</strong></p>
<ul>
  <li>mIoU: ~ 20%</li>
  <li>Bicycles, Motorcycles, Poles: ~1%</li>
</ul>

<h3 id="model-v3---bicyclemotorcycle-targeted-optimization">Model v3 - Bicycle/Motorcycle Targeted Optimization</h3>

<p><strong>Approach:</strong> Focused optimization on 6 safety-critical urban classes with emphasis on thin/small objects</p>

<p><strong>Key Improvements over v2:</strong></p>

<ol>
  <li><strong>Doubled Resolution</strong>
    <ul>
      <li><strong>1024 x 512</strong> (4x pixels)</li>
      <li>Critical for thin objects (bicycles, motorcycles, poles)</li>
      <li>Improved small object segmentation</li>
    </ul>
  </li>
  <li><strong>Enhanced Focal Loss</strong>
    <ul>
      <li>Gamma: <strong>y = 3.0</strong></li>
      <li>Stronger focus on hard negatives</li>
      <li>Better handles misclassified edge pixels</li>
    </ul>
  </li>
  <li><strong>Class Weight Boosting</strong>
    <ul>
      <li>Bicycle: <strong>3.0x</strong> base weight</li>
      <li>Motorcycle: <strong>3.0x</strong> base weight</li>
      <li>Pole: <strong>2.0x</strong> base weight</li>
      <li>Prevents model from ignoring safety-critical minority classes</li>
    </ul>
  </li>
  <li><strong>Wider Scale + Object-Centric Crops</strong>
    <ul>
      <li>Scale range: <strong>0.5 - 2.0x</strong></li>
      <li>Random crop <strong>0.6 - 1.0x</strong> to keep small/thin objects in view</li>
      <li>Gaussian blur, color jitter for robustness</li>
    </ul>
  </li>
  <li><strong>Combined Loss Refinement</strong>
    <ul>
      <li>Focal Loss (y = 3.0): Hard example mining</li>
      <li>Dice Loss (weight = 0.3): Boundary prediction</li>
      <li>Complementary benefits for small objects</li>
    </ul>
  </li>
  <li><strong>Light Random Erasing (train only)</strong>
    <ul>
      <li>p = 0.3, small holes to reduce background reliance</li>
      <li>Encourages robustness to occlusion and clutter</li>
    </ul>
  </li>
</ol>

<p><strong>Configuration:</strong></p>
<ul>
  <li>Resolution: <strong>1024 x 512</strong></li>
  <li>Classes: <strong>6-class subset</strong> (bicycle, motorcycle, pole, car, person, vegetation)</li>
  <li>Loss: Focal (y = 3.0) + Dice (weight = 0.3)</li>
  <li>Batch Size: <strong>4</strong></li>
  <li>Epochs: 10</li>
</ul>

<p><strong>Performance:</strong></p>
<ul>
  <li>mIoU: ~ 36%</li>
  <li>pole: ~ 1%</li>
  <li>vegetation: ~ 62%</li>
  <li>person: ~ 45%</li>
  <li>car: ~ 74%</li>
  <li>motorcycle: ~ 25%</li>
  <li>bicycle: ~ 9%</li>
</ul>

<h2 id="future-uses">Future uses</h2>

<p>Beyond standard IoU metrics, we implemented a <code class="language-plaintext highlighter-rouge">RoadSafetyAnalyzer</code> class in our notebook that computes a composite safety score (0 - 100) based on segmentation predictions. This scorer evaluates road visibility, pedestrian and vehicle presence, obstacle detection, and lane clarity to provide a risk-level assessment (LOW, MODERATE, HIGH, CRITICAL) for each scene.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/44/safety_comparison.png" alt="Best and worst safety scores" /></p>

<p>Future work could extend this framework to temporal analysis across video sequences, multi-sensor fusion, and adaptive policy generation based on safety scores.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This work demonstrates that semantic segmentation performance in urban driving scenes cannot be meaningfully assessed, or improved, using pixel-level accuracy alone in the presence of extreme class imbalance. Through a series of controlled model iterations on the BDD100K dataset, we show that loss design, class-aware weighting, and training strategy play a decisive role in recovering minority-class performance. By progressively shifting from a general-purpose baseline to a targeted, high-resolution optimization focused on safety-critical classes, we achieve substantial gains in mIoU and per-class IoU for thin and rare objects such as bicycles, motorcycles, and poles. These results highlight the necessity of task and risk-aware model design in autonomous driving and provide a practical blueprint for addressing imbalance-driven failure modes without reliance on dataset expansion.</p>

<h2 id="references">References</h2>

<p>[1] Chen, L. C., Papandreou, G., Schroff, F., &amp; Adam, H. “Rethinking atrous convolution for semantic image segmentation.” <em>arXiv preprint arXiv:1706.05587</em>. 2017.</p>

<p>[2] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., … &amp; Darrell, T. “BDD100K: A diverse driving dataset for heterogeneous multitask learning.” <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2020.</p>

<p>[3] Lin, T. Y., Goyal, P., Girshick, R., He, K., &amp; Dollar, P. “Focal loss for dense object detection.” <em>Proceedings of the IEEE international conference on computer vision</em>. 2017.</p>

<p>[4] Milletari, F., Navab, N., &amp; Ahmadi, S. A. “V-net: Fully convolutional neural networks for volumetric medical image segmentation.” <em>2016 fourth international conference on 3D vision (3DV)</em>. 2016.</p>

<p>[5] He, K., Zhang, X., Ren, S., &amp; Sun, J. “Deep residual learning for image recognition.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>]]></content><author><name>Benjamin Man, Nathan Leobandung, Jason Jiang, Steven Pan</name></author><summary type="html"><![CDATA[Street-level semantic segmentation is a core capability for autonomous driving systems, yet performance is often dominated by severe class imbalance where large categories such as roads and skies overwhelm safety-critical but rare classes like bikes, motorcycles, and poles. Using the BDD100K dataset, this study systematically examines how architectural choices, loss design, and training strategies affect segmentation quality beyond misleading pixel-level accuracy. Starting from a DeepLabV3-ResNet50 baseline, we demonstrate that high pixel accuracy (~ 94%) can coincide with extremely poor mIoU (~ 4%) under imbalance. We then introduce class-weighted and combined Dice-Cross-Entropy/Focal losses, auxiliary supervision, differential learning rates, and gradient clipping, achieving a 10x improvement in mIoU. Finally, we propose a targeted optimization strategy that remaps the task to six safety-critical small classes and leverages higher resolution, aggressive augmentation, and boosted class weights for thin and small objects. This approach significantly improves IoU for bicycles, motorcycles, and poles, highlighting practical trade-offs between accuracy, resolution, and computational cost. However, such increases in resolution resulted in significant increases to training time per epoch, resulting in less training. Overall, the work provides an empirically grounded blueprint for addressing class imbalance and small-object segmentation in urban scene understanding. Streetview Semantics What is streetview semantics? Dataset Initial baseline implementation Improvements atop Baseline Model Further Model Development Model Development Versions Future uses Conclusion References Streetview Semantics What is streetview semantics? Streetview semantics involves assigning meaningful labels such as road, sidewalk, building, vehicle, pedestrian, or sky to pixels or regions in images captured from a street-level perspective. Semantic segmentation of streetview images can be used to transform raw visual input into a structured representation, where each region corresponds to an object or surface with a specific functional role. This semantic understanding enables higher-level reasoning about the environment, such as identifying drivable areas, detecting obstacles, and interpreting relationships between objects in down stream tasks, like autonomus driving. Dataset The BDD100K dataset [2] includes a diverse collection of streetview images designed for autonomus driving research. The dataset includes images captured by a camera mounted on a vehicle from a variety of locations like New York and San Francisco. We use 10,000 images and their semantically segmented labeled masks to train and test our model. A sample image and its corresponding mask from the dataset. Class Pixel Count Percentage Road 14,112,633 26.43% Sidewalk 1,072,369 2.01% Building 8,773,136 16.43% Wall 300,296 0.56% Fence 520,512 0.97% Pole 635,225 1.19% Traffic light 81,342 0.15% Traffic sign 284,603 0.53% Vegetation 8,817,392 16.51% Terrain 725,905 1.36% Sky 11,134,137 20.85% Person 187,644 0.35% Rider 16,331 0.03% Car 5,684,376 10.65% Truck 622,560 1.17% Bus 401,880 0.75% Train 8,826 0.02% Motorcycle 7,140 0.01% Bicycle 10,650 0.02% Table 1: Pixel distribution across semantic classes in the dataset. The BDD100K dataset exhibits severe class imbalance, with a small number of large classes; most notably road, vegetation, and sky-dominating pixel coverage, while many semantically and operationally important classes occupy only a minimal fraction of the image area. This imbalance induces a common failure mode in semantic segmentation, whereby models achieve high pixel-level accuracy by overpredicting majority classes while largely ignoring rare objects. Such behavior is particularly problematic in autonomous driving contexts, where small and thin objects such as bicycles and poles are both visually challenging and safety-critical. Accordingly, our objective is not to optimize global accuracy, but to improve the reliable detection and segmentation of minority classes relative to a baseline, even when this entails trade-offs on dominant classes. Initial baseline implementation We first started with simply feeding our test set to DeepLabV3 [1] with a ResNet-50 Backbone [5]. We chose this model for its proven performance on urban scene understanding (Cityscapes [6], COCO), its Atrous Spatial Pyramid Pooling (ASPP) module for multi-scale context, pretrained ResNet-50 backbone for strong feature representations, and auxiliary classifier head for improved gradient flow. Our initial model performed deceptively well (when it was in fact not performant at all) because we used pixel-level accuracy as our metric. When checking model predictions on the test set, we can see the model is not performing well: High pixel accuracy: ~ 93% Very poor mIoU: ~ 4% This was likely caused by a severe class imbalance in the BDD100K dataset. The model learns to predict only common classes, causing high pixel accuracy but poor mIoU. Improvements atop Baseline Model We implemented a variety of additions to our baseline model in hopes of pushing the model to classify low-density classes more. 1. Class Weighting We assign higher weight to rarer classes (less pixels labeled in pictures in the dataset) in our loss function in order to get our model to focus on these fine grained details. This prevents the model from straight out ignoring minority classes and simply labeling majority classes to everything. This works as a sort of increased penalty for cheating. We assign inverse-frequency weight for class \(f_c\) and normalize. \[w_c = \frac{\frac{1}{f_cC}}{\sum_{i=1}^C w_i}C\] 2. Combined Loss Function We use a combined Dice loss and Cross Entropy loss to better handle the class imbalance and improve boundary predictions. Dice loss is defined as: \[{L}_{d} =1 - \frac{2\sum_{i=1}^n y_{pred_i}y_{true_i} + \epsilon}{\sum_{i=1}^n y_{pred_i} + \sum_{i=1}^n y_{true_i} + \epsilon}\] Cross Entropy loss is defined as: \[{L}_{CE} = -\sum_{c=1}^N y_clog(p_i)\] The combined loss is simply \[{L} = {L}_{d} + {L}_{CE}\] This creates a training objective that balances per-pixel classification accuracy and region level overlap, encouraging accurate semantic prediction. 3. Auxiliary Loss Usage We make use of the two predictions the network produces: full-resolution prediction and the intermediate aux_out prediction from an earlier layer. This helps our deep network’s gradients have a shorter path to reach earlier layers. main_loss = criterion(main_out, masks) aux_loss = criterion_aux(aux_out, masks) total_loss = main_loss + 0.4 * aux_loss 4. Separate Learning Rates Parameter Group Learning Rate Weight Decay Backbone Parameters 1e-4 1e-4 Classifier Parameters 1e-3 1e-4 We reduce the learning rate of the pretrained backbone and increase it for the classifier head to allow for better retainment of high-level vision understanding while better learning our dataset. 5. Gradient Clipping We clip gradients to a max norm of 1 to prevent exploding gradients. This allows for more stable training. \[g_i \leftarrow g_i \frac{1}{||\textbf{g}||_2}\] Further Model Development The initial results of running our model through 6400 training samples on 9 epochs produced best model at the 9th epoch with validation mIoU (on 1600 test samples) of 41.56%. Note that the Epoch axies are wrong here; we trained 4 extra epochs after it was already trained for 5. This shows promising improvement over our baseline. Looking at some model sample outputs, we see the model is able to identify large segments but struggles to match a precise boundary. This is reflected in our model’s per-class mIoU: Class IoU road 0.6055 sidewalk 0.4564 building 0.6682 wall 0.2186 fence 0.3026 pole 0.2435 traffic_light 0.3113 traffic_sign 0.3164 vegetation 0.7488 terrain 0.3584 sky 0.8786 person 0.4281 rider 0.2265 car 0.7943 truck 0.4343 bus 0.5171 train 0.0010 motorcycle 0.2024 bicycle 0.1842 mIoU: 0.4156 In order to test whether our model can effectively differentiate difficult classes, however, we decided to repeat the same training setup using a modified dataset. In this variant, all classes are assigned a weight of 0 except for pole, vegetation, person, car, motorcycle, and bicycle. These classes are suspected to be especially difficult to segment due to their low frequency in the dataset and, for some, their thin or fine-grained structure. The resulting model performed significantly worse overall, with particularly poor performance on thin objects such as poles, motorcycles, and bicycles with individual IoUs of less than 0.001 and an overall mIoU of 25%. This collapse in our first 6-class attempt pushed us to design a third version: moving to 1024 x 512 resolution, stronger Focal Loss [3] focus (y = 3.0), heavier class weights (bike/moto 3x, pole 2x), wider scale augmentation (0.5 - 2.0), tighter crops (0.6 - 1.0) to keep small objects in frame, and light random erasing to cut background reliance. The combined Focal Loss [3] and Dice Loss [4] formulation helped target hard examples and improve boundary predictions for thin objects. The goal was to recover and improve thin-object performance, especially for bicycles and motorcycles, while staying focused on the six safety-critical classes. However, these improvements, especially the increase in resolution did lead to a significant increase in training time (one hour per epoch) as we would need to half the batch size. After 10 epochs with the improved v3 setup, the 6-class evaluation produced the following class-wise weights and IoU scores: Class Weight IoU pole 0.233 0.0105 vegetation 0.100 0.6153 person 0.817 0.4454 car 0.100 0.7371 motorcycle 10.000 0.2537 bicycle 7.243 0.0934 mIoU: 0.3592 Although our overall mIoU was still lower than our model for all classes, it still showed significant improvement as not only were pole and bicycle able to break the 0.01 value of IoU, motorcycle improved drastically , reaching an IoU of 0.25. However we do believe that further training could generate even stronger results. Model Development Versions Model v1 - Baseline DeepLabV3 Architecture: DeepLabV3-ResNet50 pretrained on COCO Configuration: Resolution: 512 x 256 Classes: All 19 BDD100K classes Loss: Cross Entropy only Batch Size: 8 Performance: Pixel Accuracy: ~ 94% mIoU: ~ 4% Key Problem: Severe class imbalance - road and sky dominate Model v2 - Class Balancing &amp; Loss Improvements Improvements over v1: Class weighting Combined Focal Loss Auxiliary loss branch Separate learning rates Gradient clipping Configuration: Resolution: 512 x 256 Classes: All 19 BDD100K classes Loss: Focal (y = 2.0) + Dice (weight = 0.3) Batch Size: 8 Epochs: 10 Performance on all 19 classes: Pixel Accuracy: ~ 88 - 92% mIoU: ~ 40% Common: ~ 80% Medium: ~ 50% Rare: ~ 20% Performance on just 6 classes: mIoU: ~ 20% Bicycles, Motorcycles, Poles: ~1% Model v3 - Bicycle/Motorcycle Targeted Optimization Approach: Focused optimization on 6 safety-critical urban classes with emphasis on thin/small objects Key Improvements over v2: Doubled Resolution 1024 x 512 (4x pixels) Critical for thin objects (bicycles, motorcycles, poles) Improved small object segmentation Enhanced Focal Loss Gamma: y = 3.0 Stronger focus on hard negatives Better handles misclassified edge pixels Class Weight Boosting Bicycle: 3.0x base weight Motorcycle: 3.0x base weight Pole: 2.0x base weight Prevents model from ignoring safety-critical minority classes Wider Scale + Object-Centric Crops Scale range: 0.5 - 2.0x Random crop 0.6 - 1.0x to keep small/thin objects in view Gaussian blur, color jitter for robustness Combined Loss Refinement Focal Loss (y = 3.0): Hard example mining Dice Loss (weight = 0.3): Boundary prediction Complementary benefits for small objects Light Random Erasing (train only) p = 0.3, small holes to reduce background reliance Encourages robustness to occlusion and clutter Configuration: Resolution: 1024 x 512 Classes: 6-class subset (bicycle, motorcycle, pole, car, person, vegetation) Loss: Focal (y = 3.0) + Dice (weight = 0.3) Batch Size: 4 Epochs: 10 Performance: mIoU: ~ 36% pole: ~ 1% vegetation: ~ 62% person: ~ 45% car: ~ 74% motorcycle: ~ 25% bicycle: ~ 9% Future uses Beyond standard IoU metrics, we implemented a RoadSafetyAnalyzer class in our notebook that computes a composite safety score (0 - 100) based on segmentation predictions. This scorer evaluates road visibility, pedestrian and vehicle presence, obstacle detection, and lane clarity to provide a risk-level assessment (LOW, MODERATE, HIGH, CRITICAL) for each scene. Future work could extend this framework to temporal analysis across video sequences, multi-sensor fusion, and adaptive policy generation based on safety scores. Conclusion This work demonstrates that semantic segmentation performance in urban driving scenes cannot be meaningfully assessed, or improved, using pixel-level accuracy alone in the presence of extreme class imbalance. Through a series of controlled model iterations on the BDD100K dataset, we show that loss design, class-aware weighting, and training strategy play a decisive role in recovering minority-class performance. By progressively shifting from a general-purpose baseline to a targeted, high-resolution optimization focused on safety-critical classes, we achieve substantial gains in mIoU and per-class IoU for thin and rare objects such as bicycles, motorcycles, and poles. These results highlight the necessity of task and risk-aware model design in autonomous driving and provide a practical blueprint for addressing imbalance-driven failure modes without reliance on dataset expansion. References [1] Chen, L. C., Papandreou, G., Schroff, F., &amp; Adam, H. “Rethinking atrous convolution for semantic image segmentation.” arXiv preprint arXiv:1706.05587. 2017. [2] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., … &amp; Darrell, T. “BDD100K: A diverse driving dataset for heterogeneous multitask learning.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020. [3] Lin, T. Y., Goyal, P., Girshick, R., He, K., &amp; Dollar, P. “Focal loss for dense object detection.” Proceedings of the IEEE international conference on computer vision. 2017. [4] Milletari, F., Navab, N., &amp; Ahmadi, S. A. “V-net: Fully convolutional neural networks for volumetric medical image segmentation.” 2016 fourth international conference on 3D vision (3DV). 2016. [5] He, K., Zhang, X., Ren, S., &amp; Sun, J. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.]]></summary></entry><entry><title type="html">Post Template</title><link href="http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html" rel="alternate" type="text/html" title="Post Template" /><published>2024-01-01T00:00:00-08:00</published><updated>2024-01-01T00:00:00-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post</id><content type="html" xml:base="http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html"><![CDATA[<blockquote>
  <p>This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#main-content" id="markdown-toc-main-content">Main Content</a></li>
  <li><a href="#basic-syntax" id="markdown-toc-basic-syntax">Basic Syntax</a>    <ul>
      <li><a href="#image" id="markdown-toc-image">Image</a></li>
      <li><a href="#table" id="markdown-toc-table">Table</a></li>
      <li><a href="#code-block" id="markdown-toc-code-block">Code Block</a></li>
      <li><a href="#formula" id="markdown-toc-formula">Formula</a></li>
      <li><a href="#more-markdown-syntax" id="markdown-toc-more-markdown-syntax">More Markdown Syntax</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="main-content">Main Content</h2>
<p>Your survey starts here. You can refer to the <a href="https://github.com/lilianweng/lil-log/tree/master/_posts">source code</a> of <a href="https://lilianweng.github.io/lil-log/">lil’s blogs</a> for article structure ideas or Markdown syntax. We’ve provided a <a href="https://ucladeepvision.github.io/CS188-Projects-2022Winter/2017/06/21/an-overview-of-deep-learning.html">sample post</a> from Lilian Weng and you can find the source code <a href="https://raw.githubusercontent.com/UCLAdeepvision/CS188-Projects-2022Winter/main/_posts/2017-06-21-an-overview-of-deep-learning.md">here</a></p>

<h2 id="basic-syntax">Basic Syntax</h2>
<h3 id="image">Image</h3>
<p>Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.</p>

<p style="width: 400px; max-width: 100%;">You can add an image to your survey like this:
<img src="/CS163-Projects-2025Fall/assets/images/UCLAdeepvision/object_detection.png" alt="YOLO" /></p>
<p><em>Fig 1. YOLO: An object detection method in computer vision</em> [1].</p>

<p>Please cite the image if it is taken from other people’s work.</p>

<h3 id="table">Table</h3>
<p>Here is an example for creating tables, including alignment syntax.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">column 1</th>
      <th style="text-align: right">column 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">row1</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
    <tr>
      <td style="text-align: left">row2</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
  </tbody>
</table>

<h3 id="code-block">Code Block</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample code block
import torch
print (torch.__version__)
</code></pre></div></div>

<h3 id="formula">Formula</h3>
<p>Please use latex to generate formulas, such as:</p>

\[\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}\]

<p>or you can write in-text formula \(y = wx + b\).</p>

<h3 id="more-markdown-syntax">More Markdown Syntax</h3>
<p>You can find more Markdown syntax at <a href="https://www.markdownguide.org/basic-syntax/">this page</a>.</p>

<h2 id="reference">Reference</h2>
<p>Please make sure to cite properly in your work, for example:</p>

<p>[1] Redmon, Joseph, et al. “You only look once: Unified, real-time object detection.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>

<hr />]]></content><author><name>UCLAdeepvision</name></author><summary type="html"><![CDATA[This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.]]></summary></entry></feed>