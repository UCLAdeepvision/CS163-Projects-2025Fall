<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/CS163-Projects-2025Fall/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/CS163-Projects-2025Fall/" rel="alternate" type="text/html" /><updated>2025-12-12T23:04:52-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/feed.xml</id><title type="html">2025F, UCLA CS163 Course Projects</title><subtitle>Course projects for UCLA CS163, Deep Learning in Compuver Vision</subtitle><author><name>UCLAdeepvision</name></author><entry><title type="html">Project Track: Project 8 - Streetview Semantic Segmentation</title><link href="http://localhost:4000/CS163-Projects-2025Fall/2025/12/12/team02-streetviewseg.html" rel="alternate" type="text/html" title="Project Track: Project 8 - Streetview Semantic Segmentation" /><published>2025-12-12T00:00:00-08:00</published><updated>2025-12-12T00:00:00-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/2025/12/12/team02-streetviewseg</id><content type="html" xml:base="http://localhost:4000/CS163-Projects-2025Fall/2025/12/12/team02-streetviewseg.html"><![CDATA[<blockquote>
  <p>In this project, we delve into the topic of developing model to apply semantic segmentations on fine-grained urban structures based on pretrained Segformer model. We explore 3 approaches to enhance the model performance, and analyze the result of each.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a>    <ul>
      <li><a href="#dataset-split" id="markdown-toc-dataset-split">Dataset Split</a></li>
    </ul>
  </li>
  <li><a href="#model-segformer" id="markdown-toc-model-segformer">Model: Segformer</a></li>
  <li><a href="#baseline" id="markdown-toc-baseline">Baseline</a></li>
  <li><a href="#evaluation-metrics" id="markdown-toc-evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#basic-syntax" id="markdown-toc-basic-syntax">Basic Syntax</a>    <ul>
      <li><a href="#image" id="markdown-toc-image">Image</a></li>
      <li><a href="#table" id="markdown-toc-table">Table</a></li>
      <li><a href="#code-block" id="markdown-toc-code-block">Code Block</a></li>
      <li><a href="#formula" id="markdown-toc-formula">Formula</a></li>
      <li><a href="#more-markdown-syntax" id="markdown-toc-more-markdown-syntax">More Markdown Syntax</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>Understanding street-level scenes through segmentation is crucial to autonoumous driving, urban mapping, and robot perception. And it is especially important when it comes to segment fine-grained urban structures because a lot of what makes a street safe, legal, and navigable lives in small, easily missed details. So, this naturally leads to the question we want to investigate in this project: How to improve the semantic segmentation performance on those fine-grained urban structures?</p>

<h2 id="dataset">Dataset</h2>
<p>Cityscapes is a large-scale, widely used dataset for understanding complex urban environments, featuring diverse street scenes from many cities with high-quality pixel-level annotations for tasks like semantic segmentation and instance segmentation. It contains 30 classes and many of them are considered to be fine-grained urban structures, thus this dataset is a perfect choice for this project.</p>

<p>We remapped all categories in cityscapes to a consistent six-class scheme - fence, car, vegetation, pole, traffic sign, and traffic light. All of the classes are fine-grained urban structures. We define an additional implicit background class which are default to ignore by setting their pixel values to zero.</p>
<h3 id="dataset-split">Dataset Split</h3>
<p>We partition the Cityscapes dataset into three subsets from training, validation, and testing. From the original Cityscapes training split, we sample 2000 images to form our training set, 250 images to form our validation set, and another 250 images to form our test set.</p>

<h2 id="model-segformer">Model: Segformer</h2>
<p>In this project, we build everything upon the Segformer model.
Segformer is a transformer-based semantic segmentation model designed to be simple and accurate. It contains two main parts: encoder and decoder. 
The encoder is MiT (Mix Transformer), a hierarchical Transformer that produces 4 multi-scale feature maps. It uses overlapped patch embeddings and an efficient attention design.
The decoder is a lightweight All-MLP decoder. It linearly projects each of the 4 feature maps to the same channel size, upsamples them to the same resolution, concatenates and fuses them with an MLP, then outputs per-pixel class scores
<img src="/CS163-Projects-2025Fall/assets/images/team02/segformerArch.png" alt="Segformer" style="width: 400px; max-width: 100%;" />
<em>Fig 1. Segformer Architecture, consists of a hierarchical Transformer encoder to extract coarse and fine features and a lightweight All-MLP decoder to fuse these multi-level features and predict the segmentation mask</em> [1].</p>

<h2 id="baseline">Baseline</h2>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>
<p>We evaluate the performance of models using both per-class intersection over union (IoU) and mean intersection over union (mIoU). IoU measures the overlap between the predicted bounding box and the ground truth bounding box. The equation of calculating IoU is given as follows:</p>

\[\mathrm{IoU}(A,B)=\frac{|A\cap B|}{|A\cup B|}\]

<p>Where:</p>
<ul>
  <li>\(A \cap B\) is the area (or volume) of the overlap between <strong>A</strong> and <strong>B</strong></li>
  <li>\(A \cup B\) is the area (or volume) covered by <strong>A</strong> or <strong>B</strong> (their union)</li>
</ul>

<p>Given <strong>C</strong> classes, the <strong>mean IoU (mIoU)</strong> is the average IoU across classes:</p>

\[\mathrm{mIoU} = \frac{1}{C}\sum_{c=1}^{C} \mathrm{IoU}_c\]

<p>The higher the IoU and mIoU value is, the better is the model performance.</p>

<h2 id="basic-syntax">Basic Syntax</h2>
<h3 id="image">Image</h3>
<p>Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.</p>

<p style="width: 400px; max-width: 100%;">You can add an image to your survey like this:
<img src="/CS163-Projects-2025Fall/assets/images/UCLAdeepvision/object_detection.png" alt="YOLO" /></p>
<p><em>Fig 1. YOLO: An object detection method in computer vision</em> [1].</p>

<p>Please cite the image if it is taken from other people’s work.</p>

<h3 id="table">Table</h3>
<p>Here is an example for creating tables, including alignment syntax.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">column 1</th>
      <th style="text-align: right">column 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">row1</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
    <tr>
      <td style="text-align: left">row2</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
  </tbody>
</table>

<h3 id="code-block">Code Block</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample code block
import torch
print (torch.__version__)
</code></pre></div></div>

<h3 id="formula">Formula</h3>
<p>Please use latex to generate formulas, such as:</p>

\[\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}\]

<p>or you can write in-text formula \(y = wx + b\).</p>

<h3 id="more-markdown-syntax">More Markdown Syntax</h3>
<p>You can find more Markdown syntax at <a href="https://www.markdownguide.org/basic-syntax/">this page</a>.</p>

<h2 id="reference">Reference</h2>
<p>Please make sure to cite properly in your work, for example:</p>

<p>[1] Redmon, Joseph, et al. “You only look once: Unified, real-time object detection.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>

<hr />]]></content><author><name>Team02</name></author><summary type="html"><![CDATA[In this project, we delve into the topic of developing model to apply semantic segmentations on fine-grained urban structures based on pretrained Segformer model. We explore 3 approaches to enhance the model performance, and analyze the result of each.]]></summary></entry><entry><title type="html">Post Template</title><link href="http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html" rel="alternate" type="text/html" title="Post Template" /><published>2024-01-01T00:00:00-08:00</published><updated>2024-01-01T00:00:00-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post</id><content type="html" xml:base="http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html"><![CDATA[<blockquote>
  <p>This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#main-content" id="markdown-toc-main-content">Main Content</a></li>
  <li><a href="#basic-syntax" id="markdown-toc-basic-syntax">Basic Syntax</a>    <ul>
      <li><a href="#image" id="markdown-toc-image">Image</a></li>
      <li><a href="#table" id="markdown-toc-table">Table</a></li>
      <li><a href="#code-block" id="markdown-toc-code-block">Code Block</a></li>
      <li><a href="#formula" id="markdown-toc-formula">Formula</a></li>
      <li><a href="#more-markdown-syntax" id="markdown-toc-more-markdown-syntax">More Markdown Syntax</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="main-content">Main Content</h2>
<p>Your survey starts here. You can refer to the <a href="https://github.com/lilianweng/lil-log/tree/master/_posts">source code</a> of <a href="https://lilianweng.github.io/lil-log/">lil’s blogs</a> for article structure ideas or Markdown syntax. We’ve provided a <a href="https://ucladeepvision.github.io/CS188-Projects-2022Winter/2017/06/21/an-overview-of-deep-learning.html">sample post</a> from Lilian Weng and you can find the source code <a href="https://raw.githubusercontent.com/UCLAdeepvision/CS188-Projects-2022Winter/main/_posts/2017-06-21-an-overview-of-deep-learning.md">here</a></p>

<h2 id="basic-syntax">Basic Syntax</h2>
<h3 id="image">Image</h3>
<p>Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.</p>

<p style="width: 400px; max-width: 100%;">You can add an image to your survey like this:
<img src="/CS163-Projects-2025Fall/assets/images/UCLAdeepvision/object_detection.png" alt="YOLO" /></p>
<p><em>Fig 1. YOLO: An object detection method in computer vision</em> [1].</p>

<p>Please cite the image if it is taken from other people’s work.</p>

<h3 id="table">Table</h3>
<p>Here is an example for creating tables, including alignment syntax.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">column 1</th>
      <th style="text-align: right">column 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">row1</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
    <tr>
      <td style="text-align: left">row2</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
  </tbody>
</table>

<h3 id="code-block">Code Block</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample code block
import torch
print (torch.__version__)
</code></pre></div></div>

<h3 id="formula">Formula</h3>
<p>Please use latex to generate formulas, such as:</p>

\[\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}\]

<p>or you can write in-text formula \(y = wx + b\).</p>

<h3 id="more-markdown-syntax">More Markdown Syntax</h3>
<p>You can find more Markdown syntax at <a href="https://www.markdownguide.org/basic-syntax/">this page</a>.</p>

<h2 id="reference">Reference</h2>
<p>Please make sure to cite properly in your work, for example:</p>

<p>[1] Redmon, Joseph, et al. “You only look once: Unified, real-time object detection.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>

<hr />]]></content><author><name>UCLAdeepvision</name></author><summary type="html"><![CDATA[This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.]]></summary></entry></feed>