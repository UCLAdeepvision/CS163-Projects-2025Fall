<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>super res title</title>

    <meta name="description" content="Abstract: survery of 3 super res models, modification of one of them; 3 models highlight diff ways of doing super res">

    <meta content="2025F, UCLA CS163 Course Projects" property="og:site_name">
    
        <meta content="super res title" property="og:title">
    
    
        <meta content="article" property="og:type">
    
    
        <meta content="Abstract: survery of 3 super res models, modification of one of them; 3 models highlight diff ways of doing super res" property="og:description">
    
    
        <meta content="http://localhost:4000/2025/01/01/team01-super-res.html" property="og:url">
    
<!--
    
        <meta content="Thomas Peeler, Dylan Truong, Asher Christian, Daniel Chvat" property="article:author">
        <meta content="http://localhost:4000/about/" property="article:author">
     -->

    <!-- 
        <meta content="2025-01-01T00:00:00-08:00" property="article:published_time">
        <meta content="http://localhost:4000/about/" property="article:author">
    
    
    
        
     -->

    <link rel="shortcut icon" href="/CS163-Projects-2025Fall/assets/ucla_ico.jpg">
    <link rel="stylesheet" href="/CS163-Projects-2025Fall/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/CS163-Projects-2025Fall/2025/01/01/team01-super-res.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <!-- <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-8161570-6', 'auto');
        ga('send', 'pageview');
    </script> -->

    <!-- For Facebook share button -->
    <!-- <div id="fb-root"></div>
    <script>
      (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));
    </script> -->

    <!-- Twitter cards -->
    <!-- <meta name="twitter:site"    content="@">
    <meta name="twitter:creator" content="@UCLAdeepvision">
    <meta name="twitter:title"   content="super res title">

    
        <meta name="twitter:description" content="<blockquote>
  <p>Abstract: survery of 3 super res models, modification of one of them; 3 models highlight diff ways of doing super res</p>
</blockquote>

">
    

    
        <meta name="twitter:card"  content="summary">
        <meta name="twitter:image" content="">
     -->
    <!-- end of Twitter cards -->

</head>


  <body>

    <header class="site-header" role="banner" id='header-bar'>

    <div class="wrapper">
        
        <a class="site-title" style="color:#F2A900" href="/CS163-Projects-2025Fall/">2025F, UCLA CS163 Course Projects  </a>

        <!-- <nav class="site-nav">
            <a class="page-link" href="http://lilianweng.github.io" target="_blank">&#x1f349; About</a>
        </nav> -->
        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/about.html"> About</a>
        </nav>

        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/archive.html"> Archive</a>
        </nav>


        <!-- <nav class="site-nav">
            <a class="page-link" style="color:#FFD100" href="/CS163-Projects-2025Fall/FAQ.html"> FAQ</a>
        </nav> -->
        <!-- <nav class="site-nav">
            <a class="page-link" href="/CS163-Projects-2025Fall/log.html">&#x231b; Log</a>
        </nav> -->


    </div>

</header>


    <!-- Back to top button -->
    <script src="/CS163-Projects-2025Fall/assets/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop()</script>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">super res title</h1>
    <p class="post-meta">

      <time datetime="2025-01-01T00:00:00-08:00" itemprop="datePublished">
        
        Jan 1, 2025
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Thomas Peeler, Dylan Truong, Asher Christian, Daniel Chvat</span>
      </span>

      <!-- <span>
        
      </span> -->
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2025/01/01/team01-super-res.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>Abstract: survery of 3 super res models, modification of one of them; 3 models highlight diff ways of doing super res</p>
</blockquote>

<!--more-->
<!--bundle exec jekyll serve-->
<ul id="markdown-toc">
  <li><a href="#background-and-introduction" id="markdown-toc-background-and-introduction">Background and Introduction</a></li>
  <li><a href="#a-survey-of-deep-learning-for-super-resolution" id="markdown-toc-a-survey-of-deep-learning-for-super-resolution">A Survey of Deep Learning for Super Resolution</a>    <ul>
      <li><a href="#hybrid-attention-transformer-hat" id="markdown-toc-hybrid-attention-transformer-hat">Hybrid Attention Transformer (HAT)</a></li>
      <li><a href="#look-up-table-lut-methods" id="markdown-toc-look-up-table-lut-methods">Look-Up Table (LUT) Methods</a></li>
      <li><a href="#unfolding-networks" id="markdown-toc-unfolding-networks">Unfolding Networks</a></li>
    </ul>
  </li>
  <li><a href="#an-extension-of-the-hybrid-attention-transformer" id="markdown-toc-an-extension-of-the-hybrid-attention-transformer">An Extension of the Hybrid Attention Transformer</a>    <ul>
      <li><a href="#adaptive-sparse-transformer-ast" id="markdown-toc-adaptive-sparse-transformer-ast">Adaptive Sparse Transformer (AST)</a></li>
      <li><a href="#putting-it-together" id="markdown-toc-putting-it-together">Putting it Together</a></li>
      <li><a href="#results-3" id="markdown-toc-results-3">Results</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="background-and-introduction">Background and Introduction</h2>

<p>Super-resolution is a natural problem for image-to-image methods in computer vision, referring to the process of using a degraded, downsampled image to recover the original image before degradation and downsampling. Of course, this is an ill-posed problem; two different high-resolution images can downsample and degrade into the same low-resolution image (that is, the process is not injective), so truly inverting the process is impossible. For this reason, we settle for the task of estimating a function that approximates an inverse, producing a high-resolution image based solely on some mathematical assumptions and the information of the low-resolution image. This includes simple, classical methods, such as nearest-neighbor or bicubic upsampling, as well as, in recent years, statistical methods making use of deep learning for computer vision.</p>

<p>We will be surveying three of these recent methods, highlighting the very different ways that one can go about achieving the same end goal in super-resolution:</p>

<ul>
  <li>
    <p>The Hybrid Attention Transformer (HAT), a shifted-window transformer-based method</p>
  </li>
  <li>
    <p>Look-Up Table (LUT) methods, which use precomputed table of pixel estimates</p>
  </li>
  <li>
    <p>Unfolding networks, which combine classical model-based methods with learning-based methods</p>
  </li>
</ul>

<p>Additionally, we will be discussing an experiment that we carried out involving a modification to the HAT architecture.</p>

<h2 id="a-survey-of-deep-learning-for-super-resolution">A Survey of Deep Learning for Super Resolution</h2>

<h3 id="hybrid-attention-transformer-hat">Hybrid Attention Transformer (HAT)</h3>

<p>The Hybrid Attention Transformer aims to advance the field of low-level super-resolution by improving transformer architectures, which have recently become popular in SR tasks. Specifically, it addresses a key limitation of transformer-based SR models: their restricted receptive field. To overcome this, [1] introduces a Hybrid Attention Transformer (HAT) architecture designed to leverage more image pixels for reconstruction by combining self-attention, channel attention, and a novel overlapping-window attention mechanism.</p>

<!--i feel like this high-level overview does not make sense to a reader who doesnt already know what a RHAG or HAB or OCAB is-->
<!-- The high-level overview of HAT is as follows: first, the input image passes through a convolutional layer to extract shallow features. These features are then processed by a series of Residual Hybrid Attention Groups (RHAGs) followed by another convolutional layer; each RHAG block is made up of hybrid attention blocks (HAB), an overlapping cross-attention block (OCAB) and a convolution layer with a residual connection (these modules are described in detail below). Finally, a global residual connection combines the shallow features from the first convolution with the deep features produced by the RHAGs, and a reconstruction module featuring a pixel-shuffle uses this combined information to generate the final high-resolution image. -->

<p style="max-width: 90%;"><img src="/CS163-Projects-2025Fall/assets/images/01/HAT_architecture.png" alt="HAT architecture" /></p>
<p><em>Fig 1. HAT Architecture Overview [1].</em></p>

<h4 id="hybrid-attention-block-hab">Hybrid Attention Block (HAB)</h4>

<p style="max-width: 90%;"><img src="/CS163-Projects-2025Fall/assets/images/01/hab.png" alt="hybrid attention block" /></p>
<p><em>Fig 2. Hybrid Attention Block [1].</em></p>

<p>The Hybrid Attention Block (HAB) aims to combine the deep flexibility of vision transformers with the efficiency and global accessibility of channel attention.</p>

<p style="max-width: 90%;"><img src="/CS163-Projects-2025Fall/assets/images/01/cab.png" alt="channel attention block" /></p>
<p><em>Fig 3. Channel Attention Block [1].</em></p>

<p>The Channel Attention Block (CAB) consists of two convolutional layers separated by a GELU activation, followed by a channel attention (CA) module. The CA module consists of global average pooling (GAP), followed by two \(1 \times 1\) convolutions, ensuring that the channel number is the same as prior to the CA module, separated by another GELU. Then, the sigmoid function is applied to the now-\(1 \times 1 \times C\)-shaped output to create channel weights, which are multiplied channel-wise by the pre-CA module input, to get an output of the same \(H \times W \times C\) shape, where each channel has been scaled by a factor in \([0, 1]\). The factors are applied globally to each channel and are determined by GAP + convolutions, allowing all channels to utilize information from, to some degree, all positions in all other channels in an efficient manner.</p>

<p>In the HAB, the CAB is inserted into a standard Swin transformer block after the first LayerNorm in parallel with the window-based multi-head self-attention (W-MSA) module, where they are then combined additively (along with a residual connection). The efficiency and simplicity of the CAB comes at the cost of specific positional information being unutilized due to GAP, which is why the CAB is used in parallel with W-MSA, which explicitly <em>does</em> account for positional information. So, then, for a given input feature \(X\), HAB is computed as:</p>

\[X_N = \mathrm{LN}(X),\]

\[X_M = (\text{S})\text{W-MSA}(X_N) + \alpha\,\mathrm{CAB}(X_N) + X\]

\[Y = \mathrm{MLP}(\mathrm{LN}(X_M)) + X_M,\]

<p>where LN is the LayerNorm, (S)W-MSA is the (shifted) window-multihead self attention, MLP is the standard positionwise feed-forward network of a transformer, \(X_N\) and \(X_M\) denote intermediate features, \(Y\) is the output, and \(\alpha\) is a small hyperparameter that tempers the influence of the CAB compared to the W-MSA, to avoid issues of conflict between the two modules during optimization; empirically, \(\alpha=0.01\) was found to give the best performance, as this parallel scheme can introduce significant stability issues in the optimization process if the influence of each branch is not controlled. Within-window linear mappings are used to get \(Q\), \(K\), and \(V\) matrices, though, notably, these mappings are applied on the individual spatial pixels of the input, not on patches (that is, patches of size \(1 \times 1\) are used), as the spatial dimensions of the input need to be maintained throughout the network to account for residual connections. A fixed window size is employed, the same as in the regular Swin transformer, however, notably, the model does not contain any patch merging, again in service of retaining the shape of the image for residual connections. Within-window attention uses the standard attention scheme, with a fixed sinusoidal relative positional encoding within each window similar to that of “Attention is All You Need”.</p>

<h4 id="overlapping-cross-attention-block-ocab">Overlapping Cross-Attention Block (OCAB)</h4>

<p style="max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/01/overlapping.png" alt="Overlapping window partition" /></p>
<p><em>Fig 4. The overlapping window partition for OCA [1].</em></p>

<p>OCAB modules are used throughout the HAT to further allow for cross-window connections; the basic idea is that windowed self-attention is performed on the input as normal, in the exact same manner as in the HAB module, though the windows for the keys and values, while still centered on the same locations as the windows of the queries, are larger than those of the queries, allowing each window to pull in some information from neighboring windows that it normally would not see.</p>

<p>Specifically, for input features \(X\), let \(X_Q, X_K, X_V \in \mathbb{R}^{H \times W \times C}\). \(X_Q\) is partitioned into \(\frac{HW}{M^2}\) non-overlapping windows of size \(M \times M\). \(X_K\) and \(X_V\) are partitioned into \(\frac{HW}{M^2}\) overlapping windows of size \(M_o\times M_o\), where \(M_o\) is calculated as</p>

\[M_o = (1 + \gamma)M,\]

<p>where \(\gamma\) is a hyperparameter introduced in order to control the size of the overlapping windows, and the input is zero-padded to account for this larger window. Aside from shifting windows, this allows for even more cross-window information transmission, as each query window, which are the standard size, can pull in information from the key and value windows that include the query window and extend into the space that would otherwise be a separate, independent window. Additionally, OCAB is specifically designed to mitigate the blocking artifacts that the standard Swin transformer produces:</p>

<p style="max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/01/blocking.png" alt="Blocking " /></p>
<p><em>Fig 5. Comparison of blocking between SwinIR and HAT [1].</em></p>

<p>The OCAB combats this by directly stepping over the hard boundaries between windows, reducing their presence in the extracted features; each query window has access to information about the space beyond its own boundaries, so they can better create smooth boundaries in the features between adjacent windows. While standard window shifting has a similar goal, each shifted window still only has access to its own pixels, which may still lead to hard boundaries and blocking between windows that become present in the features.</p>

<p>Empirically, \(\gamma=0.5\) was found to give the best performance, translating to overlapping windows that are, by area, 4/9 composed of the original window and 5/9 composed of adjacent windows. This is less extreme than the shifted windows of the standard Swin transformer, where, since the regular shifting amount is half of the window size in each dimension, shifted windows are only 1/4 composed of their original window, and are 3/4 composed of other windows. Still, this additional method of cross-window attention gives the model greater flexibility and another opportunity to use a larger portion of the original image in its reconstruction process.</p>

<h4 id="residual-hybrid-attention-group-rhag">Residual Hybrid Attention Group (RHAG)</h4>

<p style="max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/01/rhag.png" alt="Residual Hybrid Attention Group" /></p>
<p><em>Fig 6. The structure of a Residual Hybrid Attention Group [1].</em></p>

<p>These modules are combined in a further modular format via a Residual Hybrid Attention Group (RHAG), which puts a variable number of HAB modules in sequence which are followed by an OCAB module, with a final convolutional layer to ensure compatability between shapes of the input and output that allows for a residual connection over the entire module. The sequencing of HAB modules mirrors the structure of standard Swin stages, as the attention windows are shifted between subsequent HABs; though, patch merging is skipped in favor of the OCABs, as they both aim to combine information between windows, and OCAB can retain the spatial size of the input.</p>

<h4 id="high-level-model-structure">High-Level Model Structure</h4>

<p style="max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/01/highlevelhat.png" alt="High Level HAT" /></p>
<p><em>Fig 7. The high-level structure of the HAT [1].</em></p>

<p>Contrary to the standard Swin, projection to the desired channel dimension is done using an initial convolutional layer; though, mirroring the structure of the Swin transformer, multiple RHAG modules are placed in sequence (similar to the sequence of stages in Swin), though in this case, a residual connection (along with a convolutional layer to ensure compatability of shapes) is employed across the series of RHAGs. Then, the final image reconstruction is performed using convolutions (for channel number adjustments) and pixel shuffle, where pixels from many channels are pulled together sequentially into a smaller number of channels with a larger image size.</p>

<h4 id="results">Results</h4>

<p style="max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/01/lam.png" alt="LAM Results" /></p>
<p><em>Fig 8. LAM results for different SR methods [1].</em></p>

<p>In practice, the HAT does, indeed, use a larger portion of the input image for reconstruction than other transformer-based SR methods. Above is a comparison of Local Attribution Maps (LAM) between different methods, which is a method for measuring which portions of the input were most influential for a given patch of the output. We see that HAT is highly nonlocal compared to other methods in terms of LAM, and performs better for it.</p>

<p style="max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/01/quantitativeresultshat.png" alt="Quantitative Results" /></p>
<p><em>Fig 9. Quantitative results for HAT compared to other methods [1].</em></p>

<p>The authors compared 3 versions of the model: HAT-S, HAT, and HAT-L: HAT has a similar size to the existing Swin super-resolution model SwinIR, with 6 RHAG modules with 6 HABs each; HAT-L is larger than HAT, with 12 RHAGs instead of 6; HAT-S is smaller than HAT, with depthwise convolutions and a smaller channel number throughout the model, resulting in overall computation similar to that of SwinIR. Quantitatively, in terms of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), two standard metrics for image reconstruction, HAT’s variants outperform their peers (if only slightly), and larger HAT models tend to yield better performance (again, if only slightly).</p>

<p>The HAT is a notable modification of the Swin transformer architecture, designed for super resolution, due to its large practical receptive field; while all SR methods have an entire image to work with as input, it takes effort and specific design to actually use the information that is there in an efficient manner, which can be crucial to reconstructing an image correctly. Judging by its quantitative results, the future state-of-the-art in SR will likely be driven, to some degree, by the development of methods that are designed to produce even larger receptive fields.</p>

<!-- -adv: 
--designed to have a large receptive field
--uses shifted window transformers; takes advantage of transformers while limiting computational burden
-disadv: 
--implicit assumptions about noise, blurring, downsampling of image based on training data; may not match what is seen at inference time, leading to inaccuracy -->

<h3 id="look-up-table-lut-methods">Look-Up Table (LUT) Methods</h3>

<p>There have been relatively few attempts to make modern learning-based methods of super-resolution practical for common consumer applications such as cameras, mobile phones, and televisions. Look-Up Table (LUT) methods, as described in [3], aim to bridge this gap by using a precomputed LUT, where the output values from an SR network are stored in advance; during inference, the system can quickly retrieve these high-resolution values by querying the LUT with low-resolution input pixels.</p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/lut_architecture.png" alt="LUT architecture" /></p>
<p><em>Fig 1. LUT Method Overview [3].</em></p>

<h4 id="the-lut">The LUT</h4>

<p>Although it may seem backwards, it may be useful to start with how the LUT will be used in inference: given a low-resolution image and an upscaling factor \(r\), our goal is will be to map each pixel of the low-res image to an image patch of size \(r \times r\), which we then assemble into a full upscaled image. This process will be done independently for each color channel, so that we are assembling 3 independent images with 1 color channel each and then concatenating them to form a 3-channel image. For a given image pixel at location \((i, j)\), we would like this mapping to take into account the pixels surrounding \((i, j)\) as well as the pixel itself; so, to get the patch for pixel \((i, j)\), the LUT will be indexed by a fixed number of pixel values surrounding \((i, j)\), and at this index, there will be \(r^2\) values that we can use to construct an image patch (given that we use pixel values for indexing, we use 8-bit color). For instance, for \(r=2\) and a LUT receptive field of 4, let 
\(I_0 = image[i, j], I_1 = image[i+1, j], I_2 = image[i, j+1], I_3 = image[i+1, j+1]\). 
Then, \(LUT[I_0][I_1][I_2][I_3][0][0]\) will contain the top-left corner of the upscaled patch, and \(LUT[I_0][I_1][I_2][I_3][1][1]\) will contain the bottom-right corner. This will be the basic method that the LUT uses in inference, which only requires table lookup, and no network computation.</p>

<h4 id="the-sr-network">The SR Network</h4>

<p>Now, with a clear goal of how the LUT should function, we can construct the network that will give us its values. Given that we want the patch in the LUT for each specific pixel value to depend solely on a fixed number of surrounding pixels, a convolutional network with a finely-controlled receptive field seems natural; again, note that the LUT upscaling process is performed independently on each color channel, so the input to this network will only have 1 channel. Specifically, given a receptive field size \(n\) and an upscaling factor \(r\), we can construct a network as:</p>

<ul>
  <li>
    <p>a series of convolutional layers (with nonlinearities), where, at the end, each output pixel has a gross receptive field of size \(n\) and the output has \(r^2\) channels</p>
  </li>
  <li>
    <p>a pixel shuffle that will construct upscaled patches from our channels and concatenate them together into a full upscaled image</p>
  </li>
</ul>

<p>with this, we have constructed a network where each tiled \(r \times r\) patch of the output depends only on a (contiguous) size \(n\) receptive field from the input. Now, if we optimize this network for super-resolution, to construct our LUT, we can simply generate patches of size \(n\) (or more to create an array) with specific values (which will be indexes in the LUT) and feed them into the network, and the resulting upscaled patch can be added to the LUT. The kernels of the convolutional layers could be anything as long as it conforms to the receptive field requirement, however, the authors of the paper use an architecture where the first convolutional layer results in a receptive field of size \(n\), and all subsequent layers are \(1\times 1\) convolutions (the issue of the kernel reducing the size of the input is dealt with by pre-padding the image); additionally, they use a square (or anything closest to square) kernel when possible, as it will capture the pixels that are most adjacent, and thus most relevant, to each other.</p>

<h4 id="lut-size">LUT Size</h4>

<p>Of course, the principal design priority of the LUT is efficiency in speed and size. Using 8-bit color, if we construct a LUT that accounts for every possible input patch, one can compute the size of the LUT for a given receptive field \(n\) and upscale factor \(r\) as</p>

\[(2^8)^n r^2 \text{ bytes}\]

<p>This can be seen clearly from the structure of the LUT; there are \(2^8\) possible values for a given 1-channel pixel, so for a RF of size \(n\), there are \((2^8)^n\) possible input patches. At each index corresponding to an input patch, there is an upscaled patch of size \(r^2\), whose entries are also each 8 bits (or 1 byte). Clearly, this will not scale well with \(n\), and in fact, \(n=4\) and \(r=2\) gives us a LUT that is 16GB large. To counteract this, the paper uses a “sampled LUT”, where only a subset of the possible input patches are sampled for use in the LUT; specifically, they found that sampling the color space of \(0\) to \(2^8-1\) uniformly (including endpoints) with a sampling interval of \(2^4\) over each pixel of the input patch worked well to reduce the size of the LUT while retaining good performance, reducing the 16GB LUT to just \((1+\frac{2^8}{2^4})^4 2^2 = 334084\) bytes, or about 326KB.</p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/lutsizes.png" alt="LUT sizes" /></p>
<p><em>Fig 2. A comparison of LUT sizes [3].</em></p>

<p>Of course, this sampled LUT introduces the issue of indexing: how do we index from the LUT if the input patch doesn’t match a patch that was seen while filling out the table? For this, the authors used interpolation; instead of indexing one upscaled patch, we can interpolate between up to \(2^n\) patches, for a RF size \(n\). For instance, for a LUT where \(n=2\), using a sampling interval of \(2^4\), getting the upscaled patch for an input patch where \(I_0 = 8, I_1 = 8\) would require interpolating between \(LUT[0][0], LUT[0][1], LUT[1][0], LUT[1][1]\). Using this sampling interval, these nearest points can actually be found easily by looking at the bit representation of \(I_j\): the first 4 bits, when converted to integer, will be the index of the lower part of the interpolation (e.g. for \(I_j = 20\), 20 \(\to\) 00010100, and then 0001 \(\to\) 1, meaning that \(LUT[1]\) will be the lower part of the interpolation and \(LUT[2]\) will be the upper part); this method of indexing for the interpolation points adds even further to the LUT’s speed. The issue of interpolating between points in \(n\) dimensional space is complex in its own right, though we note that the authors found that linear interpolation was slower than triangular interpolation (and its higher-dimensional couterparts), especially for larger \(n\).</p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/interpolation.png" alt="LUT sizes" /></p>
<p><em>Fig 3. A comparison of interpolation methods for \(n=2, 3, 4\) [3].</em></p>

<h4 id="rotational-ensemble">Rotational Ensemble</h4>

<p>Additionally, during training and testing, the authors employed the strategy of “rotational ensemble”. This is a method of data augmentation that involves:</p>

<ul>
  <li>
    <p>rotating each input by 90, 180, and 270 (and 0) degrees</p>
  </li>
  <li>
    <p>feeding each of 4 rotations of the input into the model</p>
  </li>
  <li>
    <p>rotating each the super-resolution outputs such that they are all at their original orientation</p>
  </li>
  <li>
    <p>taking the average over the 4 outputs, and treating that as your final output</p>
  </li>
</ul>

<p>This method is well-suited to super resolution, as the semantic meaning of the image is not important; images that are rotated still fall under our base assumption of what “real” images look like for super resolution, so the model can improve by optimizing over them. In our case, this method can actually improve the effective receptive field of the model substantially:</p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/rotensemble.png" alt="Rotational Ensemble Illustration" /></p>
<p><em>Fig 4. An illustration of the effect of rotational ensemble on the receptive field; a 2x2 RF is effectively increased to a 3x3 RF [3].</em></p>

<p>During training, rotational ensemble simply works well as a way to increase your dataset size. However, this method can be used in inference as well; we can construct 4 different super-resolution predictions using the LUT, and then average them together. For a 2x2 square RF, one can see, in the illustration above representing 2x upsampling, that the relative position of the patch in the output corresponding to the RF in the input will be determined by the relative position of the top-left pixel of the RF in the input image; that is, any RF in the input, no matter the rotation of, will correspond to the same patch in the output as long as the top-left pixel of the RF is the same across those RFs in the input. So, when we look at the RFs across our rotational ensemble when the top-left pixel is kept constant, we see that their union will be a 3x3 area in the input, which means that the given output patch will be influenced by all pixels in the 3x3 area, effectively increasing our inference RF size to 9.</p>

<p>While this does increase our computation time by a factor of 4 (discounting rotations and averaging), it is likely a worthy trade-off for such a substantial increase in RF size, which would normally require increasing the LUT size to an unmanageable degree.</p>

<h4 id="results-1">Results</h4>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/lut_comparison.png" alt="LUT Comparison Table" /></p>
<p><em>Fig 4. peak signal-to-noise ratio (PSNR) and runtime of various methods for 320 x 180 -&gt; 1280 x 720 upsampling [3].</em></p>

<p>There are 3 models mentioned in the paper: V, F, and S, which have a receptive field of 2, 3, and 4, respectively. F and S use a sampled LUT with a sampling interval of \(2^4\), while V uses a full LUT. We see that the LUT methods provide a good tradeoff in terms of speed vs reconstruction quality, where F is even faster than bicubic interpolation, and V is even faster than bilinear interpolation.</p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/lutqualitative.png" alt="LUT qualitative results" /></p>
<p><em>Fig 4. Qualitative comparison of results [3].</em></p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/lutquantitative.png" alt="LUT qualitative results" /></p>
<p><em>Fig 5. Quantitative comparison of results; runtimes for all methods besides sparse coding are on a Galaxy S7 phone [3].</em></p>

<p>We see that, quantitatively, all LUT variants perform substantially better than traditional interpolation upsampling methods, and are competitive with methods that require far greater runtime and/or storage space. Interestingly, there are some cases where the F model seems to perform quantitatively better than the S model, despite the fact that the only difference between them is that S has a larger receptive field. This likely just indicates a plateau in performance wrt receptive field size, or the differently shaped receptive fields may fare differently on different datasets (since RF=3 for F and RF=4 for S, the RF for F is a line and the RF for S is a square).</p>

<p>LUT methods are an interesting way to make super-resolution more efficient; instead of making a more efficient network, it avoids using a network for inference altogether. However, there are some obvious flaws: the quantitative results do not make it state-of-the-art in that regard, and the methodology of using the same LUT for each color channel independently seems somewhat unintuitive. Additionally, a given LUT is confined to a fixed receptive field size and upscaling factor; an entirely new SR network and table must be created to change these factors, which restricts the method’s practical use.</p>

<h4 id="im-lut">IM-LUT</h4>

<p>Since its invention in 2021, there have been numerous published papers for extensions and modifications to the LUT. One of these, published in 2025, is Interpolation Mixing LUT, or IM-LUT [5]. IM-LUT allows for the model to adapt to the scaling factor at inference time, meaning that a single IM-LUT can be used to scale any image to any size. This is achieved by first upscaling the input image with standard interpolation algorithms, such as bicubic upsampling, and then using a standard LUT on the upscaled image. However, the IM-LUT can also adapt its interpolation upscaling to the input image; a single IM-LUT uses many different interpolation algorithms to upscale the input image, and then uses a weighted average, with weights based on the image itself and the given scale factor, to combine the interpolations together before the final refinement LUT. With this, the IM-LUT can adapt to any scaling factor while still also dynamically adapting to the specific input image.</p>

<p>Now, we go through the structure of the IM-LUT</p>

<h5 id="im-net">IM-Net</h5>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/imnet.png" alt="IM Net" /></p>
<p><em>Fig 5. IM-Net, the network used to generate the LUTs for IM-LUT [3].</em></p>

<p>There are two overall parts to the IM-Net: weight map prediction, and image refinement. Image refinement is simply a network that refines an the interpolated input image; it takes an image of any given size, and produces an image of the same size. This part of the network is simple, but the weight maps are more involved:</p>

<p>The IM-LUT works with a fixed set of \(K\) interpolation functions, and for any given image, the network predicts per-pixel weights for each interpolation function; these weight maps allow the network to take advantage of the fact that different interpolation methods may fare better in different areas of a given image. The weight maps are a combination of the output of two other networks:</p>

<ul>
  <li>
    <p>An image-to-weight map network, that takes the low-resolution image and produces low-resolution weight maps.</p>
  </li>
  <li>
    <p>An upscaling factor-to-scale vector network (“scale modulator”), that takes the scaling factor as input and outputs a vector of per-interpolation function weights.</p>
  </li>
</ul>

<p>The weights from the scale modulator are multiplied by the initial weight maps (one scale weight per weight map) to produce the final weight maps. The networks optimize their predictions of these weight maps as so:</p>

<ul>
  <li>
    <p>For a given image (where the ground truth is known) and upscaling factor, downscale the image and then get interpolations back to the original size using each given interpolation function.</p>
  </li>
  <li>
    <p>Calculate per-interpolation deviations from the GT image using per-pixel L1 loss, i.e. 
\(E_k[i, j] = |I_{\text{interp } k}[i, j] - I_{\text{GT}}[i, j]|\) 
for \(k=1,...,K\).</p>
  </li>
  <li>
    <p>Compute ground-truth weight maps as a temperature-controlled per-pixel softmax over all negative interpolation deviations, i.e.
\(W^{GT}_k[i, j] = \frac{e^{-\beta E_k[i, j]}}{\sum_{\ell=1}^K e^{-\beta E_\ell[i, j]}}\) 
for \(\beta\) being the temperature constant.</p>
  </li>
  <li>
    <p>Compute the “guide” loss (or, the weight map loss) as the L2 reconstruction loss between the predicted and ground-truth weight maps, that is
\(\mathcal{L}_{\text{guide}} = \sum_{\ell=1}^K ||\hat W_\ell - W^{GT}_\ell||_2\)</p>
  </li>
</ul>

<p>Then, when the predicted weight maps are used to create a final interpolated image, and that is run through the refinement network, the final reconstruction loss can be found as 
\(\mathcal{L}_{\text{rec}} = ||\hat I - I^{GT}||_2\) 
and the final loss that we optimize over is 
\(\mathcal{L} = \mathcal{L}_{\text{rec}} + \lambda \mathcal{L}_{\text{guide}}\) 
where \(\lambda\) is our trade-off hyperparameter. With this, we can optimize both the refinement and the weight map predictions at once, and we can vary the upscaling factor to ensure that the network can adapt well to different amounts of upscaling.</p>

<h5 id="im-lut-1">IM-LUT</h5>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/imlut.png" alt="IM LUT" /></p>
<p><em>Fig 5. An overview of the way in which the LUTs are created and used [3].</em></p>

<p>Now, we have to transfer the IM-Net to LUTs. Given that IM-Net has 3 distinct sub-networks (scaling factor network, weight map network, and refinement network), we will have 3 LUTs; note that the refinement and weight map networks each use convolutional networks that leave each pixel of the output with a 2x2 receptive field in the input, as was seen with the original LUT. Additionally, each LUT is created with a sampling-interval scheme and interpolated indexing, as was also seen with the standard LUT (except for the scale LUT, which uses nearest neighbor interpolation). Specifically, the scale modulator LUT is indexed by a single number and outputs a vector of \(K\) weights, making it small and efficient; the weight map LUT is indexed by 4 pixel values and outputs \(K\) pixel values (since the weight map network <em>does not</em> perform any resizing), making it similar in size to the standard LUT; the refiner LUT is indexed by 4 pixels and outputs 1 pixel value (again, no resizing), making it smaller than the standard LUT. We see that, unlike the standard LUT, the size of the LUTs has no dependence on the scale factor; however, given that the refiner LUT will be used on the entire upscaled image, the overall computational complexity will increase with the scale factor (though, they note in the paper, it does not increase substantially with upscale factor).</p>

<p>Once the LUTs are computed, they can be used as stand-ins for the networks (as in the standard LUT), and then images can be upscaled as they were in IM-Net.</p>

<h5 id="results-2">Results</h5>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/imlutqualitative.png" alt="IM LUT qualitative" /></p>
<p><em>Fig 5. Qualitative results of IM-LUT compared to other methods [3].</em></p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/imlutresults.png" alt="IM LUT quantitative" /></p>
<p><em>Fig 5. Multiply-accumulate (MAC) operations, required storage, and PSNR for various super-resolution methods [3].</em></p>

<p>Given the freedom to choose interpolation functions, the authors highlight a few possible combinations of functions: in the table above, N means “nearest neighbor”, L means “bilinear”, C means “bicubic” and Z means “lanczos”, another kind of interpolation algorithm that uses windowed normalized sine functions; the letters, when put together, indicate that the given IM-LUT uses all of those interpolation functions. We see that the IM-LUT methods manage to be very efficient on storage and, for certain combinations of interpolations, on computations, as well; at the same time, they have much better performance than any single interpolation function, and, while not being state-of-the-art quantitatively, stay competitive with other LUT or similarly-efficient SR methods that require more storage and, in the case of the other LUT methods, cannot use a single model to adapt to different scaling factors.</p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/weightmaps.png" alt="IM LUT weight maps" /></p>
<p><em>Fig 5. Analysis of weight maps for different interpolation functions [3].</em></p>

<p>From the color-coded weight maps above, we see that the model is, in fact, using the weight maps to adapt to the input image. For instance, when nearest-neighbor and bilinear interpolation are used together, we see that the bilinear interpolation is prioritized in areas of low brightness, while the nearest neighbor method is used elsewhere. However, we also see that when nearest-neighbor, bilinear, and bicubic interpolation are used together, the nearest-neighbor interpolation has very little influence, indicating that one could even use the weight maps as a form of interpolation function selection, similar to how L1 regularization is used as variable selection in linear regression.</p>

<p>The IM-Net is already an interesting kind of reparameterization of the problem of learning-based super-resolution; instead of directly learning how to upscale an image, the network instead learns how to ensemble a set of given interpolation functions to do upscaling. This modular design allows for a great deal of flexibility in the model, as it can be combined with any arbitrary-scale upscaling method, even other learning-based methods, without any change to the core architecture. Of course, in this case, our real goal is a LUT-based method; in the realm of LUT-based methods, IM-LUT still stands out for its capability of arbitrary-scale super-resolution. Dynamically adapting to different scales will be a practical necessity for any super-resolution method, so it will be ineteresting to see if, as time goes on, efficient super-resolution methods will expand further in the direction of the IM-LUT, or if an entirely different architecture will rise to the top.</p>

<!-- To achieve this fast runtime, a convolutional SR network is trained with a small receptive field, since the size of the SR-LUT grows exponentially with the receptive field size. This limitation introduces an inherent trade-off between PSNR and runtime: increasing the receptive field can improve reconstruction quality, but it also causes the LUT to expand dramatically, leading to slower performance.   

Specifically, the SR-LUT grows exponentially as given by:   


$$ \text{LUT Size} = (2^8)^{RF} \times r^2 \times 8\text{ bits} $$

Example with RF = 2 and r = 4:

$$
\begin{align}
\text{LUT Size}
&= (2^8)^2 \times 4^2 \times 8\ \text{bits} \\
&= 256^2 \times 16 \times 8\ \text{bits} \\
&= 65{,}536 \times 16 \times 1\ \text{byte} \\
&= 1{,}048{,}576\ \text{bytes} \\
&= 1\ \text{MB}
\end{align}
$$

The LUT stores precomputed output values for every possible combination of input pixels in a receptive field. Its size depends on the number of input pixels considered and the range of values each pixel can take. The LUT must cover all possible input cases and store the corresponding outputs.

After training the SR model, an SR-LUT table is created based on the dimensions of the receptive field (e.g., a 4D SR-LUT for an RF size of 4). The output values from the trained model are computed and stored in the LUT. During inference, input values are used as indices into the LUT, and the corresponding output values are retrieved. This allows super-resolution to be performed using only the LUT, without running the original model.

Currently, LUTs only work for fixed-scale images, which limits their real-world applicability when images are zoomed in or out. Recent extensions, such as IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution, propose frameworks for arbitrary-scale SR tasks. These methods adapt to diverse image structures, providing super-resolution across arbitrary scales while maintaining the efficiency that LUTs offer. 


TODO: CIte IM-LUT -->

<!-- -adv: 
--made to be fast and small
-disadv: 
--practically, very small receptive field; lookup table grows exponentially(?) with RF size
--implicit assumptions about noise, blurring, downsampling of image based on training data -->

<h3 id="unfolding-networks">Unfolding Networks</h3>

<p>The “unfolding” in “unfolding network” refers to splitting up the problem of de-degredation into two distinct subproblems, that being (1) unblurring and upsampling, and (2) denoising. With this approach, one can show that problem (1) has a closed-form optimal solution that can explicitly adapt to specific given types of degradation with 0 learned parameters; this greatly reduces the burden on the learned portion of the network, which now only needs to do denoising. The method is a kind of fusing of model-based and learning-based approaches; despite involving a variant of a UNet, it is designed to be zero-shot adaptable to any kind of degradation that is parameterized by a known blurring kernel, downsampling factor, and noise level.</p>

<p>Now, we can go through and derive the method ourselves to illustrate how it arises:</p>

<p>The basic assumption will be that the input image for the method will be the blurred, downscaled, and additive white Gaussian noise-ed version of a ground-truth image, or in other words, our degraded image \(\vec y\) is</p>

\[\vec y = (\vec x \otimes \vec k)\downarrow_s + \vec N\]

<p>where:</p>
<ul>
  <li>\(\vec x \otimes \vec k\) represents the application of blurring kernel \(\vec k\) to ground-truth image \(\vec x\) (via convolution)</li>
  <li>\(\downarrow_s\) represents downsampling (decimation) by a factor of \(s\)</li>
  <li>\(\vec N \sim N(\vec 0, \sigma^2 I)\) for \(\sigma \in \mathbb{R}^+\)</li>
</ul>

<p>Given that \(\vec y \sim N((\vec x \otimes \vec k)\downarrow_s, \sigma^2 I)\), its PDF is</p>

\[P(\vec y | \vec x) = \frac 1 {(2\pi\sigma^2)^{\frac d 2}}e^{-\frac 1 {2\sigma^2}||(\vec x \otimes \vec k)\downarrow_s - \vec y||_2^2}\]

<p>Furthermore, it will be useful to have an image prior, which we will define as</p>

\[\Phi(\vec x) = -\log P(\vec x)\]

<p>which will stand as a measure of how “natural” an image \(\vec x\) is, ideally being minimised for any of our ground truth images and being maximised for images that are very noisy or unrealistic (i.e. unlike our GT images); we could interpret it simply as the negative logarithm of the a-priori probability distribution function of our ground truth images. In a practical sense, incorporating such a measure in our optimization will push our model toward creating images that conform to the patterns that typically appear in real images regarding color, brightness, etc, as opposed to overfitting on an image reconstruction loss function (in other words, a form of regularization).</p>

<p>Now, under a maximum a-posteriori (MAP) framework, our goal in super-resolution, given a degraded image \(\vec y\) conforming to the assumptions above, can be defined as finding</p>

\[\hat x = \arg \max_{\vec x} P(\vec x | \vec y) \\\]

<p>Or, in other words, the clean image that the degraded image most likely started as. From Bayes theorem, we have</p>

\[P(\vec x | \vec y) = \frac{P(\vec y | \vec x)P(\vec x)}{P(\vec y)} \propto P(\vec y | \vec x)P(\vec x)\]

<p>Since \(P(\vec y)\) is constant with regard to \(\vec x\). So now,</p>

\[\begin{aligned}
\hat x &amp;= \arg \max_{\vec x} P(\vec x | \vec y) \\
&amp;= \arg \max_{\vec x} P(\vec y | \vec x)P(\vec x) \\
&amp;= \arg \min_{\vec x} -\log(P(\vec y | \vec x)P(\vec x)) = \arg \min_{\vec x} -\log P(\vec y | \vec x) - \log P(\vec x) \\
&amp;= \arg \min_{\vec x} -(\log(\frac 1 {(2\pi\sigma^2)^{\frac d 2}}) - \frac 1 {2\sigma^2}||(\vec x \otimes \vec k)\downarrow_s - \vec y||_2^2) + \Phi(\vec x) \\
&amp;= \arg \min_{\vec x} \frac 1 {2\sigma^2}||(\vec x \otimes \vec k)\downarrow_s - \vec y||_2^2 + \Phi(\vec x)
\end{aligned}\]

<p>Additionally, for practical reasons, we often include a “trade-off” hyperparameter, which we represent with \(\lambda\), to balance the influence between our prior term \(\Phi\) and our data term 
\(||(\vec x \otimes \vec k)\downarrow_s - \vec y||_2^2\)
, so we have</p>

\[\hat x = \arg \min_{\vec x} \frac 1 {2\sigma^2}||(\vec x \otimes \vec k)\downarrow_s - \vec y||_2^2 + \lambda\Phi(\vec x)\]

<p>Instead of directly minimizing this expression with something like gradient descent, one can actually notice that 
\(\arg \min_{\vec x} ||(\vec x \otimes \vec k)\downarrow_s - \vec y||_2^2\)
 has a closed form solution (which we will detail later); so, to take advantage of this fact, we can decouple the data term and prior term in our optimization by using half-quadratic splitting, where we introduce an auxiliary variable \(\vec z\) into our optimization:</p>

\[\min_{\vec x, \vec z}\frac 1 {2\sigma^2}||(\vec z \otimes \vec k)\downarrow_s - \vec y||_2^2 + \lambda\Phi(\vec x) + \frac \mu 2 ||\vec x - \vec z||_2^2\]

<p>Now, \(\mu\) is a hyperparameter controlling how much leeway we want to give \(\vec x\) and \(\vec z\) to be different, where \(\mu \to +\infty\) recovers our original problem. Now, given that we have two optimization variables instead of just one, we can <em>unfold</em> the target and perform alternating iterative optimization (with \(j\) as the step number):</p>

\[\begin{aligned}
\vec z_j &amp;= \arg \min_{\vec z} \frac 1 {2\sigma^2}||(\vec z \otimes \vec k)\downarrow_s - \vec y||_2^2 + \frac \mu 2 ||\vec x_{j-1} - \vec z||_2^2 \\
\vec x_j &amp;= \arg \min_{\vec x} \lambda\Phi(\vec x) + \frac \mu 2 ||\vec x - \vec z_j||_2^2
\end{aligned}\]

<p>Now, it may be useful to change the value of \(\mu\) throughout our optimization; a small \(\mu\) near the start of our optimization will help speed up convergence, and a large \(\mu\) near the end will ensure that our solution actually corresponds to the original problem. So, define \(\mu_1, ..., \mu_J\) for an optimization of \(J\) steps, and let \(\alpha_j = \mu_j\sigma^2\), so we have</p>

\[\begin{aligned}
\vec z_j &amp;= \arg \min_{\vec z} ||(\vec z \otimes \vec k)\downarrow_s - \vec y||_2^2 + \alpha_j ||\vec x_{j-1} - \vec z||_2^2 \\
\vec x_j &amp;= \arg \min_{\vec x} \Phi(\vec x) + \frac {\mu_j} {2\lambda} ||\vec x - \vec z_j||_2^2
\end{aligned}\]

<p>Now, as alluded, \(\vec z_j\) actually still has a closed form solution:</p>

\[\vec z_j  = \mathcal{F}^{-1} \left(\frac{1}{\alpha_j}\left(d - \overline{\mathcal{F}(j)} \odot_s \frac{(\mathcal{F}(j)d) \Downarrow_s}{(\overline{\mathcal{F}(j)}\mathcal{F}(j))\Downarrow_s + \alpha_j} \right) \right)\]

<p>where</p>

\[d = \overline{\mathcal{F}(j)}\mathcal{F}(\vec y \uparrow_s) + \alpha_j \mathcal{F}(x_{j-1})\]

\[\alpha_j := \mu_j\sigma^2\]

<p>and \(\mathcal{F}( \centerdot )\) denotes the FFT and \(\odot_s\) is the distinct block processing operator with element-wise multiplication.
\(\Downarrow_s\) denotes the block downsampler and \(\uparrow_s\) denotes the upsampler.</p>

<p>The derivation of which is too long to include here, but is detailed in [4].</p>

<p>So, it remains to find 
\(\vec x_j = \arg \min_{\vec x} \Phi(\vec x) + \frac {\mu_j} {2\lambda} ||\vec x - \vec z_j||_2^2\). 
One can notice that this is similar to our very first optimization target; indeed, finding \(\vec x_j\) is equivalent to removing additive white Gaussian noise from \(\vec z_j\) with \(\sigma^2_\text{noise} = \frac {\lambda} {\mu_j}\) under a MAP framework. So, for convenience, we define 
\(\beta_j = \sqrt{\frac{\lambda}{\mu_j}}\)
 (the standard deviation of the Gaussian noise), and we have that finding \(x_j\) is equivalent to a Gaussian denoising problem with noise level \(\beta_j\) (or, \(\sigma^2_\text{noise} = \beta_j^2\)).</p>

<p>Given that this is a simple denoising task, we will opt for a denoising neural network. The paper in question uses a “ResUNet”, which is a UNet with added residual blocks, similar to those from a ResNet.</p>

<p style="max-width: 80%;"><img src="/CS163-Projects-2025Fall/assets/images/01/UNet.png" alt="Unfolding UNet" /></p>
<p><em>Fig 1. UNet architecture as depicted in  [6].</em></p>

<p>The network takes in the concatenated \(\vec z_k\) and noise level map and outputs the denoised image \(x_k\).
The ResUNet involves four scales in which 2x2 strided convolutions and 2x2 transposed convolutions are adopted and residual blocks are added during both
downscaling and upscaling.</p>

<p>In order to ensure adaptibility and nonblind-ness of our method, it is useful to incorporate the noise level \(\beta_j\) into the network input. The paper’s method for doing this is fairly simple: given an input image \(3 \times H \times W\), a constant matrix with size \(H \times W\) with all entries equal to \(\beta_j\) is appended on the channel dimension to create an input of shape \(4 \times H \times W\), which is fed into the network as normal.</p>

<p>[define the data and prior modules, talk about how prior module doesnt need to learn anything and can adapt to any kernel, sigma, and downsample factor (whcih is a good thing); talk about how removing implicit assumptions of kernel and noise levels and such makes the model more generalizable]</p>

<p>[talk about training process]</p>

<p>[talk about results]</p>

<p>[include more images somewhere]</p>

<!-- -adv: 
--nonblind; explicitly adapts for diff blurring kernels, amts of noise, and downsampling factors
--limits learned parameters by only using learning for denoising step; deblurring and upsampling is closed-form, but still included in the chain of the model to allow learning to take advantage of it
-disadv: 
--sequential design w/ conv nets and (inverse) FFTs may be slow? 
--use of only conv nets for denoising step may limit effective receptive field

-more classical method can better adapt to diff blurring kernel, etc without extensive training; balance of learning based and model based method is good idk -->

<h2 id="an-extension-of-the-hybrid-attention-transformer">An Extension of the Hybrid Attention Transformer</h2>

<h3 id="adaptive-sparse-transformer-ast">Adaptive Sparse Transformer (AST)</h3>

<p>this modifies the attention matrix AND the FFN after the values are computed from the self attention layer; potentially modifying a lot of parameters</p>

<h3 id="putting-it-together">Putting it Together</h3>

<p>idea: add AST to OCA and increase size of overlapping K/V windows, since the model can do a better job at filtering out any noise that it introduces; this part of the model is also like an overall filter for each RHAG block, which AST may be good for?</p>

<h3 id="results-3">Results</h3>

<h2 id="references">References</h2>

<p>[1] X. Chen, X. Wang, J. Zhou, Y. Qiao and C. Dong, “Activating More Pixels in Image Super-Resolution Transformer,” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 22367-22377, doi: 10.1109/CVPR52729.2023.02142.</p>

<p>[2] Zhang, Kai, et al. “Deep Unfolding Network for Image Super-Resolution.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 3217–3226.</p>

<p>[3] Jo, Younghyun, and Seon Joo Kim. “Practical Single-Image Super-Resolution Using Look-Up Table.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 691–700. doi:10.1109/CVPR46437.2021.00075.</p>

<p>[4] N. Zhao, Q. Wei, A. Basarab, N. Dobigeon, D. Kouamé and J. -Y. Tourneret, “Fast Single Image Super-Resolution Using a New Analytical Solution for ℓ2 – ℓ2 Problems,” in IEEE Transactions on Image Processing, vol. 25, no. 8, pp. 3683-3697, Aug. 2016</p>

<p>[5] Park, S., Lee, S., Jin, K., &amp; Jung, S.W. (2025). IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (pp. 14317-14325).</p>

<p>[6]  Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241.
Springer, 2015.</p>

<hr />

  </div>


  <!-- <div class="page-navigation">
    
      <a class="prev" href="/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html">&larr; Post Template</a>
    

    <!--  -->


  <!--comment-->
  
    <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-ucladeepvision-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

</article>

      </div>
    </main>

    <div style="clear: both;" />
<footer class="site-footer">
    2024 &copy; by UCLAdeepvision. All Rights Reserved. Built by <a href="https://jekyllrb.com/"
        target="_blank">Jekyll</a>

    <!-- <p>
        <a href="/CS163-Projects-2025Fall/feed.xml" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_rss.png" />
        </a>
        <a href="https://scholar.google.com/citations?user=dCa-pW8AAAAJ&hl=en&oi=ao" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_scholar.png" />
        </a>
        <a href="https://github.com/lilianweng" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_github.png" />
        </a>
    </p> -->
</footer>

  </body>

</html>
