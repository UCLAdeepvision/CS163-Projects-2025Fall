<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Project Track: Project 8 - Street-view Semantic Segmentation</title>

    <meta name="description" content="In this project, we delve into the topic of developing model to apply semantic segmentations on fine-grained urban structures based on pretrained Segformer m...">

    <meta content="2025F, UCLA CS163 Course Projects" property="og:site_name">
    
        <meta content="Project Track: Project 8 - Street-view Semantic Segmentation" property="og:title">
    
    
        <meta content="article" property="og:type">
    
    
        <meta content="In this project, we delve into the topic of developing model to apply semantic segmentations on fine-grained urban structures based on pretrained Segformer model. We explore 3 approaches to enhance the model performance, and analyze the result of each." property="og:description">
    
    
        <meta content="http://localhost:4000/2025/12/12/team02-streetviewseg.html" property="og:url">
    
<!--
    
        <meta content="Zach Liu, Tianze Zhao, Ruizhe Cheng, Ahmad Khan" property="article:author">
        <meta content="http://localhost:4000/about/" property="article:author">
     -->

    <!-- 
        <meta content="2025-12-12T00:00:00-08:00" property="article:published_time">
        <meta content="http://localhost:4000/about/" property="article:author">
    
    
    
        
     -->

    <link rel="shortcut icon" href="/CS163-Projects-2025Fall/assets/ucla_ico.jpg">
    <link rel="stylesheet" href="/CS163-Projects-2025Fall/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/CS163-Projects-2025Fall/2025/12/12/team02-streetviewseg.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <!-- <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-8161570-6', 'auto');
        ga('send', 'pageview');
    </script> -->

    <!-- For Facebook share button -->
    <!-- <div id="fb-root"></div>
    <script>
      (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));
    </script> -->

    <!-- Twitter cards -->
    <!-- <meta name="twitter:site"    content="@">
    <meta name="twitter:creator" content="@UCLAdeepvision">
    <meta name="twitter:title"   content="Project Track: Project 8 - Street-view Semantic Segmentation">

    
        <meta name="twitter:description" content="<blockquote>
  <p>In this project, we delve into the topic of developing model to apply semantic segmentations on fine-grained urban structures based on pretrained Segformer model. We explore 3 approaches to enhance the model performance, and analyze the result of each.</p>
</blockquote>

">
    

    
        <meta name="twitter:card"  content="summary">
        <meta name="twitter:image" content="">
     -->
    <!-- end of Twitter cards -->

</head>


  <body>

    <header class="site-header" role="banner" id='header-bar'>

    <div class="wrapper">
        
        <a class="site-title" style="color:#F2A900" href="/CS163-Projects-2025Fall/">2025F, UCLA CS163 Course Projects  </a>

        <!-- <nav class="site-nav">
            <a class="page-link" href="http://lilianweng.github.io" target="_blank">&#x1f349; About</a>
        </nav> -->
        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/about.html"> About</a>
        </nav>

        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/archive.html"> Archive</a>
        </nav>


        <!-- <nav class="site-nav">
            <a class="page-link" style="color:#FFD100" href="/CS163-Projects-2025Fall/FAQ.html"> FAQ</a>
        </nav> -->
        <!-- <nav class="site-nav">
            <a class="page-link" href="/CS163-Projects-2025Fall/log.html">&#x231b; Log</a>
        </nav> -->


    </div>

</header>


    <!-- Back to top button -->
    <script src="/CS163-Projects-2025Fall/assets/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop()</script>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Project Track: Project 8 - Street-view Semantic Segmentation</h1>
    <p class="post-meta">

      <time datetime="2025-12-12T00:00:00-08:00" itemprop="datePublished">
        
        Dec 12, 2025
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Zach Liu, Tianze Zhao, Ruizhe Cheng, Ahmad Khan</span>
      </span>

      <!-- <span>
        
      </span> -->
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2025/12/12/team02-streetviewseg.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>In this project, we delve into the topic of developing model to apply semantic segmentations on fine-grained urban structures based on pretrained Segformer model. We explore 3 approaches to enhance the model performance, and analyze the result of each.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a>    <ul>
      <li><a href="#dataset-split" id="markdown-toc-dataset-split">Dataset Split</a></li>
    </ul>
  </li>
  <li><a href="#model-segformer" id="markdown-toc-model-segformer">Model: Segformer</a></li>
  <li><a href="#evaluation-metrics" id="markdown-toc-evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#baseline-methods" id="markdown-toc-baseline-methods">Baseline Methods</a></li>
  <li><a href="#our-approach" id="markdown-toc-our-approach">Our Approach</a>    <ul>
      <li><a href="#approach-1---basnet-hybrid-loss" id="markdown-toc-approach-1---basnet-hybrid-loss">Approach 1 - BASNet Hybrid Loss</a></li>
      <li><a href="#approach-2---basnet-hybrid-loss--copy-paste-augmentation" id="markdown-toc-approach-2---basnet-hybrid-loss--copy-paste-augmentation">Approach 2 - BASNet Hybrid Loss + Copy-Paste Augmentation</a></li>
      <li><a href="#approach-3---ssim--lovasz-loss--copy-paste-augmentation" id="markdown-toc-approach-3---ssim--lovasz-loss--copy-paste-augmentation">Approach 3 - SSIM + Lovasz Loss + Copy-Paste Augmentation</a></li>
    </ul>
  </li>
  <li><a href="#results-and-analyses" id="markdown-toc-results-and-analyses">Results and Analyses</a>    <ul>
      <li><a href="#compare-to-baseline-model" id="markdown-toc-compare-to-baseline-model">Compare to Baseline Model</a></li>
      <li><a href="#approach-to-approach-comparison" id="markdown-toc-approach-to-approach-comparison">Approach-to-Approach Comparison</a></li>
      <li><a href="#visualization" id="markdown-toc-visualization">Visualization</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#basic-syntax" id="markdown-toc-basic-syntax">Basic Syntax</a>    <ul>
      <li><a href="#code-block" id="markdown-toc-code-block">Code Block</a></li>
      <li><a href="#more-markdown-syntax" id="markdown-toc-more-markdown-syntax">More Markdown Syntax</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>Understanding street-level scenes through segmentation is crucial to autonomous driving, urban mapping, and robot perception. And it is especially important when it comes to segment fine-grained urban structures because a lot of what makes a street safe, legal, and navigable lives in small, easily missed details. So, this naturally leads to the question we want to investigate in this project: How to improve the semantic segmentation performance on those fine-grained urban structures?</p>

<h2 id="dataset">Dataset</h2>
<p>Cityscapes is a large-scale, widely used dataset for understanding complex urban environments, featuring diverse street scenes from many cities with high-quality pixel-level annotations for tasks like semantic segmentation and instance segmentation. It contains 30 classes and many of them are considered to be fine-grained urban structures, thus this dataset is a perfect choice for this project.</p>

<p>We remapped all categories in cityscapes to a consistent six-class scheme - fence, car, vegetation, pole, traffic sign, and traffic light. All of the classes are fine-grained urban structures. We define an additional implicit background class which are default to ignore by setting their pixel values to zero.</p>
<h3 id="dataset-split">Dataset Split</h3>
<p>We partition the Cityscapes dataset into three subsets from training, validation, and testing. From the original Cityscapes training split, we sample 2000 images to form our training set, 250 images to form our validation set, and another 250 images to form our test set.</p>

<h2 id="model-segformer">Model: Segformer</h2>
<p>In this project, we build everything upon the SegFormer model.
Segformer is a transformer-based semantic segmentation model designed to be simple and accurate. It contains two main parts: encoder and decoder. 
The encoder is MiT (Mix Transformer), a hierarchical Transformer that produces 4 multi-scale feature maps. It uses overlapped patch embeddings and an efficient attention design [1].
The decoder is a lightweight All-MLP decoder. It linearly projects each of the 4 feature maps to the same channel size, upsamples them to the same resolution, concatenates and fuses them with an MLP, then outputs per-pixel class scores [1].
<img src="/CS163-Projects-2025Fall/assets/images/team02/segformerArch.png" alt="Segformer" style="width: 400px; max-width: 100%;" />
<em>Fig 1. Segformer Architecture, consists of a hierarchical Transformer encoder to extract coarse and fine features and a lightweight All-MLP decoder to fuse these multi-level features and predict the segmentation mask</em> [1].</p>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>
<p>We evaluate the performance of models using both per-class intersection over union (IoU) and mean intersection over union (mIoU). IoU measures the overlap between the predicted region and the ground truth bounding box. The equation of calculating IoU is given as follows:</p>

\[\mathrm{IoU}(A,B)=\frac{|A\cap B|}{|A\cup B|}\]

<p>Where:</p>
<ul>
  <li>\(A \cap B\) is the area (or volume) of the overlap between <strong>A</strong> and <strong>B</strong></li>
  <li>\(A \cup B\) is the area (or volume) covered by <strong>A</strong> or <strong>B</strong> (their union)</li>
</ul>

<p>Given <strong>C</strong> classes, the <strong>mean IoU (mIoU)</strong> is the average IoU across classes:</p>

\[\mathrm{mIoU} = \frac{1}{C}\sum_{c=1}^{C} \mathrm{IoU}_c\]

<p>The higher the IoU and mIoU value is, the better is the model performance.</p>

<h2 id="baseline-methods">Baseline Methods</h2>
<p>We fully fine-tuned a SegFormer-B0 segmentation model from a Cityscapes-fine-tuned checkpoint, with a newly initialized 7-class (six fine-grained urban structure classes + background) segmentation head, and we are going to use this fully-finetuned SegFormer-B0 as our baseline model. The reason is that due to limited computational resources, SegFormer-B0 is the most suitable starting point, and the original head and label set don’t match our remapped classes, and a fine-tuned baseline gives a strong, task-aligned reference so any gains can be attributed to our methods rather than simply training the model on the target data. The training setup is shown in the code below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_base = SegformerForSemanticSegmentation.from_pretrained(
    "nvidia/segformer-b0-finetuned-cityscapes-512-1024",
    num_labels=7,
    ignore_mismatched_sizes=True
)

training_args = TrainingArguments(
    output_dir="./segformer-thin-structures-v2",
    learning_rate=6e-5,
    num_train_epochs=15,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    save_total_limit=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=25,
    remove_unused_columns=False,
    dataloader_num_workers=8,
    fp16=torch.cuda.is_available(),
    gradient_accumulation_steps=1,
    report_to="none",
)

trainer = Trainer(
    model=model_base,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()
trainer.save_model("./segformer-thin-structures-v2-final")
</code></pre></div></div>
<p>All methods we explored in the project will use same set of training_args as shown above for better comparison.</p>

<p>The performance of the baseline model is shown in below:</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/finetune.png" alt="Finetune" style="width: 400px; max-width: 100%;" /></p>

<h2 id="our-approach">Our Approach</h2>
<h3 id="approach-1---basnet-hybrid-loss">Approach 1 - BASNet Hybrid Loss</h3>
<p>In our first approach we implement a <strong>boundary-aware supervision</strong> strategy designed to improve the geometric precision of the baseline model without altering its underlying architecture. While standard semantic segmentation relies on pixel-wise classification, a “Boundary-Aware” mechanism redefines the optimization objective to prioritize structural fidelity.
Inspired by Boundary-Aware Segmentation Network (BASNet), we achieve the boundary-aware supervision by adopting the hybrid loss, which is proposed in the BASNet, that combines three distinct supervisory signals to train the SegFormer. The three types of losses are described below:</p>

<ol>
  <li>
    <p><strong>Structural Similarity (SSIM):</strong> Unlike pixel-wise losses that treat neighbors as independent, SSIM evaluates the structural information within a local sliding window. By using Gaussian-weighted convolutions (<code class="language-plaintext highlighter-rouge">F.conv2d</code>), it penalizes predictions where the local variance—representing texture and edges—does not match the ground truth. This effectively forces the model to sharpen boundaries around objects. It has the following mathematical form:</p>

\[\ell_{ssim} = 1 - \frac{(2\mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}\]

    <p>where \(\mu_x\), \(\mu_y\) are the mean for x and y, and \(\sigma_x\), \(\sigma_y\) are the standard deviations for x and y, \(\sigma_{xy}\) is their covariance, \(C_1 = 0.01^2\) and \(C_2 = 0.03^2\) are used to avoid dividing by zero [2].</p>
  </li>
  <li>
    <p><strong>Multi-Class IoU Loss:</strong> This component optimizes the Jaccard Index directly. It aggregates softmax probabilities across the entire image to calculate the intersection and union for each class. This creates a global gradient that rewards the correct <em>extent</em> and <em>shape</em> of the predicted region, preventing the model from generating fragmented or “shattered” masks. It has the following mathematical form:</p>

\[\ell_{iou} = 1 - \frac{\sum_{r=1}^{H} \sum_{c=1}^{W} S(r,c) G(r,c)}{\sum_{r=1}^{H} \sum_{c=1}^{W} \left[ S(r,c) + G(r,c) - S(r,c) G(r,c) \right]}\]

    <p>where G(r,c) is the ground truth label of the pixel (r,c) and S(r,c) is the predicted probability [2].</p>
  </li>
  <li>
    <p><strong>Cross-Entropy (CE):</strong> We retain the standard Cross-Entropy loss to anchor the pixel-level class fidelity, ensuring the semantic categorization remains accurate while SSIM and IoU refine the geometry.</p>
  </li>
</ol>

<p>Build upon the original hybrid loss proposed in the BASNet, we combine the three types of losses in a weighted way, so we can have more flexibility.</p>

\[\ell_{hybrid} = \lambda_{ce} \cdot \ell_{ce} + \lambda_{ssim} \cdot \ell_{ssim} + \lambda_{iou} \cdot \ell_{iou}\]

<p>By defining this new loss function we shifted from a purely semantic focus to a hybrid focus, and more focus is expected to be shifted to fine-grained things in the image because such loss is architecturally biased toward boundaries and those fine-grained urban structures have small interiors relative to their boundary.
The hybrid loss implementation is shown below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class SSIM(nn.Module):
    def __init__(self, window_size=11, size_average=True, return_loss=True):
        super().__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.return_loss = return_loss
        self.channel = 1
        self.register_buffer('window', self._create_window(window_size, self.channel))

    def _gaussian(self, window_size, sigma):
        coords = torch.arange(window_size, dtype=torch.float32) - window_size // 2
        gauss = torch.exp(-coords**2 / (2 * sigma**2))
        return gauss / gauss.sum()

    def _create_window(self, window_size, channel):
        _1D_window = self._gaussian(window_size, 1.5).unsqueeze(1)
        _2D_window = _1D_window.mm(_1D_window.t()).unsqueeze(0).unsqueeze(0)
        return _2D_window.expand(channel, 1, window_size, window_size).contiguous()

    def forward(self, img1, img2):
        _, channel, _, _ = img1.size()

        # Recreate window if channel count changed
        if channel != self.channel:
            self.channel = channel
            self.window = self._create_window(self.window_size, channel).to(img1.device, img1.dtype)

        pad = self.window_size // 2

        # Apply reflection padding to avoid artificial edges
        img1_padded = F.pad(img1, (pad, pad, pad, pad), mode='reflect')
        img2_padded = F.pad(img2, (pad, pad, pad, pad), mode='reflect')

        # Compute means
        mu1 = F.conv2d(img1_padded, self.window, padding=0, groups=channel)
        mu2 = F.conv2d(img2_padded, self.window, padding=0, groups=channel)

        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2

        # Compute variances and covariance
        sigma1_sq = F.conv2d(img1_padded * img1_padded, self.window, padding=0, groups=channel) - mu1_sq
        sigma2_sq = F.conv2d(img2_padded * img2_padded, self.window, padding=0, groups=channel) - mu2_sq
        sigma12 = F.conv2d(img1_padded * img2_padded, self.window, padding=0, groups=channel) - mu1_mu2

        # Clamp for numerical stability
        sigma1_sq = torch.clamp(sigma1_sq, min=0)
        sigma2_sq = torch.clamp(sigma2_sq, min=0)

        # Stability constants
        C1 = 0.01 ** 2
        C2 = 0.03 ** 2

        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
                   ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

        if self.size_average:
            ssim_val = ssim_map.mean()
        else:
            ssim_val = ssim_map.mean(dim=[1, 2, 3])

        return 1 - ssim_val if self.return_loss else ssim_val


class MultiClassIoULoss(nn.Module):
    def __init__(self, num_classes=7, smooth=1e-6):
        super().__init__()
        self.num_classes = num_classes
        self.smooth = smooth

    def forward(self, pred_softmax, target_onehot):
        intersection = (pred_softmax * target_onehot).sum(dim=(2, 3))
        union = pred_softmax.sum(dim=(2, 3)) + target_onehot.sum(dim=(2, 3)) - intersection

        iou = (intersection + self.smooth) / (union + self.smooth)

        #1 - mIoU as loss
        return 1 - iou.mean()


class HybridLoss(nn.Module):
    def __init__(self, num_classes=7, ce_weight=1.0, ssim_weight=1.0, iou_weight=1.0):
        super().__init__()
        self.num_classes = num_classes
        self.ce_weight = ce_weight
        self.ssim_weight = ssim_weight
        self.iou_weight = iou_weight

        self.ce_loss = nn.CrossEntropyLoss(ignore_index=255)
        self.ssim_module = SSIM(window_size=11, size_average=True)
        self.iou_loss = MultiClassIoULoss(num_classes=num_classes)

    def forward(self, logits, labels):
        # Upsample logits to match label size
        if logits.shape[-2:] != labels.shape[-2:]:
            logits = F.interpolate(logits, size=labels.shape[-2:],
                                   mode='bilinear', align_corners=False)

        #Cross-Entropy Loss
        ce_out = self.ce_loss(logits, labels)

        pred_softmax = F.softmax(logits, dim=1)

        #one-hot encoding for valid pixels
        valid_mask = (labels != 255)
        labels_for_onehot = labels.clone()
        labels_for_onehot[~valid_mask] = 0

        target_onehot = F.one_hot(labels_for_onehot, num_classes=self.num_classes)
        target_onehot = target_onehot.permute(0, 3, 1, 2).float()

        # Mask out invalid pixels
        valid_mask_expanded = valid_mask.unsqueeze(1).float()
        target_onehot = target_onehot * valid_mask_expanded
        pred_softmax_masked = pred_softmax * valid_mask_expanded

        #SSIM Loss (boundary-aware component)
        #ssim_out = 1 - self.ssim_module(pred_softmax_masked, target_onehot)
        ssim_out = self.ssim_module(pred_softmax_masked, target_onehot)

        #IoU Loss
        iou_out = self.iou_loss(pred_softmax_masked, target_onehot)

        # Combine losses
        total_loss = (self.ce_weight * ce_out +
                      self.ssim_weight * ssim_out +
                      self.iou_weight * iou_out)

        return total_loss, {
            'ce_loss': ce_out.item(),
            'ssim_loss': ssim_out.item(),
            'iou_loss': iou_out.item()
        }
</code></pre></div></div>

<p>To adopt this new loss function, we implement a new ‘BATrainer’ based on the original Trainer provided by the HuggingFace</p>

<p>The implementation is shown below</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class BATrainer(Trainer):
    def __init__(self, *args, ce_weight=1.0, ssim_weight=1.0, iou_weight=1.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.hybrid_loss = HybridLoss(
            num_classes=7,
            ce_weight=ce_weight,
            ssim_weight=ssim_weight,
            iou_weight=iou_weight
        )

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        loss, loss_components = self.hybrid_loss(logits, labels)

        return (loss, outputs) if return_outputs else loss
</code></pre></div></div>

<p>Then, we initialize another pretrained SegFormer-B0 model, and use the same training_arg as the one we used for our baseline model training and test the per-class IoU and the mIoU across all classes</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_bas = SegformerForSemanticSegmentation.from_pretrained(
    "nvidia/segformer-b0-finetuned-cityscapes-512-1024",
    num_labels=7,
    ignore_mismatched_sizes=True
)

training_args = TrainingArguments(
    output_dir="./segformer-thin-structures-ba",
    learning_rate=6e-5,
    num_train_epochs=15,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    save_total_limit=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=25,
    remove_unused_columns=False,
    dataloader_num_workers=8,
    fp16=torch.cuda.is_available(),
    gradient_accumulation_steps=1,
    report_to="none",
)

trainer = BATrainer(
    model=model_bas,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    ce_weight=1,
    ssim_weight=0.4,
    iou_weight=0.5,
)

trainer.train()
trainer.save_model("./segformer-thin-structures-ba-final")
</code></pre></div></div>

<h3 id="approach-2---basnet-hybrid-loss--copy-paste-augmentation">Approach 2 - BASNet Hybrid Loss + Copy-Paste Augmentation</h3>
<p>Building upon the structural precision achieved by our Boundary-Aware (BA) approach, we further introduce <strong>a Copy-Paste Augmentation</strong> strategy that fundamentally alters the training data distribution. While the BA method refines <em>how</em> the model learns, this augmentation strategy refines <em>what</em> the model sees during training.</p>

<p>The motivation for this copy-paste augmentation is that: While the Boundary-Aware (BA) method successfully mitigates over-smoothing by sharpening object edges, it remains constrained by the inherent class imbalance present in the original dataset. In typical street-view scenes, safety-critical objects such as traffic poles, distant lights, and road signs constitute only a small fraction of total pixels relative to dominant classes including road, sky, and building surfaces. Consequently, even a boundary-focused model may struggle to detect these objects due to their limited representation in the training data. To directly address this “long-tail” distribution problem without incurring the cost of manual data collection, we employ Copy-Paste augmentation. By synthetically transplanting instances of rare classes onto diverse background contexts, we encourage the model to decouple objects from their typical environmental associations and recognize them based on intrinsic visual features rather than contextual priors. This augmentation strategy primarily targets <strong>recall</strong>: whereas the BA method improves the <em>quality</em> of detected object boundaries, Copy-Paste augmentation improves the <em>likelihood</em> of successful detection by exposing the model to a high frequency of rare, spatially sparse structures throughout training.</p>

<p>The core mechanism relies on an object-level augmentation technique that synthesizes new training samples dynamically. For each target image, the system randomly selects a source image from the dataset and identifies specific object classes within it. The identified objects are extracted and optionally transformed through geometric operations including resizing and horizontal flipping to introduce additional variance. These extracted objects are then composited onto the target image using alpha-blending, and the corresponding segmentation masks are updated simultaneously to reflect the newly transplanted objects. By combining extracted objects with realistic textures in a single image, this process creates training samples with increased object density and complexity. Importantly, the model is trained on these augmented samples using the same Hybrid Boundary-Aware loss function (SSIM + IoU + CE) described previously. This integrated approach ensures that the model not only encounters rare objects more frequently but also learns their precise boundaries with high fidelity.</p>

<p>Then, we initialize another pretrained SegFormer-B0 model model_bas_aug, and use the same training_arg as the one we used for our baseline model training and test the per-class IoU and the mIoU across all classes</p>

<p>The copy-paste augmentation implementation is demonstrated in the code below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class CopyPasteAugmentation:
    def __init__(self, dataset, paste_prob=0.5):
        self.dataset = dataset
        self.paste_prob = paste_prob

    def __call__(self, image, label):
        if random.random() &gt; self.paste_prob:
            return image, label

        # Get random source image
        source_idx = random.randint(0, len(self.dataset) - 1)
        source_example = self.dataset[source_idx]
        source_image = source_example['image'].convert('RGB')
        source_label = np.array(source_example['semantic_segmentation'])
        if len(source_label.shape) == 3:
            source_label = source_label[:, :, 0]
        source_label = remap_labels(source_label)

        # Resize source to match main image
        source_image = source_image.resize(image.size, Image.BILINEAR)
        source_label = np.array(Image.fromarray(source_label.astype(np.uint8)).resize(
            image.size, Image.NEAREST))

        # Random horizontal flip
        if random.random() &gt; 0.5:
            source_image = source_image.transpose(Image.FLIP_LEFT_RIGHT)
            source_label = np.fliplr(source_label).copy()

        # Select random subset of classes to paste
        available_classes = [c for c in range(1, 7) if c in source_label]
        if not available_classes:
            return image, label

        num_to_paste = random.randint(1, len(available_classes))
        classes_to_paste = random.sample(available_classes, num_to_paste)

        # Create mask and blend: I1 × α + I2 × (1 − α)
        alpha = np.isin(source_label, classes_to_paste).astype(np.float32)
        alpha_3d = alpha[:, :, np.newaxis]

        image_np = np.array(image).astype(np.float32)
        source_np = np.array(source_image).astype(np.float32)

        new_image_np = source_np * alpha_3d + image_np * (1 - alpha_3d)
        new_image = Image.fromarray(new_image_np.astype(np.uint8))

        # Update labels
        new_label = np.where(alpha &gt; 0, source_label, label)

        return new_image, new_label

class CopyPasteDataset(torch.utils.data.Dataset):
    def __init__(self, hf_dataset, processor, copy_paste_aug=None, target_size=(1024, 512)):
        self.hf_dataset = hf_dataset
        self.processor = processor
        self.copy_paste_aug = copy_paste_aug
        self.target_size = target_size

    def __len__(self):
        return len(self.hf_dataset)

    def __getitem__(self, idx):
        example = self.hf_dataset[idx]
        image = example['image'].convert('RGB')
        label = example['semantic_segmentation']

        image = image.resize(self.target_size, Image.BILINEAR)
        label = label.resize(self.target_size, Image.NEAREST)

        label = np.array(label)
        if len(label.shape) == 3:
            label = label[:, :, 0]
        label = remap_labels(label)

        if self.copy_paste_aug is not None:
            image, label = self.copy_paste_aug(image, label)

        inputs = self.processor(images=image, return_tensors="pt")
        inputs = {k: v.squeeze(0) for k, v in inputs.items()}
        inputs['labels'] = torch.tensor(label, dtype=torch.long)

        return inputs
</code></pre></div></div>

<p>Generally, this is the training flow: Boundary-aware Segformer-B0 + copy-paste augmented train split → BA loss training → save <code class="language-plaintext highlighter-rouge">segformer-thin-structures-ba_aug-final</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_bas_aug = SegformerForSemanticSegmentation.from_pretrained(
    "nvidia/segformer-b0-finetuned-cityscapes-512-1024",
    num_labels=7,
    ignore_mismatched_sizes=True
)

training_args = TrainingArguments(
    output_dir="./segformer-thin-structures-ba_aug",
    learning_rate=6e-5,
    num_train_epochs=15,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    save_total_limit=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=25,
    remove_unused_columns=False,
    dataloader_num_workers=12,
    fp16=torch.cuda.is_available(),
    gradient_accumulation_steps=1,
    report_to="none",
)

trainer = BATrainer(
    model=model_bas_aug,
    args=training_args,
    train_dataset=train_dataset_aug,
    eval_dataset=val_dataset,
    ce_weight=1,
    ssim_weight=0.4,
    iou_weight=0.5,
)

trainer.train()
trainer.save_model("./segformer-thin-structures-ba_aug-final")
</code></pre></div></div>

<h3 id="approach-3---ssim--lovasz-loss--copy-paste-augmentation">Approach 3 - SSIM + Lovasz Loss + Copy-Paste Augmentation</h3>

<h2 id="results-and-analyses">Results and Analyses</h2>
<p>After all training and evaluations are done, we print the result, and compare it with the baseline model performance (The fully-finetuned SegFormer-B0 model). The results are shown below.</p>

<p>Approach 1 - BASNet Hybrid Loss:</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/BAS.png" alt="BAS" style="width: 400px; max-width: 100%;" /></p>

<p>Approach 2 - BASNet Hybrid Loss + CopyPaste Augmentation:</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/Aug.png" alt="Aug" style="width: 400px; max-width: 100%;" /></p>

<p>Approach 3 - SSIM Loss + Lovasz Loss + CopyPaste Augmentation:</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/SLC.png" alt="SLC" style="width: 400px; max-width: 100%;" /></p>

<h3 id="compare-to-baseline-model">Compare to Baseline Model</h3>
<p>By comparing the results of all three approaches to the baseline model, we can find that the per-class IoU scores on fence, pole, traffic sign, and traffic light gets increased, and the overall mIoU scores also get increased in all three approaches. However, the performance on both the car and vegetation classes get degraded slightly. Before we start to analyze, there is something we need to know at first: although still counted as fine-grained urban structures (from project description), car and vegetation classes generally have relatively larger sizes compare to other 4 classes. Then it becomes clear to analyze performance degradation issue on these two classes. The reason this will happen for all approaches are likely the same: the SSIM loss, IoU loss, and Losvasz loss all create gradient imbalance - smaller objects can receive larger gradient from these three types of losses, and the larger objects can become suboptimal. The performance on those larger object did not degrade too much because they are well protected by the original cross-entropy loss. So this is the reason why we are getting better performance on very tiny or thin fine-grained urban structures like poles, traffic signs, etc. while getting slightly worse performance on relatively larger objects like cars
Overall, we get better mIoU score for all three approaches because mIoU is the average score, we just win more than we lose. Though all 6 classes are counted as the fine-grained classes, but the performance on those really small objects get improved more than the performance on those relatively larger objects get worsened, which proves that the change-of-loss strategy for boundary-aware supervision and the augmentation technique applied are valid ways to improve the performance semantic segmentations on fine-grained urban structures.</p>

<h3 id="approach-to-approach-comparison">Approach-to-Approach Comparison</h3>
<p>When we compare the three approaches from each other, we can observe that the approach 2, which is BASNet hybrid loss + copy-paste augmentation generally have better performance compare to the other two approaches, both per-class IoU and mIoU. The only class the approach did not win the other two is the fence class. In approach 1, which contains only BASNet hybrid loss has better per-class IoU than approach 2 for fence class. The reason is likely that copy-paste augmentation works well for self-contained objects, but fence is fundamentally different - it has a mesh grid structure and has high context dependency compare to other classes, and in copy-paste augmentation, we create binary masks that will treat fence as a solide blob, destroying the see-through mesh pattern, and pastes it in random locations where fences never natually appear, destroying the spatial context
In general, we can say that the copy-paste augmentation can help to improve the model performance because it introduces variability (good for generalization), the only downside is that the variabiliy is artificial and includes unrealistic placements that add noise, so we did not get a very huge benefits from the augmentation.</p>

<h3 id="visualization">Visualization</h3>
<p>In this part, we provide two example visualizations of the Approach 2 Model (which is the BASNet hybrid loss + copy-paste augmentation approach) and compare it with the baseline model (fully-finetuned SegFormer)</p>

<p>First visualization:</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/a2.png" alt="Vis2" style="width: 800px; max-width: 100%;" />
    <em>Fig 2. Example Visualization 1 of Baseline Model</em>.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/a1.png" alt="Vis1" style="width: 800px; max-width: 100%;" />
    <em>Fig 3. Example Visualization 1 of Approach 2 Model</em>.</p>

<p>Second visualization:</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/a4.png" alt="Vis3" style="width: 800px; max-width: 100%;" />
    <em>Fig 4. Example Visualization 2 of Baseline Model</em>.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team02/a3.png" alt="Vis3" style="width: 800px; max-width: 100%;" />
    <em>Fig 5. Example Visualization 2 of Approach 2 Model</em>.</p>

<p>From the example visualizations above we can see that the approach 2 is indeed improving the performance of the semantic segmentation task on fine-grained urban structures</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this project, we investigated methods to improve semantic segmentation performance on fine-grained urban structures using the SegFormer architecture. Starting from a fully fine-tuned SegFormer-B0 baseline, we explored three approaches that combined boundary-aware loss functions with data augmentation techniques.
Our experiments demonstrated that boundary-aware supervision—achieved through hybrid losses incorporating SSIM and IoU/Lovász components—consistently improves segmentation quality for small, thin urban structures such as poles, traffic signs, and traffic lights. All three approaches outperformed the baseline in terms of overall mIoU, with Approach 2 (BASNet Hybrid Loss + Copy-Paste Augmentation) achieving the best balance across most classes.
However, our results also revealed an inherent trade-off: the boundary-focused losses that benefit smaller objects can lead to slight performance degradation on larger structures like cars and vegetation due to gradient imbalance. This suggests that future work could explore class-aware loss weighting or adaptive loss scheduling to maintain performance across objects of varying scales.
We also observed that copy-paste augmentation, while generally beneficial for improving generalization, has limitations for context-dependent structures like fences. Objects with mesh-like patterns or strong spatial dependencies may require more sophisticated augmentation strategies that preserve their structural characteristics.
In summary, our findings confirm that shifting from purely pixel-wise supervision toward hybrid, boundary-aware objectives is a valid and effective strategy for improving segmentation of fine-grained urban structures—a capability critical for autonomous driving, urban mapping, and robotic perception systems. Future work could explore class-adaptive loss weighting, context-aware augmentation techniques, and scaling these methods to larger SegFormer variants to further improve performance across all object sizes.</p>

<h2 id="basic-syntax">Basic Syntax</h2>
<h3 id="code-block">Code Block</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample code block
import torch
print (torch.__version__)
</code></pre></div></div>

<h3 id="more-markdown-syntax">More Markdown Syntax</h3>
<p>You can find more Markdown syntax at <a href="https://www.markdownguide.org/basic-syntax/">this page</a>.</p>

<h2 id="reference">Reference</h2>
<p>[1] Xie, Enze, et al. “SegFormer: Simple and efficient design for semantic segmentation with transformers.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2021.</p>

<p>[2] Qin, Xuebin, et al. “BASNet: Boundary-aware salient object detection.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2019.</p>

<p>[3] Ghiasi, Golnaz, et al. “Simple Copy-Paste is a strong data augmentation method for instance segmentation.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2021.</p>

<p>[4] Berman, Maxim, et al. “The Lovasz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2018.</p>

<hr />

  </div>


  <!-- <div class="page-navigation">
    
      <a class="prev" href="/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html">&larr; Post Template</a>
    

    <!--  -->


  <!--comment-->
  
    <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-ucladeepvision-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

</article>

      </div>
    </main>

    <div style="clear: both;" />
<footer class="site-footer">
    2024 &copy; by UCLAdeepvision. All Rights Reserved. Built by <a href="https://jekyllrb.com/"
        target="_blank">Jekyll</a>

    <!-- <p>
        <a href="/CS163-Projects-2025Fall/feed.xml" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_rss.png" />
        </a>
        <a href="https://scholar.google.com/citations?user=dCa-pW8AAAAJ&hl=en&oi=ao" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_scholar.png" />
        </a>
        <a href="https://github.com/lilianweng" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_github.png" />
        </a>
    </p> -->
</footer>

  </body>

</html>
