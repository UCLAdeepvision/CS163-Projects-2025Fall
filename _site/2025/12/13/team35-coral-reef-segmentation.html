<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Semantic Segmentation of Coral Reefs: Evaluating SegFormer, DeepLab, and SAM3</title>

    <meta name="description" content="[Project Track: Project 1] Deep learning has become a standard tool for dense prediction in environmental monitoring. Inthis project, we focus on semantic se...">

    <meta content="2025F, UCLA CS163 Course Projects" property="og:site_name">
    
        <meta content="Semantic Segmentation of Coral Reefs: Evaluating SegFormer, DeepLab, and SAM3" property="og:title">
    
    
        <meta content="article" property="og:type">
    
    
        <meta content="[Project Track: Project 1] Deep learning has become a standard tool for dense prediction in environmental monitoring. Inthis project, we focus on semantic segmentation of coral reef imagery using the CoralScapes dataset. Starting from a pretrained SegFormer-B5 model, we design a coral-specific training pipeline that combines tiling, augmentations, and a..." property="og:description">
    
    
        <meta content="http://0.0.0.0:4000/2025/12/13/team35-coral-reef-segmentation.html" property="og:url">
    
<!--
    
        <meta content="Team 35 - Cara Burgess, Maria Campo Martins, Kevin Valencia" property="article:author">
        <meta content="http://0.0.0.0:4000/about/" property="article:author">
     -->

    <!-- 
        <meta content="2025-12-13T00:00:00+00:00" property="article:published_time">
        <meta content="http://0.0.0.0:4000/about/" property="article:author">
    
    
    
        
     -->

    <link rel="shortcut icon" href="/CS163-Projects-2025Fall/assets/ucla_ico.jpg">
    <link rel="stylesheet" href="/CS163-Projects-2025Fall/assets/css/main.css">
    <link rel="canonical" href="http://0.0.0.0:4000/CS163-Projects-2025Fall/2025/12/13/team35-coral-reef-segmentation.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <!-- <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-8161570-6', 'auto');
        ga('send', 'pageview');
    </script> -->

    <!-- For Facebook share button -->
    <!-- <div id="fb-root"></div>
    <script>
      (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));
    </script> -->

    <!-- Twitter cards -->
    <!-- <meta name="twitter:site"    content="@">
    <meta name="twitter:creator" content="@UCLAdeepvision">
    <meta name="twitter:title"   content="Semantic Segmentation of Coral Reefs: Evaluating SegFormer, DeepLab, and SAM3">

    
        <meta name="twitter:description" content="<blockquote>
  <p>[Project Track: Project 1] Deep learning has become a standard tool for dense prediction in environmental monitoring. Inthis project, we focus on semantic segmentation of coral reef imagery using the CoralScapes dataset. Starting from a pretrained SegFormer-B5 model, we design a coral-specific training pipeline that combines tiling, augmentations, and a CE+Dice loss. This yields a modest but consistent improvement in mIoU and qualitative boundary sharpness over the original checkpoint. We also run exploratory experiments with DeepLabv3 and SAM3, and discuss practical limitations due to the absence of coral-specific pretraining and limited compute.</p>
</blockquote>

">
    

    
        <meta name="twitter:card"  content="summary">
        <meta name="twitter:image" content="">
     -->
    <!-- end of Twitter cards -->

</head>


  <body>

    <header class="site-header" role="banner" id='header-bar'>

    <div class="wrapper">
        
        <a class="site-title" style="color:#F2A900" href="/CS163-Projects-2025Fall/">2025F, UCLA CS163 Course Projects  </a>

        <!-- <nav class="site-nav">
            <a class="page-link" href="http://lilianweng.github.io" target="_blank">&#x1f349; About</a>
        </nav> -->
        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/about.html"> About</a>
        </nav>

        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/archive.html"> Archive</a>
        </nav>


        <!-- <nav class="site-nav">
            <a class="page-link" style="color:#FFD100" href="/CS163-Projects-2025Fall/FAQ.html"> FAQ</a>
        </nav> -->
        <!-- <nav class="site-nav">
            <a class="page-link" href="/CS163-Projects-2025Fall/log.html">&#x231b; Log</a>
        </nav> -->


    </div>

</header>


    <!-- Back to top button -->
    <script src="/CS163-Projects-2025Fall/assets/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop()</script>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Semantic Segmentation of Coral Reefs: Evaluating SegFormer, DeepLab, and SAM3</h1>
    <p class="post-meta">

      <time datetime="2025-12-13T00:00:00+00:00" itemprop="datePublished">
        
        Dec 13, 2025
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Team 35 - Cara Burgess, Maria Campo Martins, Kevin Valencia</span>
      </span>

      <!-- <span>
        
      </span> -->
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2025/12/13/team35-coral-reef-segmentation.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>[Project Track: Project 1] Deep learning has become a standard tool for dense prediction in environmental monitoring. Inthis project, we focus on semantic segmentation of coral reef imagery using the CoralScapes dataset. Starting from a pretrained SegFormer-B5 model, we design a coral-specific training pipeline that combines tiling, augmentations, and a CE+Dice loss. This yields a modest but consistent improvement in mIoU and qualitative boundary sharpness over the original checkpoint. We also run exploratory experiments with DeepLabv3 and SAM3, and discuss practical limitations due to the absence of coral-specific pretraining and limited compute.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#main-content" id="markdown-toc-main-content">Main Content</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a>    <ul>
      <li><a href="#full-results-qualitative" id="markdown-toc-full-results-qualitative">Full Results (Qualitative)</a></li>
      <li><a href="#full-results-quantitative" id="markdown-toc-full-results-quantitative">Full Results (Quantitative)</a></li>
    </ul>
  </li>
  <li><a href="#models-and-experiments" id="markdown-toc-models-and-experiments">Models and Experiments</a>    <ul>
      <li><a href="#segformer-b5-released-coralscapes-checkpoint" id="markdown-toc-segformer-b5-released-coralscapes-checkpoint">SegFormer-B5 (Released CoralScapes Checkpoint)</a></li>
      <li><a href="#segformer-fine-tuning-our-short-budget-adaptation" id="markdown-toc-segformer-fine-tuning-our-short-budget-adaptation">SegFormer Fine-Tuning (Our Short-Budget Adaptation)</a></li>
      <li><a href="#deeplabv3-fine-tuning-resnet-50" id="markdown-toc-deeplabv3-fine-tuning-resnet-50">DeepLabV3+ Fine-Tuning (ResNet-50)</a></li>
      <li><a href="#sam3-fine-tuning-foundation-model" id="markdown-toc-sam3-fine-tuning-foundation-model">SAM3 Fine-Tuning (Foundation Model)</a></li>
    </ul>
  </li>
  <li><a href="#model-comparison" id="markdown-toc-model-comparison">Model Comparison</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#limitations-and-future-work" id="markdown-toc-limitations-and-future-work">Limitations and Future Work</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="main-content">Main Content</h2>
<p>Coral reefs are under increasing stress from climate change and local human impacts, and
high-resolution monitoring is essential for tracking reef health over time. Manual annotation of
underwater imagery is slow and requires expert knowledge, which motivates the use of
semantic segmentation models to automate benthic mapping.</p>

<p>In this project, we focus on semantic scene understanding in coral reefs using the CoralScapes
dataset.</p>

<p><strong>Dataset.</strong><br />
We use the CoralScapes dataset, a general-purpose coral reef semantic segmentation benchmark
with 2,075 images, 39 benthic classes, and 174,077 segmentation masks (≈ 3.36B annotated
pixels). The dataset is split spatially by reef site for fair generalization testing:</p>

<ul>
  <li>1,517 train images (27 sites)</li>
  <li>166 validation images (3 sites)</li>
  <li>392 test images (5 sites)</li>
</ul>

<p>The imagery spans diverse real reef conditions (e.g., shallow reefs &lt;15m, varying camera-to-
seabed distance and turbidity, different times/lighting conditions), making it a challenging,
realistic segmentation setting.</p>

<h2 id="results">Results</h2>

<h3 id="full-results-qualitative">Full Results (Qualitative)</h3>
<p style="width: 700px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team35/cs163_final_project_im_1.png" alt="Qualitative comparison of segmentation outputs" /></p>

<p>SegFormer-B5 Overlay (Video): https://drive.google.com/file/d/119cAD7lq9Zfyn4leEki5gl8dGJ9USxdg/view?ts=693df8e8</p>

<h3 id="full-results-quantitative">Full Results (Quantitative)</h3>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Data</th>
      <th>Split</th>
      <th>Pixel Acc</th>
      <th>mIoU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SegFormer Base</td>
      <td>FULL</td>
      <td>Val</td>
      <td>0.9372</td>
      <td>0.7660</td>
    </tr>
    <tr>
      <td>SegFormer Base</td>
      <td>FULL</td>
      <td>Test</td>
      <td>0.8277</td>
      <td>0.5668</td>
    </tr>
    <tr>
      <td>SegFormer Base</td>
      <td>TILED</td>
      <td>Val</td>
      <td>0.9244</td>
      <td>0.7230</td>
    </tr>
    <tr>
      <td>SegFormer Base</td>
      <td>TILED</td>
      <td>Test</td>
      <td>0.8122</td>
      <td>0.5615</td>
    </tr>
    <tr>
      <td>SegFormer FT</td>
      <td>FULL</td>
      <td>Val</td>
      <td>0.8680</td>
      <td>0.5906</td>
    </tr>
    <tr>
      <td>SegFormer FT</td>
      <td>FULL</td>
      <td>Test</td>
      <td>0.7837</td>
      <td>0.4788</td>
    </tr>
    <tr>
      <td>SegFormer FT</td>
      <td>TILED</td>
      <td>Val</td>
      <td>0.8532</td>
      <td>0.5821</td>
    </tr>
    <tr>
      <td>SegFormer FT</td>
      <td>TILED</td>
      <td>Test</td>
      <td>0.7676</td>
      <td>0.4767</td>
    </tr>
    <tr>
      <td>DeepLab FT</td>
      <td>FULL</td>
      <td>Val</td>
      <td>0.7219</td>
      <td>0.3591</td>
    </tr>
    <tr>
      <td>DeepLab FT</td>
      <td>FULL</td>
      <td>Test</td>
      <td>0.7783</td>
      <td>0.4548</td>
    </tr>
    <tr>
      <td>DeepLab FT</td>
      <td>TILED</td>
      <td>Val</td>
      <td>0.7050</td>
      <td>0.3333</td>
    </tr>
    <tr>
      <td>DeepLab FT</td>
      <td>TILED</td>
      <td>Test</td>
      <td>0.7533</td>
      <td>0.4260</td>
    </tr>
    <tr>
      <td>SAM3 FT</td>
      <td>FULL</td>
      <td>Val</td>
      <td>0.5981</td>
      <td>0.1428</td>
    </tr>
    <tr>
      <td>SAM3 FT</td>
      <td>FULL</td>
      <td>Test</td>
      <td>0.6199</td>
      <td>0.1833</td>
    </tr>
    <tr>
      <td>SAM3 FT</td>
      <td>TILED</td>
      <td>Val</td>
      <td>0.6526</td>
      <td>0.1919</td>
    </tr>
    <tr>
      <td>SAM3 FT</td>
      <td>TILED</td>
      <td>Test</td>
      <td>0.6906</td>
      <td>0.2352</td>
    </tr>
  </tbody>
</table>

<h2 id="models-and-experiments">Models and Experiments</h2>

<h3 id="segformer-b5-released-coralscapes-checkpoint">SegFormer-B5 (Released CoralScapes Checkpoint)</h3>

<p><strong>Motivation.</strong><br />
SegFormer is a transformer-based semantic segmentation architecture that performs strongly on
dense prediction tasks. Because a CoralScapes-tuned SegFormer-B5 checkpoint is publicly
available, it serves as a high-quality reference baseline for our experiments.</p>

<p><strong>Architecture.</strong><br />
SegFormer uses a hierarchical Mix Transformer (MiT-B5) encoder that produces multi-scale
feature maps, and a lightweight decoder that fuses these features into dense per-pixel logits.</p>

<p style="width: 700px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team35/cs163_final_project_im_2.png" alt="Segformer Architecture" /></p>

<p><strong>Results.</strong><br />
The released SegFormer-B5 checkpoint is the strongest model in our study across most splits,
achieving:</p>

<ul>
  <li>FULL: Val mIoU 0.7660, Test mIoU 0.5668</li>
  <li>TILED: Val mIoU 0.7230, Test mIoU 0.5615</li>
</ul>

<h3 id="segformer-fine-tuning-our-short-budget-adaptation">SegFormer Fine-Tuning (Our Short-Budget Adaptation)</h3>

<p><strong>Motivation.</strong><br />
We fine-tune SegFormer using a coral-specific pipeline (tiling, strong augmentations, and a
CE+Dice loss) to test whether a stronger training setup can improve robustness despite limited
compute.</p>

<p><strong>Training Pipeline.</strong></p>
<ul>
  <li>Train on tiles to increase sample count and reduce memory cost per step</li>
  <li>Use strong augmentations (Flip, Rotate, RGBShift, RandomFog, GaussianBlur)</li>
  <li>Optimize with Cross-Entropy + Dice to balance pixel accuracy and region overlap under class
imbalance</li>
</ul>

<p><strong>Results.</strong>
Under our constrained training schedule, our fine-tuned SegFormer model underperforms the released SegFormer-B5 checkpoint:</p>
<ul>
  <li>FULL: Val mIoU 0.5906, Test mIoU 0.4788</li>
  <li>TILED: Val mIoU 0.5821, Test mIoU 0.4767</li>
</ul>

<p><strong>Interpretation.</strong><br />
SegFormer-B5 is large and typically benefits from long schedules; short fine-tuning can fail to
match a strong dataset-tuned checkpoint. Given more time and compute, we would expect
performance to improve with longer schedules and better-tuned optimization (LR schedule,
weight decay, freezing strategy, etc.).</p>

<h3 id="deeplabv3-fine-tuning-resnet-50">DeepLabV3+ Fine-Tuning (ResNet-50)</h3>

<p><strong>Motivation.</strong><br />
DeepLabV3+ is a widely used CNN-based segmentation baseline. We include it as a comparison
point against transformer segmentation, particularly since coral imagery contains fine textures
and complex boundaries.</p>

<p><strong>Architecture (brief).</strong><br />
DeepLabV3+ combines a CNN backbone with atrous/dilated convolutions (ASPP) for multi-scale
context, followed by a decoder for boundary refinement.</p>

<p style="width: 700px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team35/cs163_final_project_im_4.png" alt="DeepLabV3+ Architecture" /></p>

<p><strong>Results.</strong>
With limited training time, DeepLabV3+ performs below SegFormer:</p>
<ul>
  <li>FULL: Val mIoU 0.3591, Test mIoU 0.4548</li>
  <li>TILED: Val mIoU 0.3333, Test mIoU 0.4260</li>
</ul>

<p><strong>Interpretation.</strong><br />
The DeepLab result is similar to our SegFormer result as we could optimize and get better results with more time/compute. CNN models can require careful tuning of LR schedules and longer training to be competitive on fine-grained multi-class segmentation.</p>

<h3 id="sam3-fine-tuning-foundation-model">SAM3 Fine-Tuning (Foundation Model)</h3>

<p><strong>Motivation.</strong><br />
SAM3 is a recent foundation model designed for promptable segmentation and tracking. We
explore whether its large-scale pretraining can transfer to coral reef semantic segmentation with
minimal adaptation.</p>

<p><strong>Adaptation for Semantic Segmentation.</strong>
SAM3 is originally built for prompt-driven instance segmentation. To repurpose it for dense multi-class segmentation, we:</p>
<ul>
  <li>Use SAM3 as an image feature backbone</li>
  <li>Attach a lightweight per-pixel semantic head</li>
  <li>Train in a prompt-free dense prediction mode with the same CE+Dice objective</li>
</ul>

<p style="width: 700px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team35/cs163_final_project_im_3.png" alt="SAM3 Architecture" /></p>

<p><strong>Results.</strong>
SAM3 performs poorly in this prompt-free, dense multi-class setup:</p>
<ul>
  <li>FULL: Val mIoU 0.1428, Test mIoU 0.1833</li>
  <li>TILED: Val mIoU 0.1919, Test mIoU 0.2352</li>
</ul>

<p><strong>Interpretation.</strong><br />
There are two main reasons for poor performance:</p>
<ol>
  <li>SAM3 is optimized for prompted instance masks (“segment the object I indicate”), not specific multi class dense semantic labeling.</li>
  <li>SAM3 is extremely large, so our fine-tuning budget was necessarily short. Under short training, the model is unlikely to adapt fully to the coral-specific label space and underwater domain shift.</li>
</ol>

<p><strong>Takeaway.</strong><br />
In our constrained setting, a dataset-tuned semantic segmentation model (SegFormer)
transfers far better than a large promptable foundation model adapted with a small semantic
head.</p>

<h2 id="model-comparison">Model Comparison</h2>
<p>Transformed-based segmentation models demonstrate the strongest performance on the CoralScapes dataset, while operating under limited compute and training time. SegFormer consistently outperformed both the CNN and foundation model approaches, likely because it captures global context without losing fine spatial detail. This matters for benthic segmentation, where visually similar classes must be separated despite varying lightings and reef structure.</p>

<p>DeepLabV3 focuses on more local texture through convolutional features. While this performs well for many image segmentation tasks, it may be less effective at separating fine-grained benthic classes without longer training or careful tuning. This suggests that CNN-based models may need additional optimization or task-specific pretraining to perform well on underwater images.</p>

<p>SAM3 performs poorly in this project, likely because it was pre-trained for prompt-based instance segmentation rather than semantic segmentation. While it can produce spatially coherent masks, it has difficulty assigning the correct class labels to fine-grained coral categories, especially with limited fine tuning. This suggests that large foundation models do not translate well to semantic segmentation tasks without task-specific adaptations.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this project, we evaluated multiple modern segmentation architectures on the CoralScapes dataset while operating under limited compute. Our results show that models explicitly designed and fine-tuned for semantic segmentation, particularly transformer-based architectures like SegFormer, perform best on fine-grained coral reef images compared to CNN or adapted foundation models with a semantic head. However, performance of models may be vastly different without the time and compute limitations we faced.</p>

<h2 id="limitations-and-future-work">Limitations and Future Work</h2>
<p>A key limitation of this study is compute and time. Large models such as SegFormer-B5 and SAM3 typically require long training schedules, careful hyperparameter tuning, and (in SAM3’s case) potentially prompt-aware training strategies to reach their full potential. Our experiments instead reflect a realistic small-budget regime, so results for the fine-tuned models (SegFormer FT, DeepLab FT, SAM3 FT) should be interpreted as lower bounds on what these approaches could achieve.</p>

<p>Future work includes:</p>
<ol>
  <li>Longer fine-tuning schedules with tuned learning rates/warmup/weight decat</li>
  <li>Layer-freezing or gradual unfreezing to reduce catastrophic forgetting</li>
  <li>Coral-domain pretraining or underwater-specific augmentation strategies</li>
  <li>SAM3 experiments that incorporate prompts or prompt-like conditioning instead of forcing a prompt-free dense prediction mode</li>
</ol>

<p>Colab notebook with project code: https://colab.research.google.com/drive/1-XIG4hYo7OcubjfQI2XydadW4JpQBowh?usp=sharing</p>

<h2 id="references">References</h2>

<p>Carion, N., et al. (2025). <em>SAM 3: Segment anything with concepts</em>. arXiv preprint arXiv:2511.16719.</p>

<p>Chen, L.-C., et al. (2017). <em>DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4), 834–848.</p>

<p>Sauder, J., et al. (2025). <em>The CoralScapes dataset: Semantic scene understanding in coral reefs</em>. arXiv preprint arXiv:2503.20000.</p>

<p>Xie, E., et al. (2021). <em>SegFormer: Simple and efficient design for semantic segmentation with transformers</em>. Advances in Neural Information Processing Systems, 34, 12077–12090.</p>

  </div>


  <!-- <div class="page-navigation">
    
      <a class="prev" href="/CS163-Projects-2025Fall/2025/12/13/team33-humanposeestimation.html">&larr; Evolution of Human Pose Estimation - From Deep Regression to YOLO-Pose</a>
    

    <!-- 
      <a class="next" href="/CS163-Projects-2025Fall/2025/12/13/team38-vqa.html">From Classifiers to Assistants: The Evolution of Visual Question Answering &rarr;</a>
     -->


  <!--comment-->
  
    <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-ucladeepvision-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

</article>

      </div>
    </main>

    <div style="clear: both;" />
<footer class="site-footer">
    2024 &copy; by UCLAdeepvision. All Rights Reserved. Built by <a href="https://jekyllrb.com/"
        target="_blank">Jekyll</a>

    <!-- <p>
        <a href="/CS163-Projects-2025Fall/feed.xml" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_rss.png" />
        </a>
        <a href="https://scholar.google.com/citations?user=dCa-pW8AAAAJ&hl=en&oi=ao" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_scholar.png" />
        </a>
        <a href="https://github.com/lilianweng" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_github.png" />
        </a>
    </p> -->
</footer>

  </body>

</html>
