<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Robot Navigation Using Deep Vision Models</title>

    <meta name="description" content="In this project we build a full robot navigation pipeline inside an embodied AI simulation (AI2-THOR). Our system detects objects, understands spatial relati...">

    <meta content="2025F, UCLA CS163 Course Projects" property="og:site_name">
    
        <meta content="Robot Navigation Using Deep Vision Models" property="og:title">
    
    
        <meta content="article" property="og:type">
    
    
        <meta content="In this project we build a full robot navigation pipeline inside an embodied AI simulation (AI2-THOR). Our system detects objects, understands spatial relationships, and navigates toward a target object using modern vision models such as CLIP, SAM (Segment Anything), and multimodal reasoning modules. We demonstrate an end-to-end system capable of..." property="og:description">
    
    
        <meta content="http://localhost:4000/2025/12/10/team29-robotvisionnav.html" property="og:url">
    
<!--
    
        <meta content="Ryan Teoh, Bill, Maddox, Andrew" property="article:author">
        <meta content="http://localhost:4000/about/" property="article:author">
     -->

    <!-- 
        <meta content="2025-12-10T00:00:00-08:00" property="article:published_time">
        <meta content="http://localhost:4000/about/" property="article:author">
    
    
    
        
     -->

    <link rel="shortcut icon" href="/CS163-Projects-2025Fall/assets/ucla_ico.jpg">
    <link rel="stylesheet" href="/CS163-Projects-2025Fall/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/CS163-Projects-2025Fall/2025/12/10/team29-robotvisionnav.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <!-- <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-8161570-6', 'auto');
        ga('send', 'pageview');
    </script> -->

    <!-- For Facebook share button -->
    <!-- <div id="fb-root"></div>
    <script>
      (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));
    </script> -->

    <!-- Twitter cards -->
    <!-- <meta name="twitter:site"    content="@">
    <meta name="twitter:creator" content="@UCLAdeepvision">
    <meta name="twitter:title"   content="Robot Navigation Using Deep Vision Models">

    
        <meta name="twitter:description" content="<blockquote>
  <p>In this project we build a full robot navigation pipeline inside an embodied AI simulation (AI2-THOR).<br />
Our system detects objects, understands spatial relationships, and navigates toward a target object using modern vision models such as <strong>CLIP</strong>, <strong>SAM (Segment Anything)</strong>, and multimodal reasoning modules.<br />
We demonstrate an end-to-end system capable of object identification, object-centric navigation, and spatial reasoning.</p>
</blockquote>

">
    

    
        <meta name="twitter:card"  content="summary">
        <meta name="twitter:image" content="">
     -->
    <!-- end of Twitter cards -->

</head>


  <body>

    <header class="site-header" role="banner" id='header-bar'>

    <div class="wrapper">
        
        <a class="site-title" style="color:#F2A900" href="/CS163-Projects-2025Fall/">2025F, UCLA CS163 Course Projects  </a>

        <!-- <nav class="site-nav">
            <a class="page-link" href="http://lilianweng.github.io" target="_blank">&#x1f349; About</a>
        </nav> -->
        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/about.html"> About</a>
        </nav>

        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/archive.html"> Archive</a>
        </nav>


        <!-- <nav class="site-nav">
            <a class="page-link" style="color:#FFD100" href="/CS163-Projects-2025Fall/FAQ.html"> FAQ</a>
        </nav> -->
        <!-- <nav class="site-nav">
            <a class="page-link" href="/CS163-Projects-2025Fall/log.html">&#x231b; Log</a>
        </nav> -->


    </div>

</header>


    <!-- Back to top button -->
    <script src="/CS163-Projects-2025Fall/assets/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop()</script>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Robot Navigation Using Deep Vision Models</h1>
    <p class="post-meta">

      <time datetime="2025-12-10T00:00:00-08:00" itemprop="datePublished">
        
        Dec 10, 2025
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Ryan Teoh, Bill, Maddox, Andrew</span>
      </span>

      <!-- <span>
        
      </span> -->
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2025/12/10/team29-robotvisionnav.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>In this project we build a full robot navigation pipeline inside an embodied AI simulation (AI2-THOR).<br />
Our system detects objects, understands spatial relationships, and navigates toward a target object using modern vision models such as <strong>CLIP</strong>, <strong>SAM (Segment Anything)</strong>, and multimodal reasoning modules.<br />
We demonstrate an end-to-end system capable of object identification, object-centric navigation, and spatial reasoning.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#computer-vision-model-architecture" id="markdown-toc-computer-vision-model-architecture">Computer Vision Model Architecture</a>    <ul>
      <li><a href="#1-object-identification-with-clip" id="markdown-toc-1-object-identification-with-clip">1. <strong>Object Identification with CLIP</strong></a></li>
      <li><a href="#2-segmentation-with-sam-segment-anything" id="markdown-toc-2-segmentation-with-sam-segment-anything">2. <strong>Segmentation with SAM (Segment Anything)</strong></a></li>
      <li><a href="#3-spatial-relationship-detection" id="markdown-toc-3-spatial-relationship-detection">3. <strong>Spatial Relationship Detection</strong></a></li>
    </ul>
  </li>
  <li><a href="#navigating-the-enviornment" id="markdown-toc-navigating-the-enviornment">Navigating the Enviornment</a></li>
  <li><a href="#example-demonstration" id="markdown-toc-example-demonstration">Example Demonstration</a></li>
  <li><a href="#conclusions" id="markdown-toc-conclusions">Conclusions</a></li>
  <li><a href="#code" id="markdown-toc-code">Code</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>Embodied AI requires agents to perceive, reason, and act in realistic 3D environments.<br />
We use <strong>AI2-THOR</strong>, an interactive simulation containing kitchens, living rooms, bedrooms, and bathrooms.<br />
Our goal is to enable an agent to:</p>

<ol>
  <li>Identify target objects using vision-language models (CLIP)</li>
  <li>Segment the target and surrounding context using SAM</li>
  <li>Understand spatial relationships (e.g., <em>“the laptop is on the sofa”</em>)</li>
  <li>Move toward the target using closed-loop visual feedback</li>
</ol>

<p>To achieve this, we integrate <strong>SAM</strong>, <strong>CLIP</strong>, and a custom spatial-reasoning module into a perception–action pipeline.</p>

<hr />

<h2 id="computer-vision-model-architecture">Computer Vision Model Architecture</h2>

<h3 id="1-object-identification-with-clip">1. <strong>Object Identification with CLIP</strong></h3>

<p>We use CLIP to match image patches against text prompts for object names.</p>

<p style="width: 500px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team-id/clip_pipeline.png" alt="CLIP Pipeline" /></p>
<p><em>Fig 1. CLIP identifies the target object using text–image similarity.</em></p>

<p>Given a target command (e.g., <em>“find the microwave”</em>), the system:</p>

<ol>
  <li>Samples candidate bounding boxes</li>
  <li>Extracts embeddings via CLIP image encoder</li>
  <li>Computes similarity with the text embedding</li>
  <li>Selects the highest-scoring region as the target</li>
</ol>

<p>CLIP allows us to compute the semantic similarity between:</p>

<ul>
  <li>a text embedding (ex. cup)</li>
  <li>an image embedding (ex. cropped SAM mask region)</li>
</ul>

<p>This allows for an open vocabulary object recognition, where even if the object is not part of a fixed category set, CLIP allows the model to still be able to identify these objects.</p>

<p>In <code class="language-plaintext highlighter-rouge">find_best_object</code> we evaluate our SAM mask crop against the test query.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>text_tokens = tokenizer([clean_query]).to(device)
image_inputs = torch.stack([clip_preprocess(img) for img in crops]).to(device)

with torch.no_grad():
    img_features = clip_model.encode_image(image_inputs)
    txt_features = clip_model.encode_text(text_tokens)

    img_features /= img_features.norm(dim=-1, keepdim=True)
    txt_features /= txt_features.norm(dim=-1, keepdim=True)

    # Softmax over similarity scores
    probs = (100.0 * img_features @ txt_features.T).softmax(dim=0)
    values, indices = probs.topk(1)
</code></pre></div></div>

<p>By cleaning the text query and computing similarity scores, we can pick the highest scoring object mask to get our predict object for the query. We use our best mask and score to draw our bounding boxes later as well as perform spatial reasoning and navigation. CLIP is also beneficial for our model because objects may be partially hidden, rotated, etc. in our robot simulation, and SAM alongside CLIP can determine whether a sigment looks like the queried box.</p>

<h3 id="2-segmentation-with-sam-segment-anything">2. <strong>Segmentation with SAM (Segment Anything)</strong></h3>

<p>After locating the target region, we refine the mask using SAM:</p>

<p style="width: 500px;"><img src="/CS163-Projects-2025Fall/assets/images/team-id/sam_mask.png" alt="SAM Example" /></p>

<p>SAM provides a high-quality segmentation mask, which we use for:</p>

<ul>
  <li>Object localization</li>
  <li>Pixel-level spatial reasoning</li>
  <li>Deriving the agent’s navigation targets</li>
</ul>

<p>To obtain object candidates in each frame, we use SAM as our instance segmengation module. SAM takes in a raw RGB frame from the RoboTHOR camera and then returns a set of masks that lets us</p>

<ul>
  <li>Object localization</li>
  <li>Pixel-level spatial reasoning</li>
  <li>Deriving the agent’s navigation targets</li>
</ul>

<p>We use a ViT-H variant of SAM alongside a <code class="language-plaintext highlighter-rouge">SamAutomaticMaskGenerator</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mask_generator = SamAutomaticMaskGenerator(
    sam,
    points_per_side=16,
    pred_iou_thresh=0.3,
    stability_score_thresh=0.3,
    box_nms_thresh=0.7,
    min_mask_region_area=400,
)
</code></pre></div></div>

<p>We have a <code class="language-plaintext highlighter-rouge">find_best_object</code> helper function which performs our core perceptron segmentation to cropping to CLIP scoring.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>masks = mask_generator.generate(image_np)

for mask_data in masks:
    x, y, w, h = map(int, mask_data["bbox"])

    if w &lt; 10 or h &lt; 10:
        continue

    crop = Image.fromarray(image_np[y:y+h, x:x+w])
    crops.append(crop)
    valid_masks.append(mask_data)
</code></pre></div></div>

<hr />

<h3 id="3-spatial-relationship-detection">3. <strong>Spatial Relationship Detection</strong></h3>

<p>We extend the system to detect relations like:</p>

<ul>
  <li><em>“X is on Y”</em></li>
  <li><em>“X is next to Y”</em></li>
  <li><em>“X is inside Y”</em></li>
</ul>

<p>Using metadata from AI2-THOR combined with SAM masks:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample code block
def get_spatial_context(controller, target_mask):
    """
    Uses AI2-THOR metadata + instance segmentation
    to determine which object the target is on or inside.
    """
</code></pre></div></div>

<p>Beyond simply finding what object matches the query, we also want to know where it is in the scene and whether it is on another object or on something else. Thus, to do so, we combine</p>

<ol>
  <li>SAM’s pixel level mask for the target object.</li>
  <li>Ai2-THOR’s instance segmentation, telling us which object each pixel belongs to.</li>
</ol>

<p>We thus implement a helper function called <code class="language-plaintext highlighter-rouge">get_spatial_context(controller, target_mask)</code> that allows us to</p>

<ol>
  <li>Read the latest event from the controller</li>
  <li>Extract pixels inside the SAM mask</li>
  <li>Map RGB colors to a Thor object ID</li>
  <li>Lookup the object metadata and parentReceptacles.</li>
</ol>

<p>Intuitiviely, the SAM mask tells us which pixels are part of the target, the instance segmentation frame tells us which simulator object those pixels correspond to, and the metadata tells us what the object is residing on, such as a table, couch, drawer, etc. We can then use this location string to visualize detections, indicating a bounding box with a CLIP score and spatial context.</p>

<p>WE MIGHT WANT TO INCLUDE A PICTURE OF IT FROM OUR COLLAB NOTEBOOK.</p>

<h2 id="navigating-the-enviornment">Navigating the Enviornment</h2>

<p>By providing the system with a natural language query, such as “find the blue cup”, our model will decipher this message into a query that determines searching for a cup, in which afterwards RoboTHOR will explore the scene until the target is visible, in which we then can run our SAM / sliding window CLIP to find the object. We have two methods of navigation, one which relies on an existing API and another which uses a heuristic to calculate distance.</p>

<p>GetShortestPath.</p>

<ol>
  <li>By using <a href="https://ai2thor.allenai.org/robothor/documentation/#sub-shortest-path">GetShortestPath</a> with the AI2-THOR, we can feed parameters of the object type we are looking for, such as cup, our current position, and the allowed error. If the object exists and is achievable, then we can get the path event from the start to the end object. One limitation of our API is that it is unable to distinguish between two objects, thus if a query supplies “apple” and there exist two apples on the simulation, it will naturally navigate to the closest one. GetShortestPath provides us with the corner paths on the path, so by setting them as waypoints, our robot can follow the action until it reaches the desired object.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from ai2thor.util.metrics import (
    get_shortest_path_to_object_type
)

path = get_shortest_path_to_object_type(
   controller=controller,
    object_type="Apple",
    initial_position=dict(
        x=0,
        y=0.9,
        z=0.25
    )
)
</code></pre></div></div>

<p>If our navigation does not work, then we perform random navigation, in which we consider four different directions (left, right, forward, backwards), and essentially like a maze, follow the right side of the wall until we reach the desired object. We may also perform a full rotation, and if it fails to reveal the object, it can move until it finds a new vantage point.</p>

<p>If the object does not exist, and if repeated movement patterns fail to detect the object, then the system concludes that the object is not in the scene and will report failure. This failsure ensures that our algorithm doesn’t infinitely search forever.</p>

<h2 id="example-demonstration">Example Demonstration</h2>

<h2 id="conclusions">Conclusions</h2>

<h2 id="code">Code</h2>

<p>Project Repo: <a href="https://github.com/Land-dev/finalProject163">GitHub Repository</a></p>

<p>SAM repo:</p>

<p>CLIP repo:</p>

<p>Ai2-Thor simulation (We use RoboThor): <a href="https://ai2thor.allenai.org/robothor">RoboThor</a></p>

<h2 id="references">References</h2>

<p>Ai2-Thor Documentation: https://ai2thor.allenai.org/robothor/documentation/</p>

  </div>


  <!-- <div class="page-navigation">
    
      <a class="prev" href="/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html">&larr; Post Template</a>
    

    <!--  -->


  <!--comment-->
  
    <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-ucladeepvision-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

</article>

      </div>
    </main>

    <div style="clear: both;" />
<footer class="site-footer">
    2024 &copy; by UCLAdeepvision. All Rights Reserved. Built by <a href="https://jekyllrb.com/"
        target="_blank">Jekyll</a>

    <!-- <p>
        <a href="/CS163-Projects-2025Fall/feed.xml" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_rss.png" />
        </a>
        <a href="https://scholar.google.com/citations?user=dCa-pW8AAAAJ&hl=en&oi=ao" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_scholar.png" />
        </a>
        <a href="https://github.com/lilianweng" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_github.png" />
        </a>
    </p> -->
</footer>

  </body>

</html>
