<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Food Detection</title>

    <meta name="description" content="Food detection is a subset of image classification that is able to classify specific foods. As such, it requires the ability to learn finer-grain details to ...">

    <meta content="2025F, UCLA CS163 Course Projects" property="og:site_name">
    
        <meta content="Food Detection" property="og:title">
    
    
        <meta content="article" property="og:type">
    
    
        <meta content="Food detection is a subset of image classification that is able to classify specific foods. As such, it requires the ability to learn finer-grain details to differentiate specific foods. In this paper, we investigate how three models, DeepFood, WISeR, and Noisy-ViT, built upon state-of-the-art (at the time) object classification models..." property="og:description">
    
    
        <meta content="/2025/12/13/team33-food-detection.html" property="og:url">
    
<!--
    
        <meta content="Adrian Pu" property="article:author">
        <meta content="/about/" property="article:author">
     -->

    <!-- 
        <meta content="2025-12-13T00:00:00+00:00" property="article:published_time">
        <meta content="/about/" property="article:author">
    
    
    
        
     -->

    <link rel="shortcut icon" href="/CS163-Projects-2025Fall/assets/ucla_ico.jpg">
    <link rel="stylesheet" href="/CS163-Projects-2025Fall/assets/css/main.css">
    <link rel="canonical" href="/CS163-Projects-2025Fall/2025/12/13/team33-food-detection.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <!-- <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-8161570-6', 'auto');
        ga('send', 'pageview');
    </script> -->

    <!-- For Facebook share button -->
    <!-- <div id="fb-root"></div>
    <script>
      (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));
    </script> -->

    <!-- Twitter cards -->
    <!-- <meta name="twitter:site"    content="@">
    <meta name="twitter:creator" content="@UCLAdeepvision">
    <meta name="twitter:title"   content="Food Detection">

    
        <meta name="twitter:description" content="<blockquote>
  <p>Food detection is a subset of image classification that is able to classify specific foods. As such, it requires the ability to learn finer-grain details to differentiate specific foods. In this paper, we investigate how three models, DeepFood, WISeR, and Noisy-ViT, built upon state-of-the-art (at the time) object classification models for food detection, along with a dataset built for food detection Food-101. On this dataset, DeepFood performed at a 77.40% accuracy, WISeR performed at a 90.27% accuracy, and Noisy-ViT performed at a 99.50% accuracy.</p>
</blockquote>

">
    

    
        <meta name="twitter:card"  content="summary">
        <meta name="twitter:image" content="">
     -->
    <!-- end of Twitter cards -->

</head>


  <body>

    <header class="site-header" role="banner" id='header-bar'>

    <div class="wrapper">
        
        <a class="site-title" style="color:#F2A900" href="/CS163-Projects-2025Fall/">2025F, UCLA CS163 Course Projects  </a>

        <!-- <nav class="site-nav">
            <a class="page-link" href="http://lilianweng.github.io" target="_blank">&#x1f349; About</a>
        </nav> -->
        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/about.html"> About</a>
        </nav>

        <nav class="site-nav">
            <a class="page-link" style="color:#F2A900" href="/CS163-Projects-2025Fall/archive.html"> Archive</a>
        </nav>


        <!-- <nav class="site-nav">
            <a class="page-link" style="color:#FFD100" href="/CS163-Projects-2025Fall/FAQ.html"> FAQ</a>
        </nav> -->
        <!-- <nav class="site-nav">
            <a class="page-link" href="/CS163-Projects-2025Fall/log.html">&#x231b; Log</a>
        </nav> -->


    </div>

</header>


    <!-- Back to top button -->
    <script src="/CS163-Projects-2025Fall/assets/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop()</script>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Food Detection</h1>
    <p class="post-meta">

      <time datetime="2025-12-13T00:00:00+00:00" itemprop="datePublished">
        
        Dec 13, 2025
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Adrian Pu</span>
      </span>

      <!-- <span>
        
      </span> -->
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2025/12/13/team33-food-detection.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>Food detection is a subset of image classification that is able to classify specific foods. As such, it requires the ability to learn finer-grain details to differentiate specific foods. In this paper, we investigate how three models, DeepFood, WISeR, and Noisy-ViT, built upon state-of-the-art (at the time) object classification models for food detection, along with a dataset built for food detection Food-101. On this dataset, DeepFood performed at a 77.40% accuracy, WISeR performed at a 90.27% accuracy, and Noisy-ViT performed at a 99.50% accuracy.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#food-detection-vs-object-detection" id="markdown-toc-food-detection-vs-object-detection">Food Detection vs. Object Detection</a></li>
  <li><a href="#food-101" id="markdown-toc-food-101">Food-101</a></li>
  <li><a href="#deepfood" id="markdown-toc-deepfood">DeepFood</a></li>
  <li><a href="#wiser-residual-network" id="markdown-toc-wiser-residual-network">WISeR Residual Network</a>    <ul>
      <li><a href="#wide-residual-block" id="markdown-toc-wide-residual-block">Wide Residual Block</a></li>
      <li><a href="#slice-convolution" id="markdown-toc-slice-convolution">Slice Convolution</a></li>
    </ul>
  </li>
  <li><a href="#noisyvit" id="markdown-toc-noisyvit">NoisyViT</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a></li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>Food detection is the task of seeing an image and determining what specific food is in the image. These classifications can be used in a variety of different applications such as monitoring diet and nutrient intake, assessing food quality, and sorting foods in large scale factories. Diet monitoring can be critical to improve the health of many by creating an easy way to keep track of the exact foods and nutritional content a person is consuming. Assessing food quality can assist farmers with detecting ripe produce and when to harvest, and also assist quality assurance in food factories by speeding up the process. Also in factories, labeling foods makes sorting foods a much easier and faster task. With the rise in computer vision in recent years, classifications can be made using machine learning techniques by inputting the image of the food into a model that outputs what type of food it is with varying levels of precision. In this report, we will investigate what makes the food detection task unique from a more general object detection and what innovations have been made to improve the performance of food detection models.</p>

<h2 id="food-detection-vs-object-detection">Food Detection vs. Object Detection</h2>
<p>Food detection is a form of object detection, which exists as its own general task. Object detection seeks to determine what a specific object is in an image. The list of objects that can potentially be outputted are very broad across vastly different genres, as it is designed to work as a generalized task. While this is ideal for a more general use case, it does not distinguish finer grain details within individual genres of image categories [1]. For instance, classifying an object as a car compared to classifying an object as a Honda Accord. Images of food do not have significant spacial relationships such as the form and structure of body parts for a human being or subject and background images for outdoor images. Because of this, models for food detection have to capable of computing finer grain details to make its decisions.</p>

<h2 id="food-101">Food-101</h2>
<p>To train and test models on food detection, Bossard et al. [1] created the Food-101 dataset. Standard object detection datasets such as ImageNet have a vast range of different genres of images in different scenes. Food detection requires just images of food to be able to learn finer grain details of each food. This dataset consists of 101000 images. These images are split into 101 categories of the 101 most popular foods and 1000 randomly picked images for each. Additionally, in those 1000 images, 750 are picked as training data and 250 as test data, and each image is 512 pixels by 512 pixels. In this paper, we will focus on the following models’ performance only on the Food-101 dataset.</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/33/Food-101.png" alt="YOLO" /></p>
<p><em>Fig 1. 100 samples of 100 of the categories of Food-101</em> [1].</p>
<h2 id="deepfood">DeepFood</h2>
<p>Proposed by Liu et al. [2], The DeepFood model utilized a fine-tuned GoogleNet model on food detection datasets. The GoogleNet model innovated upon then-current day CNNs by introducing the Inception Module.</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/33/Inception.png" alt="YOLO" /></p>
<p><em>Fig 2. Diagram of an inception module</em> [2].</p>

<p>The inception module allows for increased computational power while decreasing computation complexity, allowing for quicker training and inference. These inception modules are then concatenated together into a 22 layer CNN. The model discussed in this paper took a pre-trained GoogleNet model and then they fine- tuned the model on the Food-101 dataset and another food detection dataset called UEC-256.</p>

<h2 id="wiser-residual-network">WISeR Residual Network</h2>
<p>Proposed by Martinel et al. [3], The Wide-Slice Residual Network is a Convolutional Nerual Network that applies the residual learning on top of a new introduced convolutional layer used to locate finer grain details for the food detection task. This model was pre-trained on ImageNet and then fine-tuned with the Food-101 dataset and also UEC-256 and UEC-1000.</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/33/WISeR.png" alt="YOLO" /></p>
<p><em>Fig 3. Diagram of the WISeR architecture</em> [3].</p>

<h3 id="wide-residual-block">Wide Residual Block</h3>
<p>Residual learning through a block of convolution layers is defined as followed: \(x + F(x) = \text{output}\). This innovation allowed for the blocks to learn the identity function without the burden of trying to recreate the input, allowing for much deeper networks without degradation of quality. The innovation here is widening the convolution layers by increasing the amount of kernels to increase the size of the feature map.</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/33/WISeR-2.png" alt="YOLO" /></p>
<p><em>Fig 4. Diagram of the standard residual block (left) and the widened convolution layers of the WISeR archiecture (right)</em> [3].</p>

<h3 id="slice-convolution">Slice Convolution</h3>
<p>The standard convolution layer breaks up the image into kernels and transfers that information into a feature map. Kernels are traditionally defined as squares and is what convolutional layers use to learn spatial information and features. In this model as seen on Figure 2, in addition to using many of these convolution layers, there exists a branch that uses a slice convolution layer that breaks up kernels into horizontal slices to learn features on vertical layers. The intuition is there are many foods that are stacked veritcally such as burgers and sandwiches.</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/33/WISeR-3.png" alt="YOLO" /></p>
<p><em>Fig 5. Comparing standard square kernels to slice convolution for lasagna</em> [3].</p>

<h2 id="noisyvit">NoisyViT</h2>
<p>Proposed by Ghosh and Sazonov [4], this model built upon the Visual Transformer to tailor its use case to handle food detection. The Visual Transformer introduces self-attention to computer vision models which allows the model to learn with respect to context across the data. NoisyViT builds upon the standard ViT by injecting noise randomly in between transformer layers during training. During testing, the noise injection was removed. The noise was injected as a linear transformation of a feature space and was injected to a random layer. This model was built on top of a Visual Transformer Model pre-trained on the ImageNet dataset, and then fine-tuned with Food-101, Food-2k and CNFOOD-241 datasets.</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/33/NoisyViT.png" alt="YOLO" /></p>
<p><em>Fig 6. Diagram of the noise injection in the NoisyViT architecture</em> [4].</p>

<h2 id="results">Results</h2>
<p>Compared to other off-the-shelf models, this is how each model performed on the Food-101 dataset.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">Top 1 Performance (%)</th>
      <th style="text-align: right">Top 5 Performance (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">AlexNet</td>
      <td style="text-align: center">56.40</td>
      <td style="text-align: right">N/A</td>
    </tr>
    <tr>
      <td style="text-align: left">DeepFood [2]</td>
      <td style="text-align: center">77.40</td>
      <td style="text-align: right">93.70</td>
    </tr>
    <tr>
      <td style="text-align: left">ResNet-200</td>
      <td style="text-align: center">88.38</td>
      <td style="text-align: right">97.85</td>
    </tr>
    <tr>
      <td style="text-align: left">WISeR [3]</td>
      <td style="text-align: center">90.27</td>
      <td style="text-align: right">98.71</td>
    </tr>
    <tr>
      <td style="text-align: left">ViT-B</td>
      <td style="text-align: center">88.46</td>
      <td style="text-align: right">98.05</td>
    </tr>
    <tr>
      <td style="text-align: left">Noisy-ViT-B [4]</td>
      <td style="text-align: center">99.50</td>
      <td style="text-align: right">100.00</td>
    </tr>
  </tbody>
</table>

<h2 id="discussion">Discussion</h2>
<p>As we can see from the results, the Noisy Visual Transformer performs almost perfectly on the Food-101 dataset, and the others make improvements over more general image classification models. The improvements against the other food detection models improve over time similar to the trajectory of how the general image classificaiton models, for example GoogleNet outperforming AlexNet, and then ResNet outperforming any other CNN based architecture. Injecting noise into the Visual Transfomrer improved performance drastically compared to every other model showcased. Comparing ResNet to WISeR, we can see that the slice convolution and wider convolution layers improve performance and we can guess that these innovations allow for the model to find the finer grain details in food detection.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Food detection is a growing problem that has high upside in its applications. From diet monitoring for individuals to potentially combat obesity and diabetes to applications in food factories, food detection is the central task required. With the introduction of object classification models, we can use these innovations for food detection, but they require additional modifications to learn the finer-grain details of food images. We found that the DeepFood model performs at a 77.40% accuracy, WISeR performs at a 90.27% accuracy, and the Noisy Visual Transformer performs incredibly well at a 99.50% accuracy on the Food-101 dataset. Each of these models showcase the increasing effectiveness of computer vision models on the food detection task.</p>

<h2 id="reference">Reference</h2>

<p>[1] L. Bossard, M. Guillaumin, and L. Van Gool, “Food-101 – Mining Discriminative Components with Random Forests,” in Computer Vision – ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds., Cham: Springer International Publishing, 2014, pp. 446–461. doi: 10.1007/978-3-319-10599-4_29.</p>

<p>[2] C. Liu, Y. Cao, Y. Luo, G. Chen, V. Vokkarane, and Y. Ma. Deepfood: Deep learning-based food image recognition for computer-aided dietary assessment. In IEEE International Conference on Smart Homes and Health Telematics, volume 9677, pages 37–48, 2016.</p>

<p>[3] Martinel, Niki, Gian Luca Foresti, and Christian Micheloni. “Wide-slice residual networks for food recognition.” 2018 IEEE Winter conference on applications of computer vision (WACV). IEEE, 2018.</p>

<p>[4] T. Ghosh, and E. Sazonov. “Improving Food Image Recognition with Noisy Vision Transformer.” arXiv preprint arXiv:2503.18997 (2025).</p>

<hr />

  </div>


  <!-- <div class="page-navigation">
    
      <a class="prev" href="/CS163-Projects-2025Fall/2025/12/13/team30-deepfake_detection_methods.html">&larr; A Comparison of Recent Deepfake Video Detection Methods</a>
    

    <!-- 
      <a class="next" href="/CS163-Projects-2025Fall/2025/12/13/team33-humanposeestimation.html">Evolution of Human Pose Estimation - From Deep Regression to YOLO-Pose &rarr;</a>
     -->


  <!--comment-->
  
    <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-ucladeepvision-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

</article>

      </div>
    </main>

    <div style="clear: both;" />
<footer class="site-footer">
    2024 &copy; by UCLAdeepvision. All Rights Reserved. Built by <a href="https://jekyllrb.com/"
        target="_blank">Jekyll</a>

    <!-- <p>
        <a href="/CS163-Projects-2025Fall/feed.xml" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_rss.png" />
        </a>
        <a href="https://scholar.google.com/citations?user=dCa-pW8AAAAJ&hl=en&oi=ao" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_scholar.png" />
        </a>
        <a href="https://github.com/lilianweng" target="_blank">
            <img src="/CS163-Projects-2025Fall/assets/images/logo_github.png" />
        </a>
    </p> -->
</footer>

  </body>

</html>
