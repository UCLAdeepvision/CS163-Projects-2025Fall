<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="/CS163-Projects-2025Fall/feed.xml" rel="self" type="application/atom+xml" /><link href="/CS163-Projects-2025Fall/" rel="alternate" type="text/html" /><updated>2025-12-14T22:00:10+00:00</updated><id>/CS163-Projects-2025Fall/feed.xml</id><title type="html">2025F, UCLA CS163 Course Projects</title><subtitle>Course projects for UCLA CS163, Deep Learning in Compuver Vision</subtitle><author><name>UCLAdeepvision</name></author><entry><title type="html">Vision Language Action Models for Robotics</title><link href="/CS163-Projects-2025Fall/2025/12/13/team05-vision-language-action-models.html" rel="alternate" type="text/html" title="Vision Language Action Models for Robotics" /><published>2025-12-13T00:00:00+00:00</published><updated>2025-12-13T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/13/team05-vision-language-action-models</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/13/team05-vision-language-action-models.html"><![CDATA[<blockquote>
  <p>The core of computer vision for robotics is utilizing deep learning to allow robots to perceive, understand, and interact with the physical world. This report explores Vision Language Action (VLA) models that combine visual input, language instruction, and robot actions in end-to-end architectures.</p>
</blockquote>

<!--more-->

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#background-on-vlas" id="markdown-toc-background-on-vlas">Background on VLAs</a></li>
  <li><a href="#rt-2-google-deepmind" id="markdown-toc-rt-2-google-deepmind">RT-2 (Google Deepmind)</a>    <ul>
      <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
      <li><a href="#architecture" id="markdown-toc-architecture">Architecture</a></li>
      <li><a href="#tokenization-method-and-action-output" id="markdown-toc-tokenization-method-and-action-output">Tokenization Method and Action Output</a></li>
      <li><a href="#training-for-next-token-generation" id="markdown-toc-training-for-next-token-generation">Training for Next Token Generation</a></li>
      <li><a href="#training-configuration" id="markdown-toc-training-configuration">Training Configuration</a></li>
    </ul>
  </li>
  <li><a href="#openvla-stanford-uc-berkeley-toyota-research-institute" id="markdown-toc-openvla-stanford-uc-berkeley-toyota-research-institute">OpenVLA (Stanford, UC Berkeley, Toyota Research Institute)</a>    <ul>
      <li><a href="#overview-1" id="markdown-toc-overview-1">Overview</a></li>
      <li><a href="#architecture-1" id="markdown-toc-architecture-1">Architecture</a></li>
      <li><a href="#action-representation" id="markdown-toc-action-representation">Action Representation</a></li>
      <li><a href="#training" id="markdown-toc-training">Training</a></li>
    </ul>
  </li>
  <li><a href="#octo-uc-berkeley" id="markdown-toc-octo-uc-berkeley">Octo (UC Berkeley)</a>    <ul>
      <li><a href="#overview-2" id="markdown-toc-overview-2">Overview</a></li>
      <li><a href="#architecture-2" id="markdown-toc-architecture-2">Architecture</a></li>
      <li><a href="#action-representation-1" id="markdown-toc-action-representation-1">Action Representation</a></li>
      <li><a href="#training-1" id="markdown-toc-training-1">Training</a></li>
    </ul>
  </li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a>    <ul>
      <li><a href="#performance-vs-scale" id="markdown-toc-performance-vs-scale">Performance vs Scale</a></li>
      <li><a href="#discrete-tokens-for-actions-vs-continous-diffusion" id="markdown-toc-discrete-tokens-for-actions-vs-continous-diffusion">Discrete Tokens for Actions vs Continous Diffusion</a></li>
      <li><a href="#accessibility" id="markdown-toc-accessibility">Accessibility</a></li>
    </ul>
  </li>
  <li><a href="#limitations" id="markdown-toc-limitations">Limitations</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>The core of computer vision for robotics is utilizing deep learning to allow robots to perceive, understand, and interact with the physical world. However, training models to understand the physical world has proven to be extremely difficult, with issues stemming from the lack of training embodied data (robot centric videos, human interactions in tasks). Models also struggle with relating semantic information to the physical world to conduct actions and generalizing or extending to new environments. Vision language action models recently emerged at the forefront of end-to-end models handling visual input, language instruction, and outputting robot actions.</p>

<h2 id="background-on-vlas">Background on VLAs</h2>

<p>Various models typically utilized at most two of these three components: vision, language and action. For example R3M which is a universal visual representation utilizing video and language during training, but only serves as a backbone and relies on training a separate downstream policy for robot action. Another example is Track2Act, which utilizes Vision and Action, with 2D track predictions for trajectories of objects without language/explicit goal set.</p>

<p>Vision Language Action models utilize all 3 in a multimodal pipeline, to concurrently process image/video + language (captions) to output a robot action. Drawing inspiration from transformers, VLA models use a fine tuned vision language model to output actions instead of text answers. In this report, we discuss and compare RT-2 (55B param), OpenVLA (7B param) which use vision language backbones, with Octo, a compact (93M param) generalist robot policy that omits a vision language backbone.</p>

<h2 id="rt-2-google-deepmind">RT-2 (Google Deepmind)</h2>

<h3 id="overview">Overview</h3>

<p>RT-2 was the first model to indicate success with using pretrained vision language models to directly output robot actions. RT-2 utilized PaLI-X (larger model, overall better performance) and PaLM-E (smaller, performed better on math reasoning) for its backbone. Building on its predecessor, RT-1, which directly trained on robot data, RT-2 draws inspiration from RT-1 on the concept of discretization of action as tokens, demonstrating that robot action can be represented similarly to how VLMs tokenize and process natural language for generation.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team05/RT-2_graphic.png" alt="RT-2 Architecture" /></p>
<p><em>Fig 1. RT-2 Architecture: Vision Language Model adapted for robot action generation.</em></p>

<h3 id="architecture">Architecture</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">RT-2-PaLI-X-55B</th>
      <th style="text-align: left">RT-2-PaLM-E-12B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Vision Encoder</td>
      <td style="text-align: left">ViT-22B</td>
      <td style="text-align: left">ViT-4B</td>
    </tr>
    <tr>
      <td style="text-align: left">Language Model</td>
      <td style="text-align: left">UL2 (32B encoder-decoder)</td>
      <td style="text-align: left">PaLM (decoder-only)</td>
    </tr>
    <tr>
      <td style="text-align: left">Total Parameters</td>
      <td style="text-align: left">55B</td>
      <td style="text-align: left">12B</td>
    </tr>
  </tbody>
</table>

<h4 id="vision-encoder">Vision Encoder:</h4>

<p>Processes robot camera images into patch embeddings (e.g., 16×16 pixel patches)
Can accept sequences of n images, producing n × k tokens per image, k is number of patches per image which are passed into a projection layer
Pretrained on web-scale image data</p>

<h4 id="language-model-backbone">Language Model Backbone:</h4>

<p>For PaLI-X, the projected image tokens and text tokens are jointly fed into encoder, and the decoder autoregressively generates output
For PaLM-E, projected image tokens are concatenated directly with the text tokens, decoder processes the combined stream
Pretrained on web-scale vision-language tasks</p>

<h3 id="tokenization-method-and-action-output">Tokenization Method and Action Output</h3>

<p>Drawing inspiration from discrete representations for action encodings in RT-1, RT-2 represents the range of motion, into 256 distinct steps, otherwise known as bins. The action space consists of 8 dimensions (6 positional, rotational displacement, extension of gripper, command for termination) are discretized into 256 bins uniformly excluding the termination command. These 256 action bins are mapped to existing tokens in the VLM’s token vocabulary. For example “128” could represent a quantity of speed, so when processing an instruction like “put the apple down” the VLM would output a string of 8 discrete tokens per timestep (e.g.”1 128 91 241 5 101 127 217”). This allows the model to be trained using the standard categorical cross-entropy loss used for text generation, without adding new “action heads” or changing the model architecture.</p>

<h3 id="training-for-next-token-generation">Training for Next Token Generation</h3>

<p>Utilized co-fine-tuning strategy where training batches mix original web data along with robot data for more generalizable policies since it exposes the policy to abstract visual concepts from web data and low level robot actions, instead of just robot actions like in RT-1. Vision language data comes from WebLI dataset, consisting of ~10 billion image-text pairs across 109 languages, filtered down to the top 10% (1 billion examples) based on cross-modal similarity.</p>

<p>Robotics data comes from RT-1 dataset (Brohan et al., 2022), which includes demonstration episodes collected on a mobile manipulator. These episodes are annotated with natural language instructions covering seven core skills: “Pick Object,” “Move Object Near Object,” “Place Object Upright,” “Knock Object Over,” “Open Drawer,” “Close Drawer,” and “Place Object into Receptacle”.</p>

<h3 id="training-configuration">Training Configuration</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Parameter</th>
      <th style="text-align: left">Configuration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Loss Function</td>
      <td style="text-align: left">Categorical Cross-Entropy (Next Token Prediction)</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimization</td>
      <td style="text-align: left">Gradient Descent (Backpropagation)</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning Rate</td>
      <td style="text-align: left">1e-3 (PaLI-X), 4e-4 (PaLM-E)</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch Size</td>
      <td style="text-align: left">2048 (PaLI-X), 512 (PaLM-E)</td>
    </tr>
    <tr>
      <td style="text-align: left">Gradient Steps</td>
      <td style="text-align: left">80K (PaLI-X), 1M (PalM-E)</td>
    </tr>
  </tbody>
</table>

<h2 id="openvla-stanford-uc-berkeley-toyota-research-institute">OpenVLA (Stanford, UC Berkeley, Toyota Research Institute)</h2>

<h3 id="overview-1">Overview</h3>

<p>OpenVLA is an open-source alternative to RT-2, employs the same architectural structure, utilizing a VLM backbone repurposed to handle action tokens, but at a smaller scale. However, notable differences include training purely on the Open-X-Embodiment dataset which contains 1.3M robot trajectories, utilizing a duel vision encoder structure to capture spatial features. OpenVLA also explores fine-tuning strategies for it’s components unlike RT-2, which kept the backbone pretrained weights frozen.</p>

<h3 id="architecture-1">Architecture</h3>

<p>OpenVLA uses a dual-encoder vision system combined with a large language model backbone to process visual inputs and generate robot actions.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team05/OpenVLA_graphic.png" alt="OpenVLA Architecture" /></p>
<p><em>Fig 2. OpenVLA Architecture: Dual vision encoders with Llama 2 backbone for action generation.</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Specification</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Vision Encoders</td>
      <td style="text-align: left">DINOv2 (ViT-L/14) + SigLIP (So400m)</td>
    </tr>
    <tr>
      <td style="text-align: left">Projector</td>
      <td style="text-align: left">2-layer MLP</td>
    </tr>
    <tr>
      <td style="text-align: left">Language Model</td>
      <td style="text-align: left">Llama 2 (7B)</td>
    </tr>
    <tr>
      <td style="text-align: left">Total Parameters</td>
      <td style="text-align: left">~7B</td>
    </tr>
  </tbody>
</table>

<p><strong>Dual Vision Encoders:</strong></p>

<ul>
  <li>DINOv2 (ViT-L/14): Self-supervised encoder for low-level spatial and geometric features (“where things are”)</li>
  <li>SigLIP (So400m): Contrastive vision-language encoder for high-level semantic understanding (“what things are”)</li>
  <li>Features from both encoders are concatenated channel-wise to create a rich visual representation</li>
  <li>Both encoders pretrained on web-scale data</li>
</ul>

<p><strong>Projector:</strong></p>

<ul>
  <li>2-layer MLP</li>
  <li>Maps fused visual features into the language model’s embedding space</li>
</ul>

<p><strong>Language Model Backbone:</strong></p>

<ul>
  <li>Llama 2 (7B parameters, decoder-only)</li>
  <li>Processes concatenated visual tokens and text instruction tokens</li>
  <li>Generates action tokens autoregressively</li>
  <li>Pretrained on web-scale text data</li>
</ul>

<h3 id="action-representation">Action Representation</h3>

<p>OpenVLA adopts RT-2’s “action-as-language” strategy of discretizing dimensions of robot actions into 256 bins. Since Llama’s tokenizer only reserves 100 “special” tokens, the authors chose to override the 256 least used tokens in the vocabulary for simplicity.</p>

<p><strong>Dedicated Action Tokens:</strong></p>

<ul>
  <li>Each token represents a discrete bin of continuous motion, preventing semantic interference between “language words” and “action words”</li>
  <li>Quantization: Same 256-bin uniform discretization per action dimension as RT-2</li>
  <li>Output Format: Generates action tokens that are decoded back to continuous values</li>
</ul>

<h3 id="training">Training</h3>

<p><strong>Dataset:</strong></p>

<p>970,000 trajectories from Open X-Embodiment dataset</p>

<p><strong>Data Curation:</strong></p>

<p>Carefully filtered for high-quality subsets, removing idle actions and low-quality demonstrations to improve training efficiency and model performance.</p>

<p><strong>Training Details:</strong></p>

<ul>
  <li>Fixed learning rate: 2e-5</li>
  <li>AdamW optimizer with gradient descent (backpropagation)</li>
  <li>Significantly more epochs (27) compared to typical VLMs (1-2 epochs)</li>
  <li>Compute: 64 A100 GPUs for approximately 14 days</li>
</ul>

<p>Authors attributed increases in performance to fine-tuning the vision encoder, which captures more fine-grained spatial details about scenes for precise robotic control. Another notable change was utilizing 27 epochs at the final vision language model seeing improvement at each iteration, when in the past, vision language models typically trained on 1-2 epochs through the entire dataset.</p>

<p><strong>Training Configuration:</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Parameter</th>
      <th style="text-align: left">Configuration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Loss Function</td>
      <td style="text-align: left">Categorical Cross-Entropy (Next Token Prediction)</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimization</td>
      <td style="text-align: left">Gradient Descent (Backpropagation)</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning Rate</td>
      <td style="text-align: left">2e-5 (fixed)</td>
    </tr>
    <tr>
      <td style="text-align: left">Epochs</td>
      <td style="text-align: left">27</td>
    </tr>
    <tr>
      <td style="text-align: left">Compute</td>
      <td style="text-align: left">64 A100 GPUs (~14 days)</td>
    </tr>
  </tbody>
</table>

<h2 id="octo-uc-berkeley">Octo (UC Berkeley)</h2>

<h3 id="overview-2">Overview</h3>

<p>Octo is an open-source Generalist Robot Policy with a fundamentally different approach from VLAs like RT-2. Rather than adapting a massive pretrained VLM, Octo is trained from scratch as a compact transformer designed specifically for robot action. Octo aims to address the flaws of large robot policies trained on diverse datasets such as constrained downstream policies (e.g. restrictive inputs from single camera view) and serve as a “true” general robot policy, allowing different camera configurations, different robots, language vs goal images, and new robot setups.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team05/Octo_graphic.png" alt="Octo Architecture" /></p>
<p><em>Fig 3. Octo Architecture: Compact transformer with diffusion-based action decoder.</em></p>

<h3 id="architecture-2">Architecture</h3>

<p>Octo features a custom transformer architecture with modular input-output mechanisms:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Octo-Small</th>
      <th style="text-align: left">Octo-Base</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Transformer Parameters</td>
      <td style="text-align: left">27M</td>
      <td style="text-align: left">93M</td>
    </tr>
    <tr>
      <td style="text-align: left">Vision Encoder</td>
      <td style="text-align: left">ViT-Small (CNN Stem)</td>
      <td style="text-align: left">ViT-Small (CNN Stem)</td>
    </tr>
    <tr>
      <td style="text-align: left">Language Encoder</td>
      <td style="text-align: left">T5-Base (frozen)</td>
      <td style="text-align: left">T5-Base (frozen)</td>
    </tr>
    <tr>
      <td style="text-align: left">Action Head</td>
      <td style="text-align: left">Diffusion</td>
      <td style="text-align: left">Diffusion</td>
    </tr>
  </tbody>
</table>

<p><strong>Vision Encoder:</strong></p>

<ul>
  <li>Transformer Backbone (ViT-Style) with a lightweight CNN Stem (SmallStem16) for tokenization</li>
  <li>Processes images into patch embeddings</li>
  <li>Supports multiple camera views (e.g., third-person and wrist cameras)</li>
  <li>Trained from scratch on robot trajectories</li>
</ul>

<p><strong>Language Encoder:</strong></p>

<ul>
  <li>Frozen T5-Base model (111M parameters)</li>
  <li>Encodes natural language instructions into embeddings</li>
  <li>Language conditioning is optional as Octo can also accept goal images</li>
</ul>

<p><strong>Transformer Backbone:</strong></p>

<ul>
  <li>Custom transformer (27M for Small, 93M for Base)</li>
  <li>Processes concatenated vision and language tokens</li>
  <li>Block-wise Attention Masking: Handles missing modalities (e.g., robots without wrist cameras) by masking specific input groups during training</li>
  <li>Trained from scratch on robot data (no web pretraining)</li>
</ul>

<p><strong>Diffusion Action Head:</strong></p>

<ul>
  <li>Replaces the language modeling head with a diffusion decoder</li>
  <li>Outputs continuous actions rather than discrete tokens</li>
  <li>Iteratively denoises Gaussian noise into precise action trajectories</li>
</ul>

<h3 id="action-representation-1">Action Representation</h3>

<p>Octo converts language instructions ℓ, goals g, and observation sequences o₁,…,oₕ into tokens [Tₗ, Tᵧ, Tₒ] using modality-specific tokenizers which are processed by the transformer backbone and fed into a readout head to output desired actions. The block-wise masking forces tokens to attend to tokens from same or earlier timesteps, and tokens corresponding to non-existing observations are completely masked to handle missing modalities. Unlike RT-2 and OpenVLA which discretize actions into tokens, Octo outputs continuous actions via diffusion, utilizing denoising for action decoding for the prediction of action “chunks” ( e.g., chunk of 4 timesteps).</p>

<h3 id="training-1">Training</h3>

<p>Unlike RT-2 and OpenVLA which fine-tune massive pretrained VLMs, Octo is trained entirely from scratch on robot data. This design choice trades web-scale semantic knowledge for:</p>

<ul>
  <li>Faster training (14 hours vs. days/weeks for VLAs)</li>
  <li>Smaller compute requirements (single TPUv4-128 podvs. multi-pod setups)</li>
  <li>Full control over the learned representations</li>
</ul>

<p><strong>Dataset:</strong></p>

<p>800,000 trajectories from Open X-Embodiment dataset covering 9 distinct robot embodiments</p>

<p><strong>Training Details:</strong></p>

<ul>
  <li>AdamW optimizer, inverse square root learning rate decay</li>
  <li>Weight decay 0.1, gradient clipping 1.0</li>
  <li>300k steps, batch size 2048 on TPUv4-128 (~14 hours)</li>
</ul>

<p style="width: 800px; max-width: 100%;">Diffusion denoising process for action generation
<img src="/CS163-Projects-2025Fall/assets/images/team05/Octo_denoising.png" alt="Octo Diffusion Denoising" /></p>

<p>The action head is trained with the standard denoising objective loss function:</p>

\[\mathcal{L}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \|\epsilon - \epsilon_\theta(x_t, t)\|^2 \right]\]

<p>This trains the model to predict the noise \(\epsilon\) added to clean actions \(x_0\), enabling iterative denoising at inference time. The diffusion formulation for action generation is:</p>

\[X^{k+1} = \alpha X^k - \gamma \epsilon_\theta(X^k, e) + \sigma z\]

<p>where \(X^k\) is the result of the previous denoised stage of the action, \(k\) is the current denoising step, \(e\) is the output embedding from the transformer, and \(\alpha\), \(\gamma\), \(\sigma\) are hyperparameters for the noise schedule.</p>

<p><strong>Why Diffusion Over Alternatives:</strong></p>

<p>Simple MSE was also tried but didn’t perform as well since it incentivizes the robot to an “average” behavior, and can’t handle uncertainty or scenarios with multiple correct choices. Likewise, with discretization/cross-entropy, the robot was observed with less precision, moving to the nearest predefined bin. Diffusion treats actions as continuous values to retain precision and force the denoising process into a specific valid action.</p>

<h2 id="discussion">Discussion</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Aspect</th>
      <th style="text-align: left">RT-2-X (55B)</th>
      <th style="text-align: left">OpenVLA (7B)</th>
      <th style="text-align: left">Octo (93M)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Parameters</strong></td>
      <td style="text-align: left">55B</td>
      <td style="text-align: left">7B</td>
      <td style="text-align: left">93M (Base)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Architecture</strong></td>
      <td style="text-align: left">VLM (PaLI-X)</td>
      <td style="text-align: left">VLM (Llama 2)</td>
      <td style="text-align: left">Transformer (Scratch)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Vision Encoder</strong></td>
      <td style="text-align: left">ViT-22B (Pre-trained)</td>
      <td style="text-align: left">SigLIP + DINOv2 (Fused)</td>
      <td style="text-align: left">CNN + Transformer (Scratch)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Action Output</strong></td>
      <td style="text-align: left">Discrete (256 bins)</td>
      <td style="text-align: left">Discrete (256 bins)</td>
      <td style="text-align: left">Continuous (Diffusion)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Training Data</strong></td>
      <td style="text-align: left">130k Robot + Web</td>
      <td style="text-align: left">970k Robot (OXE)</td>
      <td style="text-align: left">800k Robot (OXE)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Open Source</strong></td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">Yes</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Overall Success Rate (tasks on WidowX Robot)</strong></td>
      <td style="text-align: left">50.6%</td>
      <td style="text-align: left"><strong>70.6%</strong></td>
      <td style="text-align: left">20.0%</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Key Strength</strong></td>
      <td style="text-align: left">Semantic Generalization</td>
      <td style="text-align: left">Physical Generalization</td>
      <td style="text-align: left">Inference Speed / Motion</td>
    </tr>
  </tbody>
</table>

<h3 id="performance-vs-scale">Performance vs Scale</h3>

<p>The comparison of 3 models with various different parameter counts demonstrates that scaling parameter count alone doesn’t necessarily determine model performance the most, as architectural decisions and fine-tuning, and other strategies still play an important role. For leveled comparisons, since RT-2 is significantly larger and trained on a different dataset, comparisons are made against the RT-2-X model (trained on 350k trajectories in Open-X-Embodiment) so all 3 models use the same dataset. For example, in OpenVLA fine-tuning the weights in vision encoder portion and uses much more epochs, whereas RT-2 keeps the frozen weights in vision encoder, and although undisclosed, likely used significantly less epochs due to model size, yet Open-VLA on overall performed 20% better than RT-2-X. Another factor that could be considered between these two models was OpenVLA’s data curation strategy from the Open-X-Embodiment compared to RT-2’s co-mixing of robot and web scale data.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team05/BridgeDataChart.png" alt="BridgeChart " /></p>
<p><em>Out-of-the-Box-Evals.</em></p>

<h3 id="discrete-tokens-for-actions-vs-continous-diffusion">Discrete Tokens for Actions vs Continous Diffusion</h3>

<p>The decision represents a tradeoff primarily between high-level semantic reasoning with a large vision language model backbone and precise low-level control with a diffusion policy. Octo was also evaluated using goal image conditioning and found it had 25% higher success rate than when being evaluated with language conditioning.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team05/OctoCharts.png" alt="" /></p>
<p><em>Comparisons between Octo and other models.</em></p>

<p>Since RT-2-X is closed-source comparisons required a hybrid approach where RT-2-X metrics were directly sourced from literature, Octo’s authors collabed with Google Deepmind to run Octo on the RT-1 robot. Another factor is the evaluated tasks were seen during pre-training, which likely explains the strong performance here compared to RT-2-X compared to the out-of-the-box evaluations shown earlier. Regardless, Octo still demonstrates the ability of flexibility running on various different robots, as well as it’s efficiency with 93M params compared to 55B in RT-2-X.</p>

<h3 id="accessibility">Accessibility</h3>

<p>A key distinction between these models is accessibility. RT-2 remains closed-source
with no public weights, training code, or fine-tuning support—limiting its use to
Google’s internal research. On the other hand, OpenVLA and Octo released full model
weights, training pipelines, and fine-tuning scripts with public github repositories. OpenVLA additionally supports
LoRA fine-tuning on consumer GPUs (single RTX 4090) and 4-bit quantization for
deployment, reducing the barrier to entry for robotics researchers without
datacenter-scale compute.</p>

<h2 id="limitations">Limitations</h2>

<p>For RT-2, despite incorporating web-scale data along with its robot data, it is still unable to learn any new motions and is limited to skills seen in the robot data, rather learning new ways to apply those skills. Computations in real-time inference can become a bottleneck due to the sheer size of the model and requires direct integration with TPU-specific hardware.</p>

<p>For OpenVLA, the authors discuss only being able to process single-image input, despite most modern robots having multiple camera views so surrounding spatial awareness is actually limited despite having Dinov2 as an additional image encoder. Inference time was another cited issue especially in high-frequency setups, same as RT-2. Both RT-2 and OpenVLA also share the issue of quantization error for extremely fine-grained and precise movements that can’t be represented in the 256 bins.</p>

<p>For Octo, the authors attributed issues to fine tuning due to the characteristics of training data, performing better in specific camera views despite being general purpose. As illustrated earlier, another limitation was it’s lack of semantioc reasoning capabilities that come from web-scale pretrained VLMs, greatly affecting its performance on language conditioned policy compared to goal conditioned policy.</p>

<p>Some shared limitations include all three models primarily being evaluated on tabletop manipulation with 7-DoF arms, but generalizations to other robot embodiments remains unexplored, (humanoids, quadrupeds, mobile manipulators)</p>

<h2 id="references">References</h2>

<p>[1] Kim, Moo Jin, et al. “OpenVLA: An Open-Source Vision-Language-Action Model.” arXiv preprint arXiv:2406.09246 (2024).</p>

<p>[2] Octo Model Team, et al. “Octo: An Open-Source Generalist Robot Policy.” arXiv preprint arXiv:2405.12213 (2024).</p>

<p>[3] Brohan, Anthony, et al. “RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.” arXiv preprint arXiv:2307.15818 (2023).</p>

<p>[4] Sapkota, Ranjan, et al. “Vision-Language-Action Models: Concepts, Progress, Applications and Challenges.” arXiv preprint arXiv:2505.04769 (2025).</p>

<hr />]]></content><author><name>Brian Liu</name></author><summary type="html"><![CDATA[The core of computer vision for robotics is utilizing deep learning to allow robots to perceive, understand, and interact with the physical world. This report explores Vision Language Action (VLA) models that combine visual input, language instruction, and robot actions in end-to-end architectures.]]></summary></entry><entry><title type="html">Visuomotor Policy</title><link href="/CS163-Projects-2025Fall/2025/12/13/team22-visuomotor-policy.html" rel="alternate" type="text/html" title="Visuomotor Policy" /><published>2025-12-13T00:00:00+00:00</published><updated>2025-12-13T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/13/team22-visuomotor-policy</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/13/team22-visuomotor-policy.html"><![CDATA[<blockquote>
  <p>Visuomotor Policy Learning studies how an agent can map high-dimensional visual observations (e.g., camera images) to motor commands in order to solve sequential decision-making tasks. In this project, we focus on settings motivated by autonomous driving and robotic manipulation, and survey modern learning-based approaches—primarily imitation learning (IL) and reinforcement learning (RL)—with an emphasis on methods that improve sample efficiency through policy/representation pretraining.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#problem-setup" id="markdown-toc-problem-setup">Problem Setup</a>    <ul>
      <li><a href="#interaction-and-trajectories" id="markdown-toc-interaction-and-trajectories">Interaction and Trajectories</a></li>
      <li><a href="#objectives" id="markdown-toc-objectives">Objectives</a></li>
    </ul>
  </li>
  <li><a href="#approaches" id="markdown-toc-approaches">Approaches</a>    <ul>
      <li><a href="#imitation-learning" id="markdown-toc-imitation-learning">Imitation Learning</a></li>
      <li><a href="#reinforcement-learning" id="markdown-toc-reinforcement-learning">Reinforcement Learning</a></li>
      <li><a href="#policy-and-representation-pretraining" id="markdown-toc-policy-and-representation-pretraining">Policy and Representation Pretraining</a></li>
    </ul>
  </li>
  <li><a href="#policy-pretraining" id="markdown-toc-policy-pretraining">Policy Pretraining</a>    <ul>
      <li><a href="#action-conditioned-contrastive-policy-pretraining" id="markdown-toc-action-conditioned-contrastive-policy-pretraining">Action-Conditioned Contrastive Policy Pretraining</a></li>
      <li><a href="#pretraining-versus-learning-from-scratch" id="markdown-toc-pretraining-versus-learning-from-scratch">Pretraining Versus Learning-from-Scratch</a></li>
      <li><a href="#summary-and-discussion" id="markdown-toc-summary-and-discussion">Summary and Discussion</a></li>
    </ul>
  </li>
  <li><a href="#diffusion-policy" id="markdown-toc-diffusion-policy">Diffusion Policy</a>    <ul>
      <li><a href="#problem-formulation-1" id="markdown-toc-problem-formulation-1">Problem formulation</a></li>
      <li><a href="#model-architecture-overview" id="markdown-toc-model-architecture-overview">Model architecture overview</a></li>
      <li><a href="#visual-encoder-deep-dive" id="markdown-toc-visual-encoder-deep-dive">Visual encoder deep dive</a></li>
      <li><a href="#diffusion-model-deep-dive" id="markdown-toc-diffusion-model-deep-dive">Diffusion model deep dive</a></li>
      <li><a href="#results--limitations" id="markdown-toc-results--limitations">Results &amp; Limitations</a></li>
    </ul>
  </li>
  <li><a href="#3d-diffusion-policy-dp3-4" id="markdown-toc-3d-diffusion-policy-dp3-4">3D Diffusion Policy (DP3) [4]</a>    <ul>
      <li><a href="#issues-with-previous-work" id="markdown-toc-issues-with-previous-work">Issues with Previous Work</a></li>
      <li><a href="#solution" id="markdown-toc-solution">Solution</a></li>
      <li><a href="#the-decision-module-4" id="markdown-toc-the-decision-module-4">The Decision Module [4]</a></li>
      <li><a href="#training-4" id="markdown-toc-training-4">Training [4]</a></li>
      <li><a href="#ablation-studies-4" id="markdown-toc-ablation-studies-4">Ablation Studies [4]</a></li>
      <li><a href="#results-4" id="markdown-toc-results-4">Results [4]</a></li>
      <li><a href="#discussion-and-impact-of-dp3-4" id="markdown-toc-discussion-and-impact-of-dp3-4">Discussion and Impact of DP3 [4]</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Visuomotor policy learning aims to learn a control policy directly from visual inputs, enabling an agent to perceive its environment and take actions that accomplish a task. Compared with classical pipelines that rely on modular perception, planning, and control, end-to-end visuomotor learning offers a unified framework that can, in principle, learn task-relevant features automatically and handle complex sensor observations.</p>

<p>However, learning from pixels is challenging: visual observations are high-dimensional, supervision is often limited or expensive to collect, and errors can compound over time as the agent visits states not covered by training data. These challenges are especially pronounced in real-world robotics and driving, where interaction data can be costly, slow, or safety-critical. As a result, recent work has emphasized data-efficient learning, including stronger IL/RL baselines, better augmentations, and pretraining strategies that improve downstream policy learning.</p>

<h2 id="problem-setup">Problem Setup</h2>

<p>We model visuomotor control as a sequential decision-making problem. Let the environment evolve over discrete time steps \(t = 1,2,\dots,T\). The agent receives an observation \(o_t \in \mathcal{O}\) (e.g., an RGB image or stacked frames), chooses an action \(a_t \in \mathcal{A}\) (e.g., steering/throttle or robot joint commands), and the environment transitions to the next state while emitting reward \(r_t\) (for RL) or providing supervision via expert actions (for IL).</p>

<h3 id="interaction-and-trajectories">Interaction and Trajectories</h3>

<p>A (partial-observation) trajectory can be written as:</p>

<p>\(\)
\tau = {(o_1, a_1, r_1), (o_2, a_2, r_2), \dots, (o_T, a_T, r_T)}.
\(\)</p>

<p>A parametrized policy \(\pi_\theta(a_t \mid o_t)\) maps observations to either:</p>

<ul>
  <li><strong>Deterministic actions</strong>: \(a_t = \pi_\theta(o_t)\), or</li>
  <li><strong>Stochastic actions</strong>: \(a_t \sim \pi_\theta(\cdot \mid o_t).\)</li>
</ul>

<h3 id="objectives">Objectives</h3>

<p><strong>Reinforcement Learning (RL).</strong> The goal is to maximize expected discounted return:</p>

<p>\(\)
\max_\theta ; \mathbb{E}<em>{\tau \sim \pi</em>\theta}\left[\sum_{t=1}^{T} \gamma^{t-1} r_t\right],
\(\)</p>

<p>where \(\gamma \in (0,1]\) is the discount factor.</p>

<p><strong>Imitation Learning (IL).</strong> Given an offline dataset of expert demonstrations
\(\mathcal{D} = {(o_i, a_i^{*})}_{i=1}^{N}\), behavior cloning typically minimizes a supervised loss:</p>

<p>\(\)
\min_\theta ; \mathbb{E}<em>{(o,a^</em>) \sim \mathcal{D}}\left[\mathcal{L}\big(\pi<em>\theta(o), a^</em>\big)\right],
\(\)</p>

<p>where \(\mathcal{L}\) is often MSE (continuous control) or cross-entropy (discrete actions).</p>

<h2 id="approaches">Approaches</h2>

<p>Broadly, modern visuomotor policy learning can be organized into three families: imitation learning, reinforcement learning, and pretraining-based methods that improve either of the above.</p>

<h3 id="imitation-learning">Imitation Learning</h3>

<p>Imitation learning learns directly from expert behavior (e.g., human driving or teleoperated robot demonstrations). The simplest and most common approach is behavior cloning (BC), which treats policy learning as supervised learning from \(o \mapsto a^*\).</p>

<p>A key limitation is distribution shift: small errors can cause the policy to drift into states not represented in the dataset, leading to compounding mistakes. Methods such as dataset aggregation (e.g., iteratively collecting corrections) and stronger regularization/augmentation are often used to improve robustness, especially in driving-like settings where stable, human-aligned behavior is preferred.</p>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<p>Reinforcement learning optimizes behavior through interaction, using reward feedback to explore and improve. RL can achieve superhuman performance in principle, but often suffers from:</p>

<ul>
  <li>high sample complexity (many environment interactions),</li>
  <li>sensitivity to reward design,</li>
  <li>instability during training (especially from pixels).</li>
</ul>

<p>In robotics, RL is widely used in simulation and increasingly combined with offline data, strong augmentations, and representation learning to reduce real-world interaction requirements.</p>

<h3 id="policy-and-representation-pretraining">Policy and Representation Pretraining</h3>

<p>Because real-world interaction is expensive, many recent methods pretrain visual representations or policy components before downstream IL/RL. Pretraining can leverage:</p>

<ul>
  <li>large-scale unlabeled videos/images,</li>
  <li>self-supervised objectives (e.g., contrastive learning),</li>
  <li>action-aware objectives when actions (or pseudo-actions) are available.</li>
</ul>

<p>This project’s later section (“Policy Pretraining”) focuses on two representative perspectives:</p>

<ol>
  <li>Action-conditioned contrastive pretraining that explicitly aligns representations with control-relevant semantics, and</li>
  <li>A critical re-evaluation showing that strong learning-from-scratch baselines (with proper augmentation and architecture choices) can match or exceed frozen pretraining in many settings—highlighting that <em>when</em> pretraining helps depends heavily on domain alignment and experimental controls.</li>
</ol>

<h2 id="policy-pretraining">Policy Pretraining</h2>

<p>Learning visuomotor policies directly from interaction data is often expensive, particularly in real-world robotic and autonomous driving settings. As a result, many approaches seek to improve sample efficiency by pretraining visual representations or policy components prior to downstream imitation or reinforcement learning. However, existing methods differ substantially in both how pretraining is performed and whether it consistently benefits policy learning.</p>

<p>In this section, we examine two complementary perspectives on policy pretraining. The first presents an action-conditioned contrastive learning framework that explicitly aligns representation learning with control objectives [1]. The second revisits the effectiveness of visual pretraining by comparing it against strong learning-from-scratch baselines, providing a critical assessment of when and why pretraining improves visuomotor policy learning [2].</p>

<h3 id="action-conditioned-contrastive-policy-pretraining">Action-Conditioned Contrastive Policy Pretraining</h3>

<p>In <em>Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining</em> [1], Zhang et al. introduce Action-Conditioned Contrastive Policy Pretraining (ACO), a contrastive representation learning framework designed specifically for visuomotor policy learning. The work builds on MoCo-style self-supervised learning, but argues that standard visual contrastive objectives are insufficient for control tasks because they primarily encode appearance-based features rather than action-relevant semantics. ACO modifies the contrastive objective to explicitly incorporate action information, enabling representations that are better aligned with downstream imitation and reinforcement learning.</p>

<h4 id="motivation">Motivation</h4>

<p>Traditional visual pretraining methods, such as ImageNet supervision or instance-level contrastive learning, aim to distinguish images based on visual content. However, in visuomotor control, the key requirement is not visual discrimination per se, but the ability to identify which aspects of an observation are predictive of the correct action. For example, road curvature and lane markings are critical for driving decisions, whereas lighting conditions or background buildings are largely irrelevant. The authors hypothesize that contrastive pretraining should group observations that require similar actions, even if they differ significantly in visual appearance.</p>

<h4 id="method-overview">Method Overview</h4>

<p>ACO extends standard contrastive learning by combining two types of positive pairs:</p>

<ul>
  <li>Instance Contrastive Pairs (ICP): as in MoCo, two augmented views of the same image form a positive pair, encouraging general visual discriminability.</li>
  <li>Action Contrastive Pairs (ACP): two different images are treated as a positive pair if their associated actions are sufficiently similar (e.g., steering angles within a predefined threshold), regardless of visual similarity.</li>
</ul>

<p>By jointly optimizing ICP and ACP objectives, ACO encourages representations that preserve general visual structure while emphasizing action-consistent features. This dual-objective design allows the model to retain the benefits of instance discrimination while mitigating its tendency to overfit to appearance-level features.</p>

<h4 id="pseudo-action-label-generation">Pseudo Action Label Generation</h4>

<p>A challenge in applying ACP at scale is the lack of action annotations in in-the-wild data such as YouTube videos. To address this, Zhang et al. train an inverse dynamics model on the NuScenes dataset, where ground-truth ego-motion actions are available. Given consecutive frames \((o_t, o_{t+1})\), the inverse dynamics model predicts the corresponding control action</p>

\[\hat{a}*t = h(o_t, o*{t+1}),\]

<p>where \(h\) is trained using supervised regression. The trained inverse dynamics model is then applied to unlabeled YouTube driving videos to generate pseudo action labels, enabling large-scale action-conditioned pretraining without manual annotation. Importantly, optical flow is used as input to the inverse dynamics model instead of raw RGB frames, improving robustness to appearance variation and reducing prediction error.</p>

<p style="width: 500px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/aco_inverse_dynamics.png" alt="Inverse Dynamics Pipeline" /></p>
<p><em>Fig. 1. Inverse dynamics training on NuScenes and pseudo action label generation on YouTube driving videos [1].</em></p>

<h4 id="training-architecture-and-objective">Training Architecture and Objective</h4>

<p>The ACO architecture follows the MoCo framework and consists of a query encoder \(f\) and a momentum-updated key encoder \(f_m\). A shared visual backbone feeds into two projection heads: \(g_{\text{ins}}\) for the ICP objective and \(g_{\text{act}}\) for the ACP objective. Momentum-smoothed counterparts \(g_{\text{ins},m}\) and \(g_{\text{act},m}\) are used to populate a dictionary of negative samples.</p>

<p>The ICP loss follows the standard InfoNCE formulation:</p>

\[\mathcal{L}_{\text{ins}} =

* \log
  \frac{\exp(z^q \cdot z^+ / \tau)}
  {\sum_{z \in \mathcal{N}(z^q)} \exp(z^q \cdot z / \tau)},\]

<p>where \(z^q\) is the query embedding, \(z^+\) is the positive key, \(\mathcal{N}(z^q)\) is the set of negatives, and \(\tau\) is a temperature parameter.</p>

<p>For ACP, the positive set is defined based on action similarity:</p>

\[\mathcal{P}_{\text{act}}(z^q) =

{ z \mid \lVert \hat{a} - \hat{a}^q \rVert &lt; \epsilon,\ (z,\hat{a}) \in \mathcal{K} },\]

<p>where \(\epsilon\) is a predefined action-distance threshold and \(\mathcal{K}\) denotes the key set. The ACP loss is then given by</p>

\[\mathcal{L}_{\text{act}} =

* \log
  \frac{\sum_{z^+ \in \mathcal{P}*{\text{act}}(z^q)} \exp(z^q \cdot z^+ / \tau)}
  {\sum*{z \in \mathcal{N}(z^q)} \exp(z^q \cdot z / \tau)}.\]

<p>The final training objective is a weighted combination of the two losses:</p>

\[\mathcal{L} =

\lambda_{\text{ins}} \mathcal{L}*{\text{ins}}
+
\lambda*{\text{act}} \mathcal{L}_{\text{act}}.\]

<p style="width: 500px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/aco_architecture.png" alt="ACO Architecture" /></p>
<p><em>Fig. 2. ACO architecture showing joint optimization of instance contrastive and action-conditioned contrastive objectives [1].</em></p>

<h4 id="experimental-results">Experimental Results</h4>

<p>The authors evaluate ACO on downstream imitation learning tasks in the CARLA driving simulator, comparing against random initialization, autoencoder pretraining, ImageNet pretraining, and MoCo. Across all demonstration sizes, ACO consistently outperforms the baselines, with the performance gap most pronounced in low-data regimes. These results indicate that action-conditioned pretraining substantially improves sample efficiency and leads to representations that are better aligned with downstream control objectives.</p>

<p style="width: 500px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/aco_results.png" alt="ACO Imitation Learning Results" /></p>
<p><em>Fig. 3. Imitation learning performance under varying demonstration sizes for different pretraining methods [1].</em></p>

<h3 id="pretraining-versus-learning-from-scratch">Pretraining Versus Learning-from-Scratch</h3>

<p>In <em>On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline</em> [2], Hansen et al. provide a systematic re-evaluation of visual policy pretraining by comparing frozen pretrained representations against strong learning-from-scratch baselines. Rather than proposing a new pretraining method, this work examines whether commonly used pretrained visual encoders offer consistent advantages compared to baselines.</p>

<h4 id="problem-formulation">Problem Formulation</h4>

<p>Learning-from-scratch refers to training a randomly initialized encoder \(f_\theta\) jointly with a policy head using only task-specific data:</p>

\[\theta^* = \arg\min_\theta ; \mathbb{E}*{(o,a)\sim\mathcal{D}} \left[ \ell(\pi*\theta(o), a) \right],\]

<p>where \(\ell\) denotes a behavior cloning or reinforcement learning objective.</p>

<p>In contrast, frozen pretraining fixes a pretrained encoder \(f_{\text{pre}}\) and optimizes only the policy parameters:</p>

\[\theta^* = \arg\min_\theta ; \mathbb{E}*{(o,a)\sim\mathcal{D}} \left[ \ell(\pi*\theta(f_{\text{pre}}(o)), a) \right].\]

<p>The authors emphasize that many prior works compare pretrained models against weak learning-from-scratch baselines that lack sufficient capacity or data augmentation.</p>

<h4 id="experimental-setup-and-findings">Experimental Setup and Findings</h4>

<p>Hansen et al. evaluate learning-from-scratch and frozen pretraining across multiple domains, including Adroit, DMControl, PixMC, and real-world robot manipulation, using behavior cloning, PPO, and DrQ-v2. The learning-from-scratch baselines employ shallow convolutional encoders paired with strong data augmentation, such as random shift and color jitter:</p>

\[o' \sim \mathcal{T}(o),\]

<p>which substantially improves robustness and generalization.</p>

<p style="width: 500px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/lfs_vs_pretraining.png" alt="LfS vs Pretraining Overview" /></p>
<p><em>Fig. 4. Comparison of learning-from-scratch and frozen pretrained representations across multiple domains [2].</em></p>

<p>Across most tasks and learning algorithms, learning-from-scratch with augmentation matches or exceeds the performance of frozen pretrained representations. Frozen pretraining provides only limited benefits in very low-data regimes, and no single pretrained representation consistently dominates across domains. The authors attribute this in part to a domain gap between pretraining data (real-world images and videos) and downstream environments (often simulated).</p>

<p>The authors further show that finetuning pretrained encoders on in-domain data,</p>

\[\theta^* = \arg\min_\theta ; \mathbb{E}*{(o,a)\sim\mathcal{D}} \left[ \ell(\pi*\theta(f_\theta(o)), a) \right],\]

<p>can outperform both frozen pretraining and learning-from-scratch when combined with strong augmentation, highlighting the importance of domain adaptation.</p>

<h3 id="summary-and-discussion">Summary and Discussion</h3>

<p>Taken together, the two works reviewed in this section offer complementary perspectives on policy pretraining for visuomotor control. Action-Conditioned Contrastive Policy Pretraining [1] demonstrates that pretraining can substantially improve downstream policy learning when the objective is explicitly aligned with action semantics, rather than relying solely on appearance-based visual similarity. By incorporating action-conditioned positives and leveraging large-scale unlabeled video data, ACO learns representations that are more directly relevant to control, yielding significant gains in low-data imitation and reinforcement learning settings.</p>

<p>In contrast, Hansen et al. [2] provide a critical re-evaluation of visual policy pretraining, showing that frozen pretrained representations do not consistently outperform strong learning-from-scratch baselines when data augmentation and architectural choices are properly controlled. Their findings highlight the importance of domain alignment and adaptation, suggesting that generic visual pretraining alone is insufficient to guarantee improvements in visuomotor policy learning.</p>

<p>Together, these results suggest that the effectiveness of policy pretraining depends not only on dataset scale, but more critically on the alignment between the pretraining objective and downstream control tasks. While action-aware and task-aligned pretraining objectives show clear promise, future work must carefully consider domain gaps, finetuning strategies, and strong baseline comparisons to fully realize the benefits of pretraining in visuomotor learning.</p>

<h2 id="diffusion-policy">Diffusion Policy</h2>

<p>Diffusion Policy is new paradigm introduced in 2023 by Chi et al [3]. It reframes visuomotor policy as a conditional diffusion process [3] over action sequences, conditioned on a short history of observations. By generating actions using iterative denoising, this new approach allows for more expressive multi-modal, high-dimensional manipulation, while improving temporal consistency in closed-loop control. In fact, the authors found that Diffusion Policy outperformed previous SoTA policies by 46.9% on average.</p>

<h3 id="problem-formulation-1">Problem formulation</h3>

<p>Diffusion Policy is formulated as an offline visuomotor imitation learning problem. The goal is to learn a policy which maps camera observations to actions.</p>

<p>Diffusion Policy makes use of three distinct horizons: the <strong>observation horizon</strong> $T_o$, the <strong>prediction horizon</strong> $T_p$, and the <strong>action execution horizon</strong> $T_a$.</p>

<p>At some time $t$, Diffusion Policy doesn’t predict a single action $a_t$, but instead an action sequence $A_t$, conditioned on a recent observation history. More specifically, the model learns to predict</p>

\[A_t=(a_t,a_{t+1}, ...,a_{t+T_p-1})\]

<p>given</p>

\[O_t=(o_{t-T_o},o_{t-T_o+1},...,o_t)\]

<p>However, at inference time, only the first $T_a$ actions from $A_t$ are executed.</p>

<table>
  <tbody>
    <tr>
      <td>More formally, Diffusion Policy trains a conditional denoising diffusion model to learn the conditional distribution $p(A_t</td>
      <td>O_t)$, where $A_t$ is in action space.</td>
    </tr>
  </tbody>
</table>

<p>Prior research [3] has shown that standard supervised policies struggle with the multi-modal and temporally correlated action distributions introduced by manipulation demonstrations in imitation learning. Diffusion Policy improves on that, due to DDPM’s inherent capabilities with high-dimensional outputs [3], as well as predicting action sequences instead of individual actions.</p>

<h3 id="model-architecture-overview">Model architecture overview</h3>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/DiffusionPolicyArchitecture.png" alt="Ablation_3D_Representation" /></p>
<p><em>Fig 5. Diffusion Policy Model Architecture</em> [3].</p>

<p>At a high level, at each time step $t$, Diffusion Policy does the following:</p>

<ol>
  <li>The <strong>visual encoder</strong> takes in the history of observations (images) $O_t$, and produces latent features $z_t$.</li>
  <li>The <strong>diffusion model</strong> diffuses the action sequence ${A_t}^{(k)}$, which at each denoising iteration $k$ is conditioned on $z_t$. It is worth noting that the same latents are reused for each diffusion time step $k$, because unlike some previous methods [3], Diffusion Policy is not predicting the joint distribution $p(A_t, O_t)$.</li>
</ol>

<p>The controller then executes the first $T_a$ actions from $A_t$.</p>

<p>Let’s now do a deep dive into the two main components of Diffusion Policy.</p>

<h3 id="visual-encoder-deep-dive">Visual encoder deep dive</h3>

<p>At each time step $t$, the visual encoder takes in the history of images $O_t$, and produces a sequence of embeddings $z_t$ which will be used as conditioning for the diffusion model.</p>

<p>The authors found that a slightly modified ResNet-18, without pre-training, performed the best for the visual encoder module. The model used differs from the regular ResNet-18 in the following ways:</p>

<ol>
  <li>It uses spatial SoftMax pooling instead of global average pooling, to maintain spatial information [3].</li>
  <li>It replaces BatchNorm with GroupNorm, to stabilize training [3].</li>
</ol>

<p>The outputs $z_t$ from the visual encoder are then fed into the diffusion model as conditioning.</p>

<h3 id="diffusion-model-deep-dive">Diffusion model deep dive</h3>

<p>The diffusion model is the backbone of Diffusion Policy. At each <strong>diffusion time step</strong> $k$, the diffusion model takes in a noisy action sequence ${A_t}^{(k)}$, the diffusion time step $k$, and the conditioning features $z_t$ from the encoder. It then produces the predicted noise $\hat{\varepsilon}_{\theta}$, which is used to de-noise ${A_t}^{(k)}$ and obtain the next sample ${A_t}^{(k-1)}$.</p>

<p>The authors evaluated two backbone architectures for the diffusion model: CNN and time-series transformer.</p>

<h4 id="cnn-based-diffusion">CNN-based diffusion</h4>

<p>The CNN-based diffusion uses a 1D temporal CNN as its backbone, adapted from previous work on using diffusion for sequence generation [3].</p>

<p>A method called FiLM is then used to condition the CNN on the conditioning latents $z_t$ [3]. FiLM generates a per-layer scale $\gamma(z_t)$ and shift $\beta(z_t)$, which are then applied to the layer’s activations. More specifically, at each layer, we apply the following transformation:</p>

\[h \leftarrow \gamma(z_t) \circ h\]

<p>The authors recommend using this approach as a starting point, as well as for more basic tasks, as it is easy to set-up and requires little tuning. It does have performance limitations, however, as it struggles with fast-changing action.</p>

<h4 id="time-series-diffusion-transformer">Time-series diffusion transformer</h4>

<p>The time-series diffusion transformer solves the issues with fast-changing action experienced by the CNN-based diffusion model.</p>

<p>In short, the noisy actions ${A_t}^{(k)}$ are passed in as input tokens to the transformer decoder blocks. The diffusion time step $k$ is also passed in as a token. The conditioning latents $z_t$ are embedded by an MLP, and passed in as conditioning features to the decoder. Each output token then corresponds to a component of $\hat{\varepsilon}_{\theta}$, which is used to de-noise ${A_t}^{(k)}$ into ${A_t}^{(k-1)}$.</p>

<p>The authors recommend using this approach if the CNN-based diffusion doesn’t lead to satisfactory results, or if the task at hand is more complex, as it is harder to set up and tune the transformer backbone.</p>

<h3 id="results--limitations">Results &amp; Limitations</h3>

<p>Diffusion Policy significantly outperforms previous SoTA policies across a variety of tasks. On average, Diffusion Policy increased performance by 46.9%.</p>

<h2 id="3d-diffusion-policy-dp3-4">3D Diffusion Policy (DP3) [4]</h2>

<h3 id="issues-with-previous-work">Issues with Previous Work</h3>

<p>The breakthrough work in Diffusion Policy [3] achieved near-human or human level performance on many tasks, proving that diffusion is a very promising direction. However, this model was incredibly data-hungry (approximately 100-200 demonstrations per task) and sensitive to camera viewpoint. Furthermore, these issues lead to the model committing safety violations, requiring interruption of the experiments by humans.</p>

<h3 id="solution">Solution</h3>

<p>The authors of 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations hypothesized that with a 3D aware diffusion policy one could achieve much better results. Rather than using multi-view 2D images fed through a ResNet backbone to condition the action diffusion process, they cleverly converted the visual input from a single camera into a point cloud representation followed by an efficient point encoder. They found that with far fewer demonstrations (as few as 10), DP3 is able to handle tasks, surpass previous baselines, and commit far fewer safety violations.</p>

<h4 id="the-perception-module-4">The Perception Module [4]</h4>

<p>First, 84 $\times$ 84 depth images from from the expert demonstrations are passed into the Perception Module. Using camera extrinsics and intrinsics, these are converted into point clouds. To improve generalization to different lighting and colors, they do not use color channels.</p>

<p>These point clouds often contain many redundant points, hence it can be very useful to downsample these. First, they crop all points not within a bounding box, eliminating useless points such as those on the ground or on the table. This is further downsampled to 1024 points using Farthest Point Sampling (FPS). FPS chooses an arbitrary start point, then iteratively chooses the furthest point from the set of those already selected and adds it, reducing the number of points while still being representative of the original image.</p>

<p>The final downsampled points are all fed into the DP3 Encoder. In the decoder, points are passed through a 3-layer MLP with LayerNorm, max-pooled, and finally projected into a compact vector, representing the entire 3D scene in a 64-dimensional vector. Surprisingly, this lightweight MLP encoder outperforms pre-trained models such as PointNet and Point Transformer (see the Ablation Studies sections).</p>

<h3 id="the-decision-module-4">The Decision Module [4]</h3>

<p>The Decision Module is a conditional denoising diffusion model conditioned on robot poses and the 3D feature vector obtained from the Perception Module. Specifically, let $q$ be robot pose,  $a^K$ be the final Gaussian noise, and $\epsilon_{\theta}$ be the denoising network. Then, the denoising network performs K iterations:
\(a^{k-1}=\alpha_k(a^k - \gamma_k\epsilon_\theta(a^k, k, v, q)) + \sigma_k\mathcal{N}(0, 1)\)</p>

<p>until it reaches the noise-free action $a^0$. Here $\gamma_k$, $\alpha_k$, and $\sigma_k$ all come from the noise scheduler and are functions of k.</p>

<h3 id="training-4">Training [4]</h3>

<p>The training data consisted of a small set of expert demonstrations, usually 10-40 per task, collected via human tele-operation or scripted oracles. The model was trained on 72 simulation tasks across 7 domains as well as 4 real robot tasks - see results section for more details.</p>

<p>The training process samples a random $a^0$ from the data and conducts a diffusion process to get $\epsilon^k$ the noise at iteration k. Then, the objective is simply 
\(\mathcal{L} = MSE(\epsilon^k, \epsilon_\theta(\bar{\alpha}_k a^0 + \bar{\beta}_k\epsilon^k, k, v, q))\)
where $\beta_k$ and $\bar{\alpha}_k$ are, again, functions of k from the noise scheduler. They use the same diffusion policy network architecture as the original 2D Diffusion Policy authors.</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/DP3Architecture.png" alt="DP3_Architecture" /></p>
<p><em>Fig 6. Overview of 3D Diffusion Policy. In the training phase, DP3 simultaneously trains its perception module and decision-making process end-to-end with expert demonstrations. During evaluation, DP3 determines actions based on visual observations from the environment.</em> [4].</p>

<h3 id="ablation-studies-4">Ablation Studies [4]</h3>

<p>The authors also performed several ablation studies to assess their design choices. They selected 6 tasks with 10 demonstrations each and ran ablations on different 3D representations, point cloud encoders, and other design choices. Here are some of their results:</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/3dRepAbl.png" alt="Ablation_3D_Representation" /></p>
<p><em>Fig 7. Ablation on the choice of 3D representations. Point clouds perform the best.</em> [4].</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/DesignChoiceAbl.png" alt="Ablation_Design" /></p>
<p>*Fig 8. Ablation on some design choices. * [4].</p>

<p>Surprisingly, the lightweight MLP Encoder greatly outperformed even pre-trained models such as PointNet and Point Transformer. Through careful analysis, the authors made modifications to PointNet and achieved competitive accuracy (72.3%) with their MLP decoder. Here are the original ablation results below:</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/EncoderAbl.png" alt="Ablation_Encoder" /></p>
<p>*Fig 9. Ablation on choice of point cloud encoder. Surprisingly, the lightweight MLP encoder outperforms larger pre-trained models. * [4].</p>

<h3 id="results-4">Results [4]</h3>

<p>DP3 was benchmarked on a wide range of tasks spanning rigid, articulated, and deformable object manipulation. It achieved over 24% relative improvement compared to Diffusion Policy, converged much faster, and required far fewer demonstrations. Here are the main results on benchmarks for DP3 and other models:</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/TableI.png" alt="ResultsTable" /></p>
<p>*Fig 10. Main simulation results. Averaged over 72 tasks, DP3 achieves 24.2% relative improvement compared to Diffusion Policy, with a smaller variance.  * [4].</p>

<p>DP3 also excelled in real robot tasks with greater generality. In particular, the real robot tasks were:</p>
<ol>
  <li><strong>Roll-up</strong>: The Allegro hand wraps the plasticine multiple times to make a roll-up.</li>
  <li><strong>Dumpling</strong>: The Allegro hand first wraps the plasticine and then pinchs it to make dumpling pleats.</li>
  <li><strong>Drill</strong>: The Allegro hand grasps the drill up and moves towards the green cube to touch the cube with the drill.</li>
  <li><strong>Pour</strong>: The gripper grasps the bowl, moves towards the plasticine, pours out the dried meat floss in the bowl, and places the bowl on the table.</li>
</ol>

<p>For the sake of time, there were 40 demonstrations per task. See the table below for results on real-robot tasks:</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/TableVIII.png" alt="RealRobotResults" /></p>
<p>*Fig 11. Results on real robot experiments with 10 tasks per trial. * [4].</p>

<p>To further assess generalization capabilities, they studied spatial generalization with the Pour task, instance generalization on drill, and more. Here are some of their exciting results:</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/TableIX.png" alt="SpatialGeneralization" /></p>
<p>*Fig 12. Spatial Generalization on Pour: The authors placed the bowl at 5 different positions that are unseen in the training data. Each position was evaluated with only one trial. * [4].</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/TableX.png" alt="ColorGeneralization" /></p>
<p>*Fig 13. Color/Appearance Generalization on Drill: The authors placed the bowl at 5 different positions that are unseen in the training data. Each position was evaluated with only one trial. * [4].</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/TableXI.png" alt="InstanceGeneralizatoin" /></p>
<p>*Fig 14. Instance generalization on Drill. The authors replaced the cube used in Drill with five objects in varied sizes from our daily life. Each instance is evaluated with one trial. * [4].</p>

<p>This generalizability of the model is perhaps the most crucial aspect, as robots will have to handle many different conditions in real-world applications. The instance generalization is an especially promising result, as it suggests possibilities for 0-shot learning on very similar tasks to training data.</p>

<p>Finally, the authors also present encouraging results about safety. They define a safety violation as “unpredictable behaviors in real-world experiments, which necessitates human termination to ensure robot safety.” Below are the safety results for DP3 and Diffusion Policy:</p>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team22/TableXIV.png" alt="SafetyViolations" /></p>
<p>*Fig 15. Average safety violation rates. DP3 rarely committed any safety violations, in contrast to the other models. * [4].</p>

<p>They found that DP3 rarely makes safety violations, and suspect this is due to its 3D awareness, though further work needs to be carried out in this direction.</p>

<h3 id="discussion-and-impact-of-dp3-4">Discussion and Impact of DP3 [4]</h3>

<p>Overall, DP3 represents a major step forward from 2D Diffusion Policy by showing that a simple, well-designed 3D representation—sparse point clouds plus a lightweight MLP encoder—is enough to dramatically improve data efficiency, generalization, and safety in visuomotor diffusion policies. However, despite their success, the authors believe that the optimal 3D representation is yet to be discovered. Furthermore, these tasks had relatively short horizons, and they acknowledge that much work needs to be done for tasks with extremely long horizons. Despite these limitations, DP3 has already lead to future work and been built upon in many other projects. For example, ManiCM [5] improves on inference speed, one of the most pressing practical issues. By distilling the multi-step denoising process into a consistency model that can generate actions in single step, ManiCM preserves the strengths of DP3’s 3D-aware policy while making it more suitable for real-time control. Taken together, DP3 and its extensions like ManiCM suggest that 3D diffusion policies are not just a one-off improvement over 2D image-based methods, but a robust foundation for future work in Imitation Learning for visuo-motor policy.</p>

<h2 id="references">References</h2>

<p>[1] Zhang, Qihang, et al. “Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining.” European Conference on Computer Vision, 2022.</p>

<p>[2] Hansen, Nicklas, et al. “On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline.” International Conference on Learning Representations, 2023.</p>

<p>[3] Chi, Cheng, Zhenjia Xu, Shuran Song, and others.<br />
“Diffusion Policy: Visuomotor Policy Learning via Action Diffusion.”<br />
<em>arXiv preprint arXiv:2303.04137</em>, 2023.</p>

<p>[4] Ze, Yanjie, Yiming Zhang, Zhiyang Dou, Xingyu Lin, and Shuran Song.<br />
“3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations.”<br />
<em>arXiv preprint arXiv:2403.03954</em>, 2024.</p>

<p>[5] Lu, Guanxing, Zifeng Gao, Tianxing Chen, Wenxun Dai, Ziwei Wang, Wenbo Ding, and Yansong Tang.
“ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation.”
arXiv preprint arXiv:2406.01586, 2024.</p>]]></content><author><name>UCLAdeepvision</name></author><summary type="html"><![CDATA[Visuomotor Policy Learning studies how an agent can map high-dimensional visual observations (e.g., camera images) to motor commands in order to solve sequential decision-making tasks. In this project, we focus on settings motivated by autonomous driving and robotic manipulation, and survey modern learning-based approaches—primarily imitation learning (IL) and reinforcement learning (RL)—with an emphasis on methods that improve sample efficiency through policy/representation pretraining.]]></summary></entry><entry><title type="html">From Labeling to Prompting: The Paradigm Shift in Image Segmentation</title><link href="/CS163-Projects-2025Fall/2025/12/13/team23-mask-sam.html" rel="alternate" type="text/html" title="From Labeling to Prompting: The Paradigm Shift in Image Segmentation" /><published>2025-12-13T00:00:00+00:00</published><updated>2025-12-13T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/13/team23-mask-sam</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/13/team23-mask-sam.html"><![CDATA[<blockquote>
  <p>The evolution from Mask R-CNN to SAM represents a paradigm shift in computer vision segmentation, moving from supervised specialists constrained by fixed vocabularies to promptable generalists that operate class-agnostically. We examine the technical innovations that distinguish these approaches, including SAM’s decoupling of spatial localization from semantic classification and its ambiguity-aware prediction mechanism, alongside future directions in image segmentation.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#mask-r-cnn" id="markdown-toc-mask-r-cnn">Mask R-CNN</a>    <ul>
      <li><a href="#architecture" id="markdown-toc-architecture">Architecture:</a></li>
    </ul>
  </li>
  <li><a href="#segment-anything-sam" id="markdown-toc-segment-anything-sam">Segment Anything (SAM)</a>    <ul>
      <li><a href="#architecture-1" id="markdown-toc-architecture-1">Architecture:</a></li>
      <li><a href="#results" id="markdown-toc-results">Results:</a></li>
    </ul>
  </li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a>    <ul>
      <li><a href="#training-paradigms" id="markdown-toc-training-paradigms">Training Paradigms</a></li>
      <li><a href="#the-semantic-gap" id="markdown-toc-the-semantic-gap">The Semantic Gap</a></li>
      <li><a href="#the-future-towards-unified-perception" id="markdown-toc-the-future-towards-unified-perception">The Future: Towards Unified Perception</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>For the past decade, the “holy grail” of computer vision was fully automated perception: teaching a machine to look at an image and assign a semantic label to every pixel. The 2024 survey “Image Segmentation in Foundation Model Era: A Survey” by Zhou et al defines this era as “Generic Image Segmentation” (GIS), which was dominated by supervised specialists. One of the pinnacles of this approach was Mask R-CNN, a framework that efficiently detected objects and generated high-quality masks in parallel. While powerful, these models were fundamentally limited by their training data. They were “closed-vocabulary” systems that could only see what they were explicitly taught to label.</p>

<p>To formalize this limitation, the 2024 survey introduces a unified formulation for the segmentation task:
\(f: \mathcal{X} \mapsto \mathcal{Y} \quad \text{where} \quad \mathcal{X} = \mathcal{I} \times \mathcal{P}\)
Here, I represents the image and P represents the user prompt. In the specialist era of Mask R-CNN, the prompt set was empty, forcing the model to rely entirely on learned semantics to determine the output. [1]</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/fig1imageseg.png" alt="Image 1" /> 
<em>Figure 1 from “Image Segmentation in Foundation Model Era: A Survey”.</em></p>

<p>We are now witnessing a “new epoch” in computer vision driven by the rise of Foundation Models. The paradigm is shifting from passive labeling to active prompting. Leading this is the Segment Anything Model (SAM), which introduced the “promptable segmentation task”. Unlike its predecessors, SAM acts as a “segmentation generalist” that may not know what an object is but knows exactly where it is given a simple click or box prompt.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/fig1sama.png" alt="Image 2" /> 
<em>Figure 1 from “Segment Anything”.</em></p>

<p>Our discussion of the two models explores this transition by contrasting the supervised precision of Mask R-CNN with the zero-shot flexibility of SAM, discussing how the move from curated datasets to massive “data engines” is redefining what it means for a machine to “see”.</p>

<h2 id="mask-r-cnn">Mask R-CNN</h2>

<p>Mask R-CNN represents one of the culmination of the supervised segmentation era, exemplifying both the strengths and limitations of closed-vocabulary, instance-level segmentation systems. Designed to assign fixed semantic labels, such as “person” or “car” to every pixel belonging to an object instance, it operates with precision on known categories. Unlike segmentation-first approaches that group pixels before classification, Mask R-CNN adopts an instance-first strategy by detecting object bounding boxes first, then segmenting the pixels within those regions. This design allows segmentation to run parallel to detection, providing a clean conceptual separation between the two tasks. However, achieving pixel-accurate masks required precise spatial alignment, something earlier detection pipelines struggled to provide.[3]</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/fig1mask.png" alt="Image 3" />
<em>Figure 1 from “Mask R-CNN”.</em></p>

<h3 id="architecture">Architecture:</h3>

<p>The breakthrough that elevated Mask R-CNN to exception was an innovation known as RoIAlign which solved a critical spatial alignment problem. Prior methods relied on RoIPool which used coarse quantization when mapping regions of interest to feature maps. This introduced misalignments which were okay when it came to bounding box detection but catastrophic for pixel-level segmentation accuracy. RoIAlign came and eliminated quantization, using bilinear interpolation to preserve spatial coordinates. This led to dramatic improvements in mask accuracy, going from 10% to 50% across benchmarks, showing how spatial precision was the primary bottleneck in achieving quality segmentation. [3]</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/roipooltable.png" alt="Image 4" /><br />
<em>Table 6. RoIAlign vs. RoIPool for keypoint detection on minival. The backbone is ResNet-50-FPN.</em></p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/roifig.png" alt="Image 5" /> 
<em>Figure 3, “Research on Intelligent Detection and Segmentation of RockJoints Based on Deep Learning”.</em></p>

<p>Mask R-CNN’s efficiency stems from its decoupling of mask prediction from class prediction. The architecture predicts binary masks independently for every class without inter-class competition. The classification branch determines object identity while the mask branch focuses exclusively on spatial extent within detected regions. This separation of concerns enabled Mask R-CNN to surpass all competing single-models entries on the COCO object detection challenges, establishing it as the dominant approach for instance segmentation tasks. The mask loss, is the average binary cross-entropy (BCE) loss over all pixels in the RoI, calculated only for the ground-truth class:</p>

\[L_{\text{mask}} =
\frac{1}{M^2}
\sum_{i=1}^{M^2}
\text{BCE}\bigl(p_i^{k}, y_i^{k}\bigr)\]

<p>Despite its advances in computer vision, Mask R-CNN operates within a crucial limitation because it functions as a closed-vocabulary model, capable of segmenting only those object categories encountered during training. Extending the model to recognize new categories requires substantial data collection, labor-intensive pixel-level annotation, and complete model retraining. This rigidity, inherent to the supervised learning paradigm, ultimately necessitated the evolution toward promptable architectures like SAM, which transcend fixed category constraints through foundation model approaches.</p>

<h2 id="segment-anything-sam">Segment Anything (SAM)</h2>

<p>In the GIS era, models like Mask R-CNN were designed as “all-in-one” systems. They were trained to simultaneously localize an object and assign it a specific semantic label from a fixed vocabulary (e.g., class_id: 1 for “person”). The architecture explicitly coupled these tasks: the network’s classification branch and mask branch ran in parallel, meaning the model could only segment what it could also recognize. The Segment Anything Model (SAM) flips this paradigm. [1]</p>

<p>Created by Meta AI, SAM fundamentally redefines the segmentation task so that instead of predicting a fixed class label for every pixel, the goal is to return a valid segmentation mask for any prompt. It is trained to be class-agnostic. It does not output a semantic label; instead, it outputs a “valid mask” and a confidence score reflecting the object’s “thingness” rather than its category. SAM essentially understands structure (boundaries, occlusion, and connectivity) without necessarily understanding semantics. It knows that a pixel belongs to a distinct entity, but it relies on the prompter to define the context. This shift transforms the model from a static labeler into an interactive “generalist” that decouples the concept of “where an object is” (segmentation) from “what an object is” (semantics).</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/segment-anything-cut-out.gif" alt="Image 7" />
<em>GIF of SAM UI, taken from: <a href="https://learnopencv.com/segment-anything/">https://learnopencv.com/segment-anything/</a>.</em></p>

<p>In the supervised era, models like Mask R-CNN were limited by the high cost of manual pixel-level annotation. To overcome this, SAM utilized a “Data Engine” as follows:</p>

<ul>
  <li>Assisted-Manual: Annotators used a SAM-powered tool to label masks, working 6.5x faster than standard COCO annotation.</li>
  <li>Semi-Automatic: The model automatically labeled confident objects (like “stuff” categories), allowing annotators to focus on difficult, less prominent objects.</li>
  <li>Fully Automatic: The model lastly ran on 11 million images to generate the SA-1B dataset, containing 1.1 billion masks (400x larger than any existing segmentation dataset).</li>
</ul>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/fig1csam.png" alt="Image 8" /> 
<em>Figure 1 from “Segment Anything”.</em></p>

<p>This massive scale allowed SAM to learn a generalized notion of “thingness” that transfers zero-shot to underwater scenes, microscopy, and ego-centric views without specific retraining. [2]</p>

<h3 id="architecture-1">Architecture:</h3>

<p>SAM achieves real-time interactivity through a distinct three-part architecture that separates heavy computation from fluid interaction.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/fig4sam.png" alt="Image 9" /> 
<em>Figure 4 from “Segment Anything”.</em></p>

<p>The backbone of SAM is a Vision Transformer (ViT) pre-trained using Masked Autoencoders (MAE). Unlike standard supervised pre-training, MAE masks a large portion of the input image patches (e.g., 75%) and forces the model to reconstruct the missing pixels.This self-supervised approach allows the model to learn robust, scalable visual representations without human labels. In SAM, this encoder runs once per image, outputting a 64 x 64 image embedding. While computationally expensive, this cost is “amortized” over the interaction because once the embedding is calculated, the model can respond to hundreds of prompts in milliseconds.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/maefig1.png" alt="Image 10" /> 
<img src="/CS163-Projects-2025Fall/assets/images/team23/maefig3.png" alt="Image 11" /> 
<em>Figures 1 and 3 from “Masked Autoencoders Are Scalable Vision Learners”.</em></p>

<p>The image encoder produces a high-dimensional embedding that preserves spatial structure. Unlike traditional CNNs that progressively downsample, the ViT maintains a 64 x 64 grid where each location encodes rich contextual information about that region. This design is essential for allowing the downstream decoder to perform pixel-precise localization even though the encoder operates at 16x downsampled resolution.</p>

<p>To enable the “promptable” paradigm, SAM represents sparse inputs (points, boxes, text) as positional encodings:</p>

<ul>
  <li>Points &amp; Boxes: Represented by positional encodings summed with learned embeddings for each prompt type.</li>
  <li>Text: Processed via an off-the-shelf CLIP text encoder, bridging the gap between language and pixels.</li>
</ul>

<p>Dense prompts (masks) are handled differently by being embedded using convolutions and summed element-wise with the image embedding. This enables SAM to accept a previous mask prediction as input, allowing iterative refinement, a key capability for interactive use cases where users progressively correct the model’s output.</p>

<p>The decoder is a modification of a Transformer decoder block that efficiently maps the image embedding and prompt embeddings to an output mask. It utilizes a mechanism of cross-attention to update the image embedding with prompt information:</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V\]

<p>In this context, the prompt tokens act as queries (Q) to attend to the image embedding (K, V), ensuring the model focuses only on the regions relevant to the user’s input.</p>

<p>The decoder performs bidirectional cross-attention where not only do prompt tokens query the image embedding, but the image embedding also queries the prompt tokens. This two-way information flow allows the model to both focus on relevant regions (prompt-to-image) and update its understanding based on spatial context (image-to-prompt). After two decoder blocks, the model upsamples the updated image embedding and applies a dynamic linear classifier (implemented as an MLP) to predict per-pixel mask probabilities.</p>

<p>A critical innovation in SAM is its ambiguity awareness. In the Mask R-CNN era, a single input had to correspond to a single ground truth. However, a point on a person’s shirt is ambiguous: does the user want the shirt or the person?</p>

<p>To solve this, SAM predicts three valid masks for a single prompt (corresponding to the whole, part, and sub-part) along with confidence scores (IoU), allowing the user or downstream system to resolve the ambiguity. This design choice acknowledges that segmentation is inherently ill-posed (i.e., a single input can have multiple valid outputs), a nuance often ignored by previous fully supervised specialists.</p>

<p>SAM is trained with a multi-task loss combining focal loss and dice loss in a 20:1 ratio. The focal loss addresses class imbalance by down-weighting easy examples, while the dice loss directly optimizes for mask overlap (IoU). Critically, when SAM predicts multiple masks, only the mask with the lowest loss receives gradient updates (a technique that prevents the model from averaging over ambiguous ground truths). SAM also predicts an IoU score for each mask, trained via mean-squared-error loss, which enables automatic ranking of predictions without human intervention. The total loss is formulated as:</p>

<p>\(L_{\text{total}} = \bigl(20 \cdot L_{\text{focal}} + L_{\text{dice}}\bigr) + L_{\text{MSE}}\)
\(\underbrace{20 \cdot L_{\text{focal}} + L_{\text{dice}}}_{\text{Mask Prediction}}
\quad + \quad
\underbrace{L_{\text{MSE}}}_{\text{IoU Ranking}}\)</p>

<p>During training, SAM also simulates interactive annotation by sampling prompts in 11 rounds per mask, including one initial prompt (point or box), 8 iteratively sampled points from error regions, and 2 refinement iterations with no new points. This forces the model to learn both initial prediction and self-correction, making it robust to imperfect prompts in deployment. [2]</p>

<h3 id="results">Results:</h3>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/samtable.png" alt="Image 12" /> 
<em>Figures 9 from “Segment Anything”.</em></p>

<p>Upon its release, SAM demonstrated unprecedented zero-shot generalization across 23 diverse segmentation datasets spanning underwater imagery, microscopy, X-ray scans, and ego-centric video. On single-point prompts, SAM achieved competitive IoU with specialist models like RITM, but critically, human annotators rated SAM’s masks 1-2 points higher (on a 1-10 scale) than the strongest baselines of the time. This gap revealed a key limitation of IoU metrics because SAM produced perceptually better masks that effectively segmented valid objects, even when they differed from the specific ground truth, resulting in artificially deflated scores. [2]</p>

<h2 id="discussion">Discussion</h2>

<h3 id="training-paradigms">Training Paradigms</h3>

<p>Mask R-CNN relies entirely on manually curated datasets like COCO and Cityscapes, which require pixel-level annotation. The model’s understanding was fundamentally bottlenecked by human time and effort. SAM flipped this paradigm by making the segmentation model itself the primary data generator. Instead of humans doing the work, the model proposes masks while humans simply validate and correct mistakes. This created a virtuous cycle that produced 1.1 billion masks, a scale utterly impossible with manual annotation alone. In the foundation model era, how data is collected may be as important as the model architecture itself.</p>

<h3 id="the-semantic-gap">The Semantic Gap</h3>

<p>The most important conceptual difference between Mask R-CNN and SAM lies in how they handle semantics. Mask R-CNN tightly couples segmentation and classification, where every predicted mask corresponds to a predefined category. This makes the model effective within its training distribution but also enforces a closed vocabulary.</p>

<p>SAM deliberately breaks this coupling. It focuses on identifying coherent regions in an image without assigning semantic labels. In doing so, segmentation becomes a modular capability rather than a final output. SAM can be combined with other components (object detectors, language models, or task-specific logic) that provide semantic interpretation separately. This separation allows segmentation to function as general visual infrastructure that can support many downstream tasks without retraining.</p>

<h3 id="the-future-towards-unified-perception">The Future: Towards Unified Perception</h3>

<p>SAM represents a significant milestone, but recent developments suggest the field is moving beyond promptable segmentation. Models like DINO and Stable Diffusion now perform segmentation as an emergent capability, despite never being explicitly trained for it. Optimized for self-supervised learning and image generation respectively, these models spontaneously learn to group pixels into coherent objects. This suggests that segmentation may arise naturally from learning good visual representations, rather than requiring dedicated training.</p>

<p><img src="/CS163-Projects-2025Fall/assets/images/team23/fig3survey.png" alt="Image 13" /> 
<em>Figure 3 from “Image Segmentation in Foundation Model Era: A Survey”.</em></p>

<p>These observations point toward unified perception systems that blur traditional task boundaries. Instead of separate modules for detection, segmentation, and classification, future architectures may provide continuous visual understanding from which any capability can be extracted as needed. The integration of large language models with vision systems exemplifies this trend, enabling reasoning about images at multiple levels simultaneously (e.g. like from pixel groupings to semantic relationships to natural language descriptions).</p>

<p>The current landscape is characterized by coexisting paradigms rather than outright replacement. Specialist models remain optimal for well-defined domains with stable data distributions and strict performance requirements. Foundation models provide flexible infrastructure for open-world scenarios where generalization matters most. Emergent capabilities in self-supervised systems hint at a future where task boundaries dissolve entirely. Effective computer vision practice now requires combining these complementary approaches correctly for the right tasks.</p>

<h2 id="references">References</h2>

<p>[1] Zhou, T., Xia, W., Zhang, F., Chang, B., Wang, W., Yuan, Y., Konukoglu, E., &amp; Cremers, D. (2024). <em>Image Segmentation in Foundation Model Era: A Survey</em>. arXiv preprint arXiv:2408.12957. &lt;<a href="https://arxiv.org/abs/2408.12957">https://arxiv.org/abs/2408.12957</a>&gt;</p>

<p>[2] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., &amp; Girshick, R. (2023). <em>Segment Anything</em>. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 4015-4026. &lt;<a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a>&gt;</p>

<p>[3] He, K., Gkioxari, G., Dollár, P., &amp; Girshick, R. (2017). <em>Mask R-CNN</em>. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2961-2969. &lt;<a href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a>&gt;</p>

<p>[4] Peng, L., Wang, H., Zhou, C., Hu, F., Tian, X., &amp; Hongtai, Z. (2024). <em>Research on Intelligent Detection and Segmentation of Rock Joints Based on Deep Learning</em>. Complexity, 2024, Article ID 8810092. &lt;<a href="https://onlinelibrary.wiley.com/doi/10.1155/2024/8810092">https://onlinelibrary.wiley.com/doi/10.1155/2024/8810092</a>&gt;</p>

<p>[5] He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2022). <em>Masked Autoencoders Are Scalable Vision Learners</em>. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16000-16009. &lt;<a href="https://arxiv.org/abs/2111.06377">https://arxiv.org/abs/2111.06377</a>&gt;</p>]]></content><author><name>Abdallah Fares, Dean Ali, Olana Abraham</name></author><summary type="html"><![CDATA[The evolution from Mask R-CNN to SAM represents a paradigm shift in computer vision segmentation, moving from supervised specialists constrained by fixed vocabularies to promptable generalists that operate class-agnostically. We examine the technical innovations that distinguish these approaches, including SAM’s decoupling of spatial localization from semantic classification and its ambiguity-aware prediction mechanism, alongside future directions in image segmentation.]]></summary></entry><entry><title type="html">XAI in Facial Recognition</title><link href="/CS163-Projects-2025Fall/2025/12/13/team39-XAI-Face.html" rel="alternate" type="text/html" title="XAI in Facial Recognition" /><published>2025-12-13T00:00:00+00:00</published><updated>2025-12-13T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/13/team39-XAI-Face</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/13/team39-XAI-Face.html"><![CDATA[<blockquote>
  <p>Facial Recognition (FR) systems are being increasingly used in high stake environments, but their decision making processes remain a mystery, raising concerns regarding trust, bias, and robustness. Traditional methods such as occlusion sensitivity or saliency maps (e.g., Grad-CAM), often fail to capture the causal mechanisms driving verification decisions or diagnosis reliance on shortcuts. This report analyzes three modern paradigms that shift Explainable AI (XAI) from passive visualization to active, feature level interrogation. We examined FastDiME [5] which utilizes generative diffusion models to create counterfactuals for detecting shortcut learning, Feature Guided Gradient Backpropagation (FGGB) [3], which mitigates vanishing gradients to produce similarity and dissimilarity maps, and Frequency Domain Explainability [2], which introduces Frequency Heat Plots (FHPs) to diagnose biases in CNNs. By synthesizing these approaches, we examine how modern XAI tools can assess model reliance on noise versus structural identity, with the goal of offering a pathway toward more robust and transparent biometric systems.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#1-introduction" id="markdown-toc-1-introduction">1. Introduction</a></li>
  <li><a href="#2-fast-diffusion-based-counterfactuals-for-shortcut-removal-and-generation" id="markdown-toc-2-fast-diffusion-based-counterfactuals-for-shortcut-removal-and-generation">2. Fast Diffusion-Based Counterfactuals for Shortcut Removal and Generation</a>    <ul>
      <li><a href="#21-introduction" id="markdown-toc-21-introduction">2.1 Introduction</a></li>
      <li><a href="#22-shortcut-learning" id="markdown-toc-22-shortcut-learning">2.2 Shortcut Learning</a></li>
      <li><a href="#23-fastdime-guided-diffusion-for-counterfactual-faces" id="markdown-toc-23-fastdime-guided-diffusion-for-counterfactual-faces">2.3 FastDiME: Guided Diffusion for Counterfactual Faces</a></li>
      <li><a href="#24-self-optimized-masking-for-localized-image-edits" id="markdown-toc-24-self-optimized-masking-for-localized-image-edits">2.4 Self-Optimized Masking for Localized Image Edits</a></li>
      <li><a href="#25-quantifying-shortcut-dependence" id="markdown-toc-25-quantifying-shortcut-dependence">2.5 Quantifying Shortcut Dependence</a></li>
    </ul>
  </li>
  <li><a href="#3-explainable-face-verification-via-feature-guided-gradient-backpropagation-fggb" id="markdown-toc-3-explainable-face-verification-via-feature-guided-gradient-backpropagation-fggb">3. Explainable Face Verification via Feature-Guided Gradient Backpropagation (FGGB)</a>    <ul>
      <li><a href="#31-introduction" id="markdown-toc-31-introduction">3.1 Introduction</a></li>
      <li><a href="#32-methodology" id="markdown-toc-32-methodology">3.2 Methodology</a></li>
      <li><a href="#33-quantitative-evaluation" id="markdown-toc-33-quantitative-evaluation">3.3 Quantitative Evaluation</a></li>
    </ul>
  </li>
  <li><a href="#4-frequency-domain-based-explainability" id="markdown-toc-4-frequency-domain-based-explainability">4. Frequency-Domain Based Explainability</a>    <ul>
      <li><a href="#41-introduction" id="markdown-toc-41-introduction">4.1 Introduction</a></li>
      <li><a href="#42-methodology-frequency-masking-and-influence-scoring" id="markdown-toc-42-methodology-frequency-masking-and-influence-scoring">4.2 Methodology: Frequency Masking and Influence Scoring</a></li>
      <li><a href="#43-key-insights-diagnosing-structural-bias" id="markdown-toc-43-key-insights-diagnosing-structural-bias">4.3 Key Insights: Diagnosing Structural Bias</a></li>
    </ul>
  </li>
  <li><a href="#5-conclusion" id="markdown-toc-5-conclusion">5. Conclusion</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<p>As Deep Learning models achieve state-of-the-art performance in biometric security, the “black box” nature of Facial Recognition (FR) and Face Verification (FV) systems present a growing liability. Unlike general object classification, FR systems operate in high-risk sociological contexts where errors can lead to wrongful arrests, discriminatory access denial, and security breaches. The challenge is no longer accuracy but trustworthiness to ensure that a model can distinguish identities on robust biometric features rather than spurious “shortcuts” like background textures, lighting artifacts, or demographic biases.</p>

<p>Historically, explainability in computer vision (CV) has relied on saliency methods, such as Grad-CAM [4]. While effective for localizing where a model looks, these methods are fundamentally correlational rather than causal. They often fail to explain why a decision was reached, like whether a match was driven by genuine identity features or coincidental pixel statistics. Furthermore, standard backpropagation methods suffer from noisy or vanishing gradients when applied to deep feature embeddings used in modern FV, rendering conventional heatmaps difficult to interpret for fine-grained verification tasks.</p>

<p>Currently, the landscape of Explainable AI (XAI) is expanding rapidly to address these issues. Researchers are exploring diverse methodologies, ranging from logic-based formal verifications [1] that uses prime implicates to identify minimum sufficient reasons for a classification decision to interactive user interfaces. While these approaches offer promising theoretical guarantees, the need for CV practitioners is often direct feature level diagnostics that can visually and causally verify model behavior.</p>

<p>To address these limitations, this report looks at three methodologies. FastDiME [5] is an approach that leverages diffusion probabilistic models to create counterfactuals. By removing specific attributes and observing a shift, FastDiME offers a causal mechanism for detecting shortcut learning. Feature-Guided Gradient Backpropagation (FGGB) [3], is a different method that shifts focus from final output score to the deep feature level. By normalizing gradients channel-wise, FGGB separates decision-making process into distinct similarity and dissimilarity maps that provide a clearer view of what features argue for or against a match. Frequency Domain Explainability [2] which challenges the spatial bias of human interpretation by analyzing how Convolutional Neural Networks (CNNs) rely on specific frequency bands. This approach reveals that models may rely on high-frequency noise or specific texture patterns offering a powerful tool for quantifying algorithmic bias across different demographics. Together, these methods represent a shift from simple visualization to rigorous feature aware model diagnostics.</p>

<h2 id="2-fast-diffusion-based-counterfactuals-for-shortcut-removal-and-generation">2. Fast Diffusion-Based Counterfactuals for Shortcut Removal and Generation</h2>

<h3 id="21-introduction">2.1 Introduction</h3>
<p>Saliency-based models are among the most popular methods for explainability in computer vision applications. Grad-CAM, for instance, highlights the regions that have the largest effects on model predictions, but doesn’t allow an assessment of how a prediction would change due to the addition or removal of a feature. Beyond saliency by occlusion, current methods cannot help determine whether altering a feature will affect model performance.</p>

<p>This limitation is especially important in facial recognition tasks, where cues unrelated to facial structure, such as glasses or a mask, could affect model predictions. This methodology proposes a generative approach to explaining model behavior. Instead of relying on visual maps, counterfactual explanations apply a diffusion model to edit the input image to measure the resulting change in prediction. Fast Diffusion-Based Counterfactuals for Shortcut Removal and Generation (FastDiME) generates counterfactuals and uses them to identify “shortcuts,” or features correlated with the label that are not causally relevant.</p>

<h3 id="22-shortcut-learning">2.2 Shortcut Learning</h3>
<p>Shortcut learning occurs when a vision model uses spurious features that happen to correlate with the label instead of information relevant to the task to identify a class. The paper focuses on medical imaging shortcuts, like pacemakers to correlate with heart disease labels. For facial recognition use cases, shortcuts can include background patterns, accessories, or artifacts from image processing. Objects or patterns unrelated to facial recognition can become correlated with identity.</p>

<p>Shortcuts affect the model’s results when applied to real data where the shortcut is not present. For instance, a facial recognition model could learn to identify a person’s glasses instead of biometric or facial features. Saliency maps can suggest aspects of the face that the model focuses on, but cannot counterfactually confirm whether the model would still recognize the individual without the shortcut. Counterfactual explanations address this by enabling comparisons between model results with and without the shortcut feature.</p>

<p>We can mathematically express shortcut dependence as the sensitivity of the model prediction \(f(x)\)to a shortcut feature\(s\):</p>

\[\Delta_{s}=E_{x}[|f(x)-f(x^{(s\rightarrow\overline{s})})|]\]

<p>where \(x^{(s\rightarrow\overline{s})}\)denotes a counterfactual image in which the shortcut feature\(s\) has been removed or altered.</p>

<h3 id="23-fastdime-guided-diffusion-for-counterfactual-faces">2.3 FastDiME: Guided Diffusion for Counterfactual Faces</h3>
<p>The FastDiME framework is built atop diffusion probabilistic models (DDPMs). In a typical DDPM, the model gradually adds Gaussian noise to an image in a forward process, and then learns a reverse process that progressively denoises the image. The forward diffusion process is defined as:</p>

\[q(x_{t}|x_{t-1})=N(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}I)\]

<p>Equivalently, the noisy image at time step \(t\) can be written as:</p>

\[x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon, \quad \epsilon\sim \mathcal{N}(0,I)\]

<p>With:</p>

\[\bar{\alpha}_{t}=\prod_{i=1}^{t}(1-\beta_{i})\]

<p>FastDiME changes the reverse process to make a counterfactual image instead of an identical copy. The high-level process is as follows: After noising the input image to an intermediate step, the reverse process is performed with a loss that encourages the removal of a specific feature. The guided mean of reverse diffusion is given by:</p>

\[\mu_{g}(x_{t})=\mu(x_{t})-\Sigma(x_{t})\nabla_{x}\mathcal{L}_{CF}(x_{t})\]

<p>The counterfactual is kept as close to the original as possible using a combined loss:</p>

\[\mathcal{L}_{CF}=\lambda_{c}\mathcal{L}_{cls}+\lambda_{1}||x-x_{0}||_{1}+\lambda_{p}\mathcal{L}_{perc}\]

<p>FastDiME has a time complexity of \(O(T)\), beating the original DiME with a time complexity of \(O(T^{2})\) by avoiding extraneous inner diffusion calculations that DiME performs during reconstruction.</p>

<p style="width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team-39/paper1.png" alt="CelebA counterfactual explanations" /></p>

<h3 id="24-self-optimized-masking-for-localized-image-edits">2.4 Self-Optimized Masking for Localized Image Edits</h3>
<p>Using FastDiME to generate images creates the risk that the algorithm alters the facial structure or biometric information entirely as opposed to only the shortcut. Many shortcut features are spatially localized, and therefore, the method introduces a self-optimized mask to restrict changes.</p>

<p>The high-level process is as follows:
At each diffusion step, a binary mask is computed by comparing the denoised estimate to the original image:</p>

\[M_{t}=\delta(\overline{x}_{t},x_{0})\]

<p>The mask is used to constrain updates so only the masked regions may change:</p>

\[x_{t}^{\prime}=x_{t}\odot M_{t}+x_{t}^{orig}\odot(1-M_{t})\]

<p>The mask is updated throughout the diffusion process to encourage localized changes but remove the shortcut feature.</p>

<h3 id="25-quantifying-shortcut-dependence">2.5 Quantifying Shortcut Dependence</h3>
<p>The paper proposes a quantitative method for classifying the extent to which a classifier is correlated with a shortcut feature. The model is trained with datasets with varied degrees of shortcut-label correlations. Then, counterfactuals are generated based on a balanced test set. The impact is then measured by the change in model predictions between the original and counterfactual images, often using the mean absolute difference (MAD):</p>

\[MAD=E_{x}[|f(x)-f(x^{c})|]\]

<p>where \(x^{c}\) represents the removed counterfactual image.</p>

<p>Overall, FastDiME presents a generative and causal approach to explainability for computer vision classifiers, serving as an alternative to saliency-based methods and enabling direct analysis of the features affecting predictions, shortcut dependence, and the robustness of a model.</p>

<h2 id="3-explainable-face-verification-via-feature-guided-gradient-backpropagation-fggb">3. Explainable Face Verification via Feature-Guided Gradient Backpropagation (FGGB)</h2>

<h3 id="31-introduction">3.1 Introduction</h3>
<p>As Deep Learning models achieve state-of-the-art performance in Face Verification (FV), their lack of transparency remains a critical barrier to deployment. This report examines the landscape of Explainable Face Verification (XFV), specifically addressing the limitations of traditional gradient-based explanation methods. We highlight a novel approach, Feature-Guided Gradient Backpropagation (FGGB), which overcomes the “noisy gradient” problem by shifting the backpropagation focus from the final score to the deep feature level.</p>

<p>This method generates sharper visualizations designed to offer interpretations from the user’s perspective. Specifically, the algorithm aims to explain why the system believes a pair of facial images is matching (“Accept”) or non-matching (“Reject”) by generating precise similarity maps for acceptance decisions and dissimilarity maps for rejection decisions.</p>

<h3 id="32-methodology">3.2 Methodology</h3>
<p>The FGGB method addresses the issue where derivatives of the output score fluctuate sharply or vanish. Instead of backpropagating from the final similarity score, FGGB operates at the deep feature level in a channel-wise manner. The process is divided into two phases:</p>

<p><strong>Gradient Backpropagation &amp; Normalization</strong>
First, the system extracts the deep face representations (embeddings) for the input images, denoted as \(F_{A}\) and \(F_{B}\) each with dimension \(N\).</p>

<ul>
  <li>
    <p><strong>Gradient Extraction:</strong> The system backpropagates gradients from each channel of the feature vector \(F_{A}\). For the \(k\)-th dimension of the feature, the gradient map \(G_{A}^{k}\) is calculated as the derivative with respect to the input image \(I_{A}\):</p>

\[G_{A}^{k}=\frac{\partial F_{A}^{k}}{\partial I_{A}}\]
  </li>
  <li>
    <p><strong>Normalization:</strong> To mitigate local variations (such as vanishing gradients), each gradient map is normalized using the Frobenius norm to produce \(\overline{G}_{A}^{k}\):</p>

\[\overline{G}_{A}^{k}=\frac{G_{A}^{k}}{\|G_{A}^{k}\|}\]
  </li>
</ul>

<p><strong>Saliency Map Generation</strong>
In the second phase, the system aggregates these \(N\) normalized gradient maps into a final saliency map. The aggregation is weighted by the actual contribution of each feature channel to the verification decision.</p>

<ul>
  <li>
    <p><strong>Weight Calculation:</strong> A weight vector is defined as the element-wise cosine similarity between the two feature vectors \(F_{A}\) and \(F_{B}\):</p>

\[weight=\frac{F_{A}\odot F_{B}}{||F_{A}||||F_{B}||}\]
  </li>
  <li>
    <p><strong>Weighted Sum:</strong> The final saliency map \(S\) is computed by summing the normalized gradient maps. The weights are adjusted by subtracting the decision threshold to distinguish between positive and negative contributions:</p>

\[S_{A}=\sum_{k=1}^{N}\overline{G}_{A}^{k}\cdot(weight_{k}-\frac{threshold}{N})\]
  </li>
  <li>
    <p><strong>Map Decomposition:</strong> Finally, the map is split into two distinct visualizations:</p>
    <ul>
      <li><strong>Similarity Map (\(S_{A}^{+}\)):</strong> Contains positive values, highlighting features that contribute to a match.</li>
      <li><strong>Dissimilarity Map (\(S_{A}^{-}\)):</strong> Contains negative values, highlighting features that cause a conflict or rejection.</li>
    </ul>

\[S_{A}^{+}=S_{A}[S_{A}\ge0] , \quad S_{A}^{-}=S_{A}[S_{A}&lt;0]\]
  </li>
</ul>

<p style="width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team-39/paper2.png" alt="FGGB Verification and Explanation Flow" /></p>

<h3 id="33-quantitative-evaluation">3.3 Quantitative Evaluation</h3>
<p>Validation of the FGGB method was done using the Deletion &amp; Insertion metrics on three datasets (LFW, CPLFW, and CALFW). The “Deletion” metric measures the drop in verification accuracy when salient pixels are removed (lower score is better), while “Insertion” measures accuracy gain when pixels are added (higher is better).</p>

<ul>
  <li><strong>Performance:</strong> FGGB demonstrated superior performance, particularly in generating “Dissimilarity Maps”.</li>
  <li>In the LFW dataset for Similarity Maps, FGGB achieved a Deletion score of <strong>24.18%</strong>, significantly outperforming the popular LIME method (35.82%) and performing comparably to perturbation methods like CorrRISE (24.51%) but with much greater efficiency.</li>
  <li>For Dissimilarity Maps (explaining rejections), FGGB achieved a Deletion score of <strong>44.03%</strong> on LFW, outperforming the competing gradient-based method xSSAB, which scored 49.72%.</li>
</ul>

<p>The method’s robustness was validated across different architectures, including ArcFace (99.70% accuracy), AdaFace (99.27%), and MobileFaceNet (98.87%), showing consistent explainability scores across all three.</p>

<p>FGGB bypasses the limitations of score-level backpropagation. By calculating gradients at the feature level and weighting them by their contribution to the cosine similarity, the method successfully generates distinct maps for both matching and non-matching faces. This approach allows for precise, pixel-level explanations of verification decisions without requiring model retraining or expensive iterative perturbations.</p>

<h2 id="4-frequency-domain-based-explainability">4. Frequency-Domain Based Explainability</h2>

<h3 id="41-introduction">4.1 Introduction</h3>
<p>While methods like FGGB offer precision in the spatial domain, they still operate within a visualization framework that aligns with human intuition. However, a model’s reliance on features is often rooted in the structural information encoded by frequency components, which are invisible in a simple spatial heatmap. To achieve a deeper, mechanistic understanding of feature dependence and model robustness, we shift our focus from pixels and gradients to the frequency domain.</p>

<p>This final method, Frequency Domain Explainability [2], challenges the spatial bias of traditional XAI by analyzing the relative influence of specific frequency bands on verification decisions. By moving beyond where the model looks to what kind of information is driving the decision, this approach offers a powerful new diagnostic tool, especially for quantifying algorithmic bias across different demographics.</p>

<h3 id="42-methodology-frequency-masking-and-influence-scoring">4.2 Methodology: Frequency Masking and Influence Scoring</h3>
<p>Mathematically, the convolution operation in the spatial domain is equivalent to element-wise multiplication in the frequency domain. Therefore, the learned kernels within a deep network act as filters that selectively emphasize or suppress different spatial frequencies in the input image. These frequencies encode distinct types of image information crucial for recognition, such as fine textures like face wrinkles (high frequency), identifiable features such as eyes and mouth (mid frequency), and overall shapes (low frequency).</p>

<p>The methodology for quantifying frequency influence is perturbation-based and systematic:</p>

<ol>
  <li><strong>Transformation:</strong> The input spatial image is transformed into the frequency domain using the Discrete Fourier Transform (DFT).</li>
  <li><strong>Masking:</strong> Specific radial frequency bands are masked (removing information present in those bands). This masked frequency image is then re-transformed back into the spatial domain in a lossless process.</li>
  <li><strong>Scoring:</strong> The resulting frequency-masked image, along with the unaltered image, is passed through the FR model to create face embeddings and calculate new cosine similarity scores \(S_{masked}\).</li>
  <li><strong>Influence Score:</strong> The importance of the masked frequency band is assessed by taking the difference between the unaltered baseline similarity score \(S_{unaltered}\) and the masked score \(S_{masked}\). This difference is interpreted as the direct influence of that frequency component on the verification decision.</li>
  <li><strong>FHP Generation:</strong> The normalized influences are visualized as Frequency Heat Plots (FHPs). These can be <strong>Absolute FHPs</strong>, which display the magnitude of the performance drop caused by masking a frequency band, or <strong>Directed FHPs</strong>, showing whether the masking operation worsened the similarity score (positive influence) or improved the score (negative influence).</li>
</ol>

<p>The detection of negative influence is highly valuable, as it diagnoses scenarios where the model was relying on identity-irrelevant artifacts or noise, which, when removed, stabilizes the decision and increases similarity.</p>

<h3 id="43-key-insights-diagnosing-structural-bias">4.3 Key Insights: Diagnosing Structural Bias</h3>
<p>The application of this frequency-based approach in extensive experiments on FR models yielded crucial diagnostic insights:</p>

<ul>
  <li><strong>Differential Feature Reliance:</strong> The experiments showed that different frequencies are important to FR models depending on the ethnicity of the input samples. This finding explains algorithmic bias beyond superficial visual factors.</li>
  <li><strong>Quantifying Bias:</strong> The differences in frequency importance across demographic groups were observed to increase proportionally with the degree of bias exhibited by the model. This means that frequency importance shifts serve as a measurable metric for diagnosing the structural feature dependencies driving discriminatory behavior.</li>
  <li><strong>Model Vulnerability:</strong> The approach was also applied to scenarios like cross-resolution FR and morphing attacks, allowing researchers to understand precisely which frequency bands (e.g., high-frequency texture or low-frequency shape) a model fails to rely on when performance degrades in challenging scenarios.</li>
</ul>

<p>The research establishes the spatial frequency domain as an essential dimension for explaining the complex decision-making processes of deep learning systems. The introduction of FHPs provides a powerful, quantitative, and diagnostically potent alternative to traditional spatial explanations.</p>

<p>Operationally, the current FHP generation methodology is computationally intensive. For the frequency-domain analysis to become a standard tool in high-throughput operational FR deployments, the field must pursue the development of lightweight, real-time frequency-domain explanation methods that can be integrated directly into the inference pipeline.</p>

<h2 id="5-conclusion">5. Conclusion</h2>
<p>The analysis reveals a key insight: modern FR models are sensitive to non-semantic cues that escape human notice. While classical models may imply that CNNs “look” at faces much like humans do, recent methods demonstrate that models frequently rely on fragile correlations and spurious shortcuts.</p>

<p>FastDiME’s counterfactual generation proves causality by showing that removing a specific artifact (like glasses or mask) can flip a prediction, while FGGB exposes the precise deep features contributing to false acceptances, and Frequency Heat Plots reveal hidden structural biases across demographics.</p>

<p>Moving forward, the integration of these XAI tools into the standard development pipeline is a must, in order for safe deployment. The shifts from post-hoc visualization to causal and structural analysis allows for detection of time where a model appears correct for the wrong reasons, because they can cause harm in the real world. However, challenges remain regarding computational efficiency. Methods like iterative frequency masking and diffusion-based generation are too expensive for real-time inference. Future research must focus on optimizing these tools to provide real-time feedback ensuring that robustness and fairness are not just retrospective metrics but active components of the recognition process.</p>

<h2 id="references">References</h2>

<p>[1] A. Darwiche, “Logic for explainable AI,” <em>arXiv:2305.05172</em>, May 2023. [Online]. Available: https://arxiv.org/abs/2305.05172</p>

<p>[2] M. Huber, F. Boutros, and N. Damer, “Frequency matters: Explaining biases of face recognition in the frequency domain,” <em>arXiv:2501.16896</em>, Jan. 2025. [Online]. Available: https://arxiv.org/abs/2501.16896</p>

<p>[3] Y. Lu, Z. Xu, and T. Ebrahimi, “Explainable face verification via feature-guided gradient backpropagation,” <em>arXiv:2403.04549</em>, Mar. 2024. [Online]. Available: https://arxiv.org/abs/2403.04549</p>

<p>[4] R. R. Selvaraju et al., “Grad-CAM: Visual explanations from deep networks via gradient-based localization,” <em>International Journal of Computer Vision</em>, vol. 128, no. 2, pp. 336–359, Oct. 2019. [Online]. Available: https://dx.doi.org/10.1007/s11263-019-01228-7</p>

<p>[5] N. Weng et al., “Fast diffusion-based counterfactuals for shortcut removal and generation,” <em>arXiv:2312.14223</em>, Dec. 2023. [Online]. Available: https://arxiv.org/abs/2312.14223</p>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']]
  }
};
</script>

<script type="text/javascript" id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>]]></content><author><name>Erick Rosas Gonzalez, Maya Josifovska, Andrew Rubio, Wanda Barahona</name></author><summary type="html"><![CDATA[Facial Recognition (FR) systems are being increasingly used in high stake environments, but their decision making processes remain a mystery, raising concerns regarding trust, bias, and robustness. Traditional methods such as occlusion sensitivity or saliency maps (e.g., Grad-CAM), often fail to capture the causal mechanisms driving verification decisions or diagnosis reliance on shortcuts. This report analyzes three modern paradigms that shift Explainable AI (XAI) from passive visualization to active, feature level interrogation. We examined FastDiME [5] which utilizes generative diffusion models to create counterfactuals for detecting shortcut learning, Feature Guided Gradient Backpropagation (FGGB) [3], which mitigates vanishing gradients to produce similarity and dissimilarity maps, and Frequency Domain Explainability [2], which introduces Frequency Heat Plots (FHPs) to diagnose biases in CNNs. By synthesizing these approaches, we examine how modern XAI tools can assess model reliance on noise versus structural identity, with the goal of offering a pathway toward more robust and transparent biometric systems.]]></summary></entry><entry><title type="html">Efficient Super-Resolution: Bridging Quality and Computation</title><link href="/CS163-Projects-2025Fall/2025/12/13/team48-efficient-super-resolution.html" rel="alternate" type="text/html" title="Efficient Super-Resolution: Bridging Quality and Computation" /><published>2025-12-13T00:00:00+00:00</published><updated>2025-12-13T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/13/team48-efficient-super-resolution</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/13/team48-efficient-super-resolution.html"><![CDATA[<blockquote>
  <p>Super-resolution has long faced a fundamental tension: the highest-quality models require billions of operations, while edge devices demand sub-100ms inference. This article examines three recent methods—SPAN, EFDN, and DSCLoRa—that challenge this tradeoff through architectural innovation, training-time tricks, and efficient adaptation. We’ll see how rethinking the upsampling operation, leveraging structural reparameterization, and applying low-rank decomposition can each dramatically improve efficiency without sacrificing output quality.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#the-efficiency-problem-in-super-resolution" id="markdown-toc-the-efficiency-problem-in-super-resolution">The Efficiency Problem in Super-Resolution</a></li>
  <li><a href="#background-the-anatomy-of-an-sr-network" id="markdown-toc-background-the-anatomy-of-an-sr-network">Background: The Anatomy of an SR Network</a>    <ul>
      <li><a href="#background-measuring-performance" id="markdown-toc-background-measuring-performance">Background: Measuring Performance</a></li>
    </ul>
  </li>
  <li><a href="#efdn-edge-enhanced-feature-distillation-network-for-efficient-super-resolution-ntire-2023-winner-1" id="markdown-toc-efdn-edge-enhanced-feature-distillation-network-for-efficient-super-resolution-ntire-2023-winner-1">EFDN: Edge-enhanced Feature Distillation Network for Efficient Super-Resolution (NTIRE 2023 Winner) [1]</a>    <ul>
      <li><a href="#the-edge-problem" id="markdown-toc-the-edge-problem">The Edge Problem</a></li>
      <li><a href="#structural-reparameterization-training--inference" id="markdown-toc-structural-reparameterization-training--inference">Structural Reparameterization: Training ≠ Inference</a></li>
      <li><a href="#edge-enhanced-gradient-loss" id="markdown-toc-edge-enhanced-gradient-loss">Edge-Enhanced Gradient Loss</a></li>
      <li><a href="#quantitative-results" id="markdown-toc-quantitative-results">Quantitative Results</a></li>
      <li><a href="#ablation-study-scale-times-2" id="markdown-toc-ablation-study-scale-times-2">Ablation Study: (Scale \(\times 2\))</a></li>
    </ul>
  </li>
  <li><a href="#span-swift-parameter-free-attention-network-ntire-2024-winner-2" id="markdown-toc-span-swift-parameter-free-attention-network-ntire-2024-winner-2">SPAN: Swift Parameter-free Attention Network (NTIRE 2024 Winner) [2]</a>    <ul>
      <li><a href="#spab-the-swift-parameter-free-attention-block" id="markdown-toc-spab-the-swift-parameter-free-attention-block">SPAB: the Swift Parameter-free Attention Block</a></li>
      <li><a href="#architectural-nuances" id="markdown-toc-architectural-nuances">Architectural Nuances</a></li>
      <li><a href="#experimental-results" id="markdown-toc-experimental-results">Experimental Results:</a></li>
    </ul>
  </li>
  <li><a href="#dsclora-efficient-domain-adaptation-via-dynamic-sparse-low-rank-matrices-ntire-2025-winner-3" id="markdown-toc-dsclora-efficient-domain-adaptation-via-dynamic-sparse-low-rank-matrices-ntire-2025-winner-3">DSCLoRa: Efficient Domain Adaptation via Dynamic Sparse Low-Rank Matrices (NTIRE 2025 Winner) [3]</a>    <ul>
      <li><a href="#innovation-1-convlora--sconvlb" id="markdown-toc-innovation-1-convlora--sconvlb">Innovation 1: ConvLoRA &amp; SConvLB</a></li>
      <li><a href="#innovation-2-spatial-affinity-distillation" id="markdown-toc-innovation-2-spatial-affinity-distillation">Innovation 2: Spatial Affinity Distillation</a></li>
      <li><a href="#architecture-the-sconvlb-upgrade" id="markdown-toc-architecture-the-sconvlb-upgrade">Architecture: The “SConvLB” Upgrade</a></li>
      <li><a href="#experimental-results-1" id="markdown-toc-experimental-results-1">Experimental Results</a></li>
      <li><a href="#why-does-this-matter" id="markdown-toc-why-does-this-matter">Why does this matter?</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
  <li><a href="#references-1" id="markdown-toc-references-1">References</a></li>
</ul>

<p>Image super-resolution is the problem of taking a low-resolution image and turning it into a high-resolution image. Ex. taking a 1080p picture on your cell phone and upscaling it into a beautiful 4K photo. Using deep learning, we can essentially recover detail from the low-resolution input by training a model to learn the relationship between the low- and high- resolution versions of an image.</p>

<h2 id="the-efficiency-problem-in-super-resolution">The Efficiency Problem in Super-Resolution</h2>

<p>Leading edge image super-resolution models have gotten very good. Unfortunately, the best models are unsuited to be run on lower-end edge devices due to three main reasons:</p>
<ul>
  <li>Runtime: models designed to be run on server GPUs will run far slower on edge devices. This makes them unsuitable for real-time or close to real-time applications where the image can’t be shipped off to an inference server</li>
  <li>Parameter Count: edge devices have limited VRAM, RAM, and storage to store the models in, especially considering that they will be running programs other than super-resolution</li>
  <li>FLOPs: edge devices have limited CPU and GPU compute to run models with, especially considering that they will be running other programs</li>
</ul>

<p>As such, the problem of “efficient image super-resolution” is how to design models to reduce the runtime and parameter count while maintaining some baseline of decent upscaling performance.</p>

<p>The inspiration for this report was the NTIRE Challenge on Efficient Super Resolution (NTIRE ESR), held annually at CVPR. I will analyze three submissions to this challenge, from different years, to compare the differing methods used in recent years to further improve efficient image super resolution models.</p>

<h2 id="background-the-anatomy-of-an-sr-network">Background: The Anatomy of an SR Network</h2>

<p>Modern SR networks follow a common template:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Low-Resolution Input → Feature Extraction → Upsampling → High-Resolution Output
</code></pre></div></div>

<p><strong>Feature extraction</strong> uses stacked convolutional blocks to learn hierarchical representations. This is where most computation happens.</p>

<p><strong>Upsampling</strong> increases spatial resolution. The dominant method is <em>sub-pixel convolution</em> (pixel shuffle), which:</p>
<ol>
  <li>Expands channels from \(C\) to \(C \cdot r^2\) (where \(r\) is the scale factor)</li>
  <li>Rearranges these channels into spatial dimensions: \((H, W, C \cdot r^2) \rightarrow (rH, rW, C)\)</li>
</ol>

<p>This channel expansion is expensive. For \(\times 4\) upsampling with 64 channels, a single \(3 \times 3\) convolution requires:
\(64 \times 1024 \times 9 = 589,824 \text{ parameters}\)</p>

<p>In EDSR-baseline (1.37M total parameters), upsampling alone accounts for 31%—nearly a third of the model.</p>

<p>The three papers I analyze optimize different parts of this pipeline.</p>
<h3 id="background-measuring-performance">Background: Measuring Performance</h3>
<p>There are many ways to measure the performance of an image super-resolution model.
To generate training data, generally the approach is to take a high-resolution image and apply bicubic downsampling to generate the low-resolution version. Then, evaluate the model by applying the model to the low-resolution version and seeing how close the model’s high-resolution generation matches the original high-resolution version.</p>

<p><strong>PSNR (Peak Signal-to-Noise Ratio)</strong> compares the “noise” of an image to the original. This noise is calculated using pixel differences between the two images. This is a good measure of pure, mathematical “difference” between two signals, but for images, it should be clear that small differences in pixel values may not matter much. Ex. red 254 vs red 255 is technically a difference, but not very noticeable.</p>

<p>PSNR is measured in decibels (dB), which you will see in the later sections. For the 2025 NTIRE ESR challenge, the submissions were required to have a minimum PSNR of 26.99 on a certain test set.</p>

<p><strong>SSIM (Structural Similarity Index Measure)</strong> compares the luminance, contrast, and structure of two images to measure similarity, calculated using the sample means, variances, and covariance of the pixels in both images. It can be thought of as a more higher-level approach as to how similar two images look, vs. how mathematically similar they are.</p>

<h2 id="efdn-edge-enhanced-feature-distillation-network-for-efficient-super-resolution-ntire-2023-winner-1">EFDN: Edge-enhanced Feature Distillation Network for Efficient Super-Resolution (NTIRE 2023 Winner) <a href="#1">[1]</a></h2>

<h3 id="the-edge-problem">The Edge Problem</h3>

<p>Efficient SR networks often produce blurry edges. One reason why is that, to the model, edges are not much different than any regular pixels. A one pixel difference in an edge may make it blurry, whereas a one pixel difference in a smooth region may not even be noticeable. Both will be interpreted as similarly bad by the model.</p>

<p>EFDN attempts to solve this from two angles:</p>
<ol>
  <li><strong>Architecture</strong>: Edge-specialized convolution blocks</li>
  <li><strong>Training</strong>: Explicit edge supervision via gradient losses</li>
</ol>

<p>The key innovation is achieving both while minimizing increases to inference cost. At deployment, the edge-specialized convolutions are packed as regular convolutions.</p>

<h3 id="structural-reparameterization-training--inference">Structural Reparameterization: Training ≠ Inference</h3>

<p>EFDN builds on <em>structural reparameterization</em>, similar to RepVGG and DBB. The core idea: train a complex multi-branch architecture, then merge branches into a single convolution for deployment.</p>

<p><strong>During training</strong>, EFDN’s EDBB block uses <strong>seven parallel branches</strong> to enforce explicit edge learning:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input
  ├── 3×3 Conv (standard features)
  ├── 1×1 Conv (channel mixing)
  ├── 1×1 Conv → 3×3 Conv (expanding-squeezing)
  ├── 1×1 Conv → Sobel-X (learnable horizontal edges)
  ├── 1×1 Conv → Sobel-Y (learnable vertical edges)
  ├── 1×1 Conv → Laplacian (isotropic edges)
  └── Identity (residual)
       ↓
   Output = sum of all branches
</code></pre></div></div>

<p>Instead of just asymmetric shapes, EFDN leverages <strong>Scaled Filter Convolutions</strong>. The \(1 \times 1\) convolutions before the fixed edge filters (Sobel/Laplacian) act as learnable scalers, allowing the network to adaptively weight the contribution of specific gradient directions. During inference, all branches (including the fixed filters) are mathematically collapsed into a single \(3 \times 3\) vanilla convolution. Ex.:</p>

\[W_{\text{deploy}} = W_{3 \times 3} + \text{pad}(W_{3 \times 1}) + \text{pad}(W_{1 \times 3}) + W_{\text{identity}}\]

<p>where \(\text{pad}()\) zero-pads the asymmetric kernels to \(3 \times 3\), and \(W_{\text{identity}}\) is the identity mapping expressed as convolution weights.The model during training sees seven specialized pathways; the model during inference sees one standard convolution ⟶ Zero overhead.</p>

<h3 id="edge-enhanced-gradient-loss">Edge-Enhanced Gradient Loss</h3>

<p>EFDN also introduces explicit edge supervision through loss:</p>

\[\mathcal{L} = \mathcal{L}_{\text{pixel}} + \lambda_x \mathcal{L}_x + \lambda_y \mathcal{L}_y + \lambda_l \mathcal{L}_l\]

<p>where:</p>
<ul>
  <li>\(\mathcal{L}_{\text{pixel}}\): Standard L1 loss on RGB values</li>
  <li>\(\mathcal{L}_x\): L1 loss on horizontal Sobel gradients</li>
  <li>\(\mathcal{L}_y\): L1 loss on vertical Sobel gradients</li>
  <li>\(\mathcal{L}_l\): L1 loss on Laplacian (edge localization)</li>
</ul>

<p>The Sobel operators detect gradients:
\(S_x = \begin{bmatrix} -1 &amp; 0 &amp; 1 \\ -2 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 1 \end{bmatrix}, \quad S_y = \begin{bmatrix} -1 &amp; -2 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 2 &amp; 1 \end{bmatrix}\)</p>

<p>The Laplacian detects edges regardless of orientation:
\(L = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; -4 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}\)</p>

<p>The authors use these filters to calculate gradient maps. These maps are then unfolded into patches, the patches used to calculate gradient variances, the variances used to calculate gradient variance loss for each of the three filters.</p>

\[\begin{align}
\mathcal{L}_{x} &amp;= \mathbb{E}_{I^{SR}} \| v_{x}^{HR} - v_{x}^{SR} \|_{2} \\
\mathcal{L}_{y} &amp;= \mathbb{E}_{I^{SR}} \| v_{y}^{HR} - v_{y}^{SR} \|_{2} \\
\mathcal{L}_{l} &amp;= \mathbb{E}_{I^{SR}} \| v_{l}^{HR} - v_{l}^{SR} \|_{2}
\end{align}\]

<p>By minimizing gradient errors directly, the network learns to preserve high-frequency details that pixel-only losses would smooth over.</p>

<h4 id="hyperparameter-analysis">Hyperparameter Analysis</h4>

<p>The paper experiments with different \(\lambda\) values:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">\(\lambda_x = \lambda_y\)</th>
      <th style="text-align: center">\(\lambda_l\)</th>
      <th style="text-align: center">Set5 PSNR</th>
      <th style="text-align: left">Visual Quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0.1</td>
      <td style="text-align: center">0.05</td>
      <td style="text-align: center">32.15 dB</td>
      <td style="text-align: left">Slight blur</td>
    </tr>
    <tr>
      <td style="text-align: center">0.2</td>
      <td style="text-align: center">0.1</td>
      <td style="text-align: center">32.19 dB</td>
      <td style="text-align: left">Sharp edges</td>
    </tr>
    <tr>
      <td style="text-align: center">0.3</td>
      <td style="text-align: center">0.15</td>
      <td style="text-align: center">32.17 dB</td>
      <td style="text-align: left">Oversharpened</td>
    </tr>
    <tr>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.2</td>
      <td style="text-align: center">32.03 dB</td>
      <td style="text-align: left">Ringing artifacts</td>
    </tr>
  </tbody>
</table>

<p>The authors decided on a sweet spot of \(\lambda_x = \lambda_y = 0.2\), \(\lambda_l = 0.1\). Too high causes ringing; too low reverts to pixel-loss behavior.</p>

<h3 id="quantitative-results">Quantitative Results</h3>

<p>Comparison on \(\times 4\) SR benchmarks:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Method</th>
      <th style="text-align: center">Params</th>
      <th style="text-align: center">Multi-Adds</th>
      <th style="text-align: center">Set5</th>
      <th style="text-align: center">Urban100</th>
      <th style="text-align: center">Inference (RTX 3090)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">CARN</td>
      <td style="text-align: center">1,592K</td>
      <td style="text-align: center">90.9G</td>
      <td style="text-align: center">32.13 dB</td>
      <td style="text-align: center">26.07 dB</td>
      <td style="text-align: center">-</td>
    </tr>
    <tr>
      <td style="text-align: left">IMDN</td>
      <td style="text-align: center">715K</td>
      <td style="text-align: center">41.0G</td>
      <td style="text-align: center">32.13 dB</td>
      <td style="text-align: center">26.04 dB</td>
      <td style="text-align: center">92 ms</td>
    </tr>
    <tr>
      <td style="text-align: left">PAN</td>
      <td style="text-align: center">272K</td>
      <td style="text-align: center">28.2G</td>
      <td style="text-align: center">32.13 dB</td>
      <td style="text-align: center">26.11 dB</td>
      <td style="text-align: center">-</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>EFDN</strong></td>
      <td style="text-align: center"><strong>276K</strong></td>
      <td style="text-align: center"><strong>14.7G</strong></td>
      <td style="text-align: center"><strong>32.08 dB</strong></td>
      <td style="text-align: center"><strong>26.00 dB</strong></td>
      <td style="text-align: center"><strong>19 ms</strong></td>
    </tr>
  </tbody>
</table>

<p>EFDN achieves the best efficiency: ~2.5x fewer Multi-Adds than IMDN and significantly faster inference (19ms vs 92ms), while maintaining comparable quality.</p>

<h3 id="ablation-study-scale-times-2">Ablation Study: (Scale \(\times 2\))</h3>
<p>As is standard, the authors performed an ablation study to test the improvements from each new addition independently, then all togehter.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Configuration</th>
      <th style="text-align: center">Set5 PSNR</th>
      <th style="text-align: center">Inference Cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Baseline (\(L_1\))</td>
      <td style="text-align: center">37.09 dB</td>
      <td style="text-align: center">Baseline</td>
    </tr>
    <tr>
      <td style="text-align: left">+ Edge Loss (\(L_{EG}\))</td>
      <td style="text-align: center">37.14 dB</td>
      <td style="text-align: center">+0 ms</td>
    </tr>
    <tr>
      <td style="text-align: left">+ Edge Blocks (EDBB)</td>
      <td style="text-align: center">37.19 dB</td>
      <td style="text-align: center">+0 ms</td>
    </tr>
    <tr>
      <td style="text-align: left">+ Both (EFDN strategy)</td>
      <td style="text-align: center">37.27 dB</td>
      <td style="text-align: center">+0 ms</td>
    </tr>
  </tbody>
</table>

<p>The edge blocks and edge loss are complementary:</p>
<ul>
  <li><strong>EDBB:</strong> Adds structural extraction capacity (Sobel/Laplacian branches) that collapses to a single conv at inference.</li>
  <li><strong>\(L_{EG}\):</strong> Calibrates the training of these branches to focus on gradient variance.</li>
</ul>

<p>Together they achieve consistent improvement (+0.18 dB) at <strong>zero</strong> additional inference cost.</p>

<h2 id="span-swift-parameter-free-attention-network-ntire-2024-winner-2">SPAN: Swift Parameter-free Attention Network (NTIRE 2024 Winner) <a href="#2">[2]</a></h2>

<p>Attention mechanisms have become increasingly important to achieve high-quality super resolution. Architectures like RCAN or SwinIR leverage channel or spatial attention to focus processing power on high-frequency details (textures, edges) rather than smooth backgrounds.</p>

<p>However, attention mechanisms are computationally expensive. SPAN attempts to replicate the benefits of attention without dramatically increasing the complexity and parameter count of the model.</p>

<p>SPAN’s fundamental hypothesis is that the network doesn’t need a separate branch to tell it which features are important. In Convolutional Neural Networks (CNNs), feature magnitude is already a proxy for importance. In SR tasks, “information-rich” regions like edges and complex textures typically trigger high-magnitude activations in convolutional filters (similar to how a Sobel filter reacts strongly to an edge). SPAN leverages this by deriving the attention map directly from the feature map itself, eliminating the need for extra learnable weights.</p>

<h3 id="spab-the-swift-parameter-free-attention-block">SPAB: the Swift Parameter-free Attention Block</h3>

<p>The backbone of SPAN consists of cascaded <strong>Swift Parameter-free Attention Blocks (SPABs)</strong>. Unlike traditional blocks that use separate pathways for attention, SPAB integrates it directly into the feature extraction flow.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/48/SPAN.png" alt="YOLO" /></p>
<p><em>Fig 1. SPAN: architecture design</em>.</p>

<p>Essentially, SPAB applies an activation function on the feature map to generate an attention map. This attention map is then element-wise multiplied with the (feature map + residual connection) to get the output of the SPAB block.</p>

<p>High magnitude activations in a CNN feature map generally correspond to important features like edges, keypoints, etc. The authors wanted an activation function that kept these high magnitude activations while suppressing near-zero activations. The sign of the activation also should not matter, since key features generate high magnitude activations regardless of sign (ex. a Sobel filter detects an edge no matter if the edge is oriented one way or the other). Thus, the activation function should be odd-symmetric.</p>

<p>After experimentation, the authors settled on a shifted sigmoid to generate the attention map \(V_i\):</p>

<p>\(\sigma_a(x) = \text{Sigmoid}(x) - 0.5\)
By shifting the Sigmoid, the function becomes odd-symmetric about the origin. This ensures that features with high absolute magnitudes (whether positive or negative) generate significant responses in the attention map, while near-zero values (smooth background regions) are suppressed.</p>

<h4 id="mathematical-intuition-self-supervised-gradient-boosting">Mathematical Intuition: Self-Supervised Gradient Boosting</h4>
<p>The paper provides a theoretical analysis of why this works. When using this attention mechanism, the gradient used to update the weights during training is scaled by the attention map itself.</p>

<p>Mathematically, the gradient term effectively becomes proportional to the activation magnitude. This means the network naturally receives <strong>stronger supervision signals in information-rich regions</strong> (edges/textures) and weaker signals in smooth regions. It acts as a form of self-supervised hard-example mining, forcing the network to “try harder” on difficult textures without explicit guidance.</p>

<h3 id="architectural-nuances">Architectural Nuances</h3>

<p><strong>The “Forgetfulness” Problem &amp; Residual Connections</strong>
One danger of attention mechanisms is that they act as filters. By aggressively modulating features, deep networks can inadvertently suppress low-level information needed for reconstruction. SPAN solves this by adding a residual connection <em>within</em> the attention block.</p>
<ul>
  <li><strong>Mechanism:</strong> The input to the block (\(O_{i-1}\)) is added to the extracted features (\(H_i\)) before the attention modulation is applied.</li>
  <li><strong>Result:</strong> This ensures that even if the attention map suppresses the current layer’s output, the original information flows through, preventing the “vanishing feature” problem common in deep attention networks.</li>
  <li><strong>Testing</strong>: From the authors’ testing, the residual connections substantially increased the ability of the attention mechanism to retain information</li>
</ul>

<p><strong>Structural Re-parameterization</strong>
To further boost speed, SPAN employs structural re-parameterization (similar to RepVGG).</p>

<h3 id="experimental-results">Experimental Results:</h3>

<p>SPAN won <strong>1st place</strong> in both the <em>Overall Performance</em> and <em>Runtime</em> tracks of the <strong>NTIRE 2024 Efficient Super-Resolution Challenge</strong>.</p>

<p><strong>Performance vs. Latency Trade-off (\(\times 4\) Upscaling):</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Method</th>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Inference Speed (RTX 3090)</th>
      <th style="text-align: center">PSNR (Urban100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>IMDN</strong></td>
      <td style="text-align: center">715K</td>
      <td style="text-align: center">20.56 ms</td>
      <td style="text-align: center">26.04 dB</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>RLFN</strong></td>
      <td style="text-align: center">543K</td>
      <td style="text-align: center">16.41 ms</td>
      <td style="text-align: center">26.17 dB</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>SPAN</strong></td>
      <td style="text-align: center"><strong>498K</strong></td>
      <td style="text-align: center"><strong>13.67 ms</strong></td>
      <td style="text-align: center"><strong>26.18 dB</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Key Takeaways:</strong></p>
<ol>
  <li><strong>Fastest in Class:</strong> SPAN runs ~15% faster than RLFN and ~33% faster than IMDN while using fewer parameters.</li>
  <li><strong>Visual Quality:</strong> Qualitative comparisons show that SPAN reconstructs sharper lattice structures (e.g., building windows in Urban100) where other lightweight models suffer from aliasing or blurring.</li>
  <li><strong>Real-World Viability:</strong> By removing the “latency tax” of attention, SPAN proves that high-performance SR is feasible on constrained hardware without sacrificing the benefits of content-aware processing.</li>
</ol>

<h2 id="dsclora-efficient-domain-adaptation-via-dynamic-sparse-low-rank-matrices-ntire-2025-winner-3">DSCLoRa: Efficient Domain Adaptation via Dynamic Sparse Low-Rank Matrices (NTIRE 2025 Winner) <a href="#3">[3]</a></h2>

<p>Winner of the <strong>NTIRE 2025 Efficient Super-Resolution Challenge (Overall Performance Track)</strong>, DSCLoRA introduces a framework to fine-tune lightweight models (specifically SPAN) using Low-Rank Adaptation supervised by a massive teacher network. Through these methods, we are able to squeeze more performance out of existing architectures.</p>

<h3 id="innovation-1-convlora--sconvlb">Innovation 1: ConvLoRA &amp; SConvLB</h3>

<p>While LoRA is famous in the LLM world for fine-tuning transformers, DSCLoRA adapts it for Convolutional Neural Networks (CNNs) via the <strong>SConvLB (Super-resolution Convolutional LoRA Block)</strong>.</p>

<p><strong>The Mechanism:</strong>
Standard LoRA approximates weight updates using low-rank matrices. DSCLoRA applies this to convolution kernels. For a frozen pre-trained convolution weight \(W_{PT}\), the update is modeled as:</p>

\[W_{\text{updated}} = W_{PT} + X \times Y\]

<p>where \(Y\) (initialized to zero) and \(X\) (random Gaussian) are low-rank matrices (\(r \ll \text{channel dimension}\)).</p>

<p><strong>Zero-Overhead Inference:</strong>
 Because convolution is a linear operation, the learned low-rank weights (\(X \times Y\)) can be mathematically merged into the original frozen weights:</p>

\[W_{\text{final}} = W_{PT} + (X \times Y)\]

<p>During inference, the auxiliary LoRA branches are removed. The model architecture remains <strong>identical</strong> to the original lightweight baseline, meaning <strong>zero additional FLOPs</strong> and <strong>zero extra inference latency</strong>, despite the performance boost.</p>

<h3 id="innovation-2-spatial-affinity-distillation">Innovation 2: Spatial Affinity Distillation</h3>

<p>Simply adding LoRA layers isn’t enough to achieve state-of-the-art results. The authors found that pixel-wise loss functions (L1/L2) fail to capture the high-frequency structural details essential for super-resolution.</p>

<p>To solve this, DSCLoRA employs <strong>Spatial Affinity Knowledge Distillation (SAKD)</strong>.</p>

<p><strong>How it works:</strong>
Instead of forcing the student (DSCLoRA) to mimic the teacher’s (Large SPAN) raw feature maps, it forces the student to mimic the teacher’s <em>spatial relationships</em>.</p>
<ol>
  <li>Feature maps \(F \in \mathbb{R}^{C \times H \times W}\) are flattened.</li>
  <li>An affinity matrix \(A \in \mathbb{R}^{HW \times HW}\) is computed, representing the pairwise similarity between every spatial location in the image.</li>
  <li>The distillation loss minimizes the distance between the Student’s and Teacher’s affinity matrices:</li>
</ol>

\[\mathcal{L}_{AD} = \frac{1}{|A|} \sum \| A_{student} - A_{teacher} \|_1\]

<p>This transfers the “structural knowledge” (texture patterns, edge continuity) from the heavy teacher to the lightweight student, guiding the optimization of the LoRA parameters.</p>

<h3 id="architecture-the-sconvlb-upgrade">Architecture: The “SConvLB” Upgrade</h3>

<p>The framework is built upon the <strong>SPAN (Swift Parameter-free Attention Network)</strong> architecture. DSCLoRA modifies it by:</p>
<ol>
  <li>Replacing Blocks: Standard SPAB blocks are replaced with <strong>SConvLB</strong> blocks containing the parallel LoRA branches.</li>
  <li>Applying ConvLoRA to the main feature extraction convolutions and the critical upsampling (PixelShuffle) layers.</li>
  <li>Better Training: The backbone weights are frozen; only the small LoRA parameters are updated.</li>
</ol>

<h3 id="experimental-results-1">Experimental Results</h3>

<p>The method was evaluated on standard SR benchmarks (\(\times 4\) scale), comparing the original lightweight SPAN against the DSCLoRA-enhanced version.</p>

<p><strong>Quantitative Performance (Manga109):</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: center">Params (Inference)</th>
      <th style="text-align: center">FLOPs</th>
      <th style="text-align: center">PSNR</th>
      <th style="text-align: center">SSIM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">SPAN-Tiny (Baseline)</td>
      <td style="text-align: center">131K</td>
      <td style="text-align: center">8.54G</td>
      <td style="text-align: center">29.56 dB</td>
      <td style="text-align: center">0.8967</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>DSCLoRA (Ours)</strong></td>
      <td style="text-align: center"><strong>131K</strong></td>
      <td style="text-align: center"><strong>8.54G</strong></td>
      <td style="text-align: center"><strong>29.60 dB</strong></td>
      <td style="text-align: center"><strong>0.8971</strong></td>
    </tr>
  </tbody>
</table>

<p>Some takeaways:</p>
<ul>
  <li>Both models have the exact same parameter count and FLOPs during inference.</li>
  <li>DSCLoRA achieves consistently higher PSNR/SSIM across all benchmarks (Set5, Set14, Urban100, Manga109).</li>
  <li>Qualitative comparisons show DSCLoRA recovers finer details (e.g., building lattices, animal fur) where the baseline and other lightweight models (like RFDN or RLFN) produce aliasing or blur.</li>
</ul>

<h3 id="why-does-this-matter">Why does this matter?</h3>
<p>Nevertheless, DSCLoRA (using SPAN Tiny) won the NTIRE 2025 ESR Challenge, beating out SPANF, a further optimized version of SPAN from the original authors.</p>

<p>DSCLoRA shifts the focus from <em>architecture design</em> to <em>training dynamics</em>. It proves that existing efficient architectures are often under-optimized. By using a temporary, high-capacity training setup (Teacher + LoRA) that collapses into a simple deployment model, we can achieve superior results on resource-constrained devices without any hardware penalties.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Overall, we’ve seen impressive growth in efficient super resolution models. PSNR/SSIM is just as good or better while models have achieved ~5x reductions in parameter size and runtime. There is a lot of optimizations to be gained, whether through better architectures (focusing on edges or attention maps), more nuanced loss functions, or additional work put into finetuning and further training models.</p>

<hr />

<h2 id="references">References</h2>
<h2 id="references-1">References</h2>

<p><a id="1">[1]</a> Y. Wang, “Edge-Enhanced Feature Distillation Network for Efficient Super-Resolution,” in <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, 2022, pp. 777-785.</p>

<p><a id="2">[2]</a> C. Wan, H. Yu, Z. Li, Y. Chen, Y. Zou, Y. Liu, X. Yin, and K. Zuo, “Swift Parameter-free Attention Network for Efficient Super-Resolution,” in <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024, pp. 6246–6256.</p>

<p><a id="3">[3]</a> X. Chai, Y. Zhang, Y. Zhang, Z. Cheng, Y. Qin, Y. Yang, and L. Song, “Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution,” <em>arXiv preprint arXiv:2504.11271</em>, 2025. [Online]. Available: https://arxiv.org/abs/2504.11271</p>]]></content><author><name>Anthony Yu</name></author><summary type="html"><![CDATA[Super-resolution has long faced a fundamental tension: the highest-quality models require billions of operations, while edge devices demand sub-100ms inference. This article examines three recent methods—SPAN, EFDN, and DSCLoRa—that challenge this tradeoff through architectural innovation, training-time tricks, and efficient adaptation. We’ll see how rethinking the upsampling operation, leveraging structural reparameterization, and applying low-rank decomposition can each dramatically improve efficiency without sacrificing output quality.]]></summary></entry><entry><title type="html">Representation and Prediction for Generalizable Robot Control</title><link href="/CS163-Projects-2025Fall/2025/12/13/team50-control-from-vision.html" rel="alternate" type="text/html" title="Representation and Prediction for Generalizable Robot Control" /><published>2025-12-13T00:00:00+00:00</published><updated>2025-12-13T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/13/team50-control-from-vision</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/13/team50-control-from-vision.html"><![CDATA[<blockquote>
  <p>Vision-based learning has become a central paradigm for enabling robots to operate in complex, unstructured environments. Rather than relying on hand-engineered perception pipelines or task-specific supervision, recent work increasingly leverages large-scale video data to learn transferable visual representations and predictive models for control. This survey reviews a sequence of recent approaches that illustrate this progression: learning control directly from video demonstrations, pretraining universal visual representations, incorporating predictive dynamics through visual point tracking, and augmenting learning with synthetic visual data. Together, these works highlight how representation learning and prediction from video are enabling increasingly generalizable robot manipulation capabilities.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#video-learning" id="markdown-toc-video-learning">Video Learning</a>    <ul>
      <li><a href="#real-to-sim-to-real-from-monocular-video" id="markdown-toc-real-to-sim-to-real-from-monocular-video">Real-to-Sim-to-Real from Monocular Video</a></li>
      <li><a href="#joint-humanscene-reconstruction" id="markdown-toc-joint-humanscene-reconstruction">Joint Human–Scene Reconstruction</a></li>
      <li><a href="#motion-retargeting-and-physics-aware-imitation" id="markdown-toc-motion-retargeting-and-physics-aware-imitation">Motion Retargeting and Physics-Aware Imitation</a></li>
      <li><a href="#context-aware-control-via-policy-distillation" id="markdown-toc-context-aware-control-via-policy-distillation">Context-Aware Control via Policy Distillation</a></li>
    </ul>
  </li>
  <li><a href="#generalizable-visual-representations" id="markdown-toc-generalizable-visual-representations">Generalizable Visual Representations</a>    <ul>
      <li><a href="#problem-setup" id="markdown-toc-problem-setup">Problem Setup</a></li>
      <li><a href="#learning-objectives" id="markdown-toc-learning-objectives">Learning Objectives</a></li>
      <li><a href="#full-training-objective" id="markdown-toc-full-training-objective">Full Training Objective</a></li>
      <li><a href="#downstream-policy-learning" id="markdown-toc-downstream-policy-learning">Downstream Policy Learning</a></li>
      <li><a href="#empirical-results-and-implications" id="markdown-toc-empirical-results-and-implications">Empirical Results and Implications</a></li>
    </ul>
  </li>
  <li><a href="#context-expansion" id="markdown-toc-context-expansion">Context Expansion</a>    <ul>
      <li><a href="#predicting-object-motion-via-point-tracks" id="markdown-toc-predicting-object-motion-via-point-tracks">Predicting Object Motion via Point Tracks</a></li>
      <li><a href="#from-2d-tracks-to-3d-rigid-motion" id="markdown-toc-from-2d-tracks-to-3d-rigid-motion">From 2D Tracks to 3D Rigid Motion</a></li>
      <li><a href="#open-loop-execution" id="markdown-toc-open-loop-execution">Open-Loop Execution</a></li>
      <li><a href="#residual-policies-for-closed-loop-correction" id="markdown-toc-residual-policies-for-closed-loop-correction">Residual Policies for Closed-Loop Correction</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#acknowledgements" id="markdown-toc-acknowledgements">Acknowledgements</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Vision has become a primary interface through which robots perceive, reason, and act in the physical world. As robots leave structured factory floors and enter human environments, hand-engineered perception pipelines and narrowly specified rewards increasingly fail to scale. Recent research instead treats video as a unifying supervision signal, leveraging its richness to learn perception, dynamics, and control jointly or modularly.</p>

<p>This post surveys a sequence of representative approaches that illustrate this trajectory. We begin with methods that directly convert human videos into executable robot behaviors, then move toward approaches that decouple perception from control through reusable visual representations. We conclude by examining predictive, object-centric models that anticipate future scene evolution. Together, these works outline a shift toward generalizable, context-aware robot control grounded in visual experience.</p>

<h2 id="video-learning">Video Learning</h2>

<p>A natural starting point for learning robot behavior from vision is direct imitation from video. Rather than decomposing perception and control into separate modules, recent work investigates whether robots can acquire complex, environment-aware skills simply by observing humans act in the world. <strong>VideoMimic</strong> represents a strong instantiation of this paradigm, demonstrating that monocular human videos can be converted into transferable, context-aware humanoid control policies.</p>

<h3 id="real-to-sim-to-real-from-monocular-video">Real-to-Sim-to-Real from Monocular Video</h3>

<p>VideoMimic introduces a real-to-sim-to-real pipeline that transforms casually recorded RGB videos into whole-body control policies for humanoid robots. Given a monocular video, the system jointly reconstructs <strong>4D human motion</strong> and <strong>static scene geometry</strong> in a common world frame. Unlike earlier approaches that reconstruct only the person or assume simplified environments, VideoMimic explicitly recovers both the human trajectory and the surrounding physical context, such as stairs, furniture, or uneven terrain, at metric scale.</p>

<p>This joint reconstruction enables downstream learning to reason about <strong>environment-conditioned behavior</strong>, rather than memorizing isolated motion patterns.</p>

<hr />

<h3 id="joint-humanscene-reconstruction">Joint Human–Scene Reconstruction</h3>

<p>Given an input video, VideoMimic estimates per-frame human pose using a parametric body model and reconstructs scene geometry via monocular structure-from-motion. The key challenge is <strong>metric alignment</strong>: monocular reconstructions are inherently scale-ambiguous.</p>

<p>VideoMimic resolves this by jointly optimizing human motion and scene scale using a human-height prior. Let</p>
<ul>
  <li>\(\gamma_t\) denote the global human translation at time \(t\),</li>
  <li>\(\phi_t\) the global orientation,</li>
  <li>\(\theta_t\) the local joint angles,</li>
  <li>and \(\alpha\) the global scene scale.</li>
</ul>

<p>The optimization objective is:</p>

\[\min_{\alpha, \gamma_{1:T}, \phi_{1:T}, \theta_{1:T}}
\; w_{3D} \mathcal{L}_{3D}
+ w_{2D} \mathcal{L}_{2D}
+ \mathcal{L}_{\text{smooth}}\]

<p>where:</p>
<ul>
  <li>\(\mathcal{L}_{3D}\) enforces consistency between reconstructed and lifted 3D joints,</li>
  <li>\(\mathcal{L}_{2D}\) penalizes reprojection error in image space,</li>
  <li>\(\mathcal{L}_{\text{smooth}}\) regularizes temporal motion jitter.</li>
</ul>

<p>This optimization yields <strong>world-frame human trajectories</strong> and <strong>gravity-aligned scene meshes</strong> suitable for physics simulation.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team50/mimic_pipeline.png" alt="VideoMimic Pipeline" /></p>
<p><em>Fig. 1. VideoMimic real-to-sim-to-real pipeline. Monocular videos are reconstructed into human motion and scene geometry, retargeted to a humanoid, and used to train a single context-aware control policy.</em> [1]</p>

<hr />

<h3 id="motion-retargeting-and-physics-aware-imitation">Motion Retargeting and Physics-Aware Imitation</h3>

<p>The reconstructed human motion is retargeted to a humanoid robot under kinematic and physical constraints, including joint limits, collision avoidance, and foot-contact consistency. These retargeted trajectories serve as reference motions for reinforcement learning in simulation.</p>

<p>Policy learning follows a DeepMimic-style formulation, where the robot tracks reference motion while respecting physics. The reward is defined entirely through <strong>data-driven tracking terms</strong>, avoiding handcrafted task rewards:</p>

\[r_t =
w_p \| p_t - p_t^{*} \|
+ w_q \| q_t - q_t^{*} \|
+ w_{\dot{q}} \| \dot{q}_t - \dot{q}_t^{*} \|
+ w_c \, \mathbb{I}_{\text{contact}}
- w_a \| a_t - a_{t-1} \|\]

<p>where starred quantities denote reference motion targets. An action-rate penalty discourages exploiting simulator artifacts and promotes physically plausible behavior.</p>

<p>To improve stability, policies are <strong>pretrained on motion-capture data</strong>, then fine-tuned on reconstructed video motions, mitigating noise and embodiment mismatch.</p>

<hr />

<h3 id="context-aware-control-via-policy-distillation">Context-Aware Control via Policy Distillation</h3>

<p>A central contribution of VideoMimic is the distillation of multiple motion-specific policies into a <strong>single unified controller</strong>. After imitation training, the policy is distilled using DAgger into a form that no longer depends on target joint angles.</p>

<p>At test time, the controller observes only:</p>
<ul>
  <li>Proprioceptive state,</li>
  <li>A local \(11 \times 11\) height-map centered on the torso,</li>
  <li>A desired root direction in the robot’s local frame.</li>
</ul>

<p>Formally, the deployed policy is:</p>

\[\pi(a_t \mid s_t, h_t, d_t)\]

<p>where \(s_t\) is proprioception, \(h_t\) is the terrain height-map, and \(d_t\) is the root-direction command. This enables the robot to <strong>autonomously select behaviors</strong> like walking, climbing, sitting purely from environmental context, without explicit task labels or skill switching.</p>

<p style="width: 700px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team50/mimic_policy.png" alt="Policy Training" /></p>
<p><em>Fig. 2. Scene-conditioned imitation learning and policy distillation.</em> [1]</p>

<hr />

<h2 id="generalizable-visual-representations">Generalizable Visual Representations</h2>

<p>While VideoMimic shows that robots can learn context-aware control directly from reconstructed video demonstrations, its policies remain closely tied to the specific motions and environments observed during training. A complementary direction asks whether vision can be separated from control by learning reusable visual representations that transfer efficiently across tasks, robots, and environments.</p>

<p>R3M (Reusable Representations for Robotic Manipulation) represents a clean instantiation of this idea. Rather than learning actions, rewards, or dynamics, R3M focuses exclusively on perception. It learns a frozen visual embedding from large-scale human video that can be reused across downstream robot learning problems. This separation of representation learning from control enables strong generalization while remaining easy to integrate into standard imitation or reinforcement learning pipelines.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team50/r3m_pretraining.png" alt="R3M Overview" /></p>
<p><em>Fig. 3. R3M pre-training pipeline. Visual representations are learned from large-scale egocentric human video using temporal and semantic objectives, then reused as frozen perception modules for downstream robot manipulation.</em> [2]</p>

<h3 id="problem-setup">Problem Setup</h3>

<p>R3M assumes access to a dataset of human videos paired with natural language descriptions. Each video consists of RGB frames</p>

\[[I_0, I_1, \dots I_T]\]

<p>and an associated text annotation describing the activity. The goal is to learn a single image encoder</p>

\[F_\phi : I \to z\]

<p>that maps an image \(I\) to a continuous embedding \(z\). After pre-training, \(F_\phi\) is frozen and reused across downstream tasks. Robot policies consume the visual embedding concatenated with proprioceptive state rather than raw pixels.</p>

<p>This formulation intentionally isolates representation learning from control and dynamics.</p>

<h3 id="learning-objectives">Learning Objectives</h3>

<p>R3M is trained on Ego4D, a large-scale egocentric video dataset covering diverse everyday activities. The representation is optimized using three complementary objectives that target properties useful for robotic manipulation.</p>

<h4 id="temporal-contrastive-learning">Temporal Contrastive Learning</h4>

<p>To encourage the representation to encode scene dynamics rather than static appearance, R3M uses time-contrastive learning. Frames close in time are encouraged to map to nearby embeddings, while temporally distant frames or frames from different videos are pushed apart.</p>

<p>The temporal contrastive loss has the following formulation:</p>

\[\mathcal{L}_{\text{TCN}} =
- \sum_{b \in \mathcal{B}}
\log
\frac{\exp(S(z_i^b, z_j^b))}
{\exp(S(z_i^b, z_j^b)) + \exp(S(z_i^b, z_k^b)) + \exp(S(z_i^b, z_{i'}))}\]

<p>where \(z = F_\phi(I)\), indices \(i, j, k\) correspond to frames with increasing temporal distance, \(z_{i'}\) is a negative sample from a different video, and \(S(\cdot,\cdot)\) denotes similarity computed via negative L2 distance.</p>

<p>This objective biases the embedding toward encoding how scenes evolve over time.</p>

<h4 id="videolanguage-alignment">Video–Language Alignment</h4>

<p>Temporal structure alone is insufficient. A useful representation must focus on semantically meaningful elements such as objects, interactions, and task-relevant state changes. R3M therefore aligns visual embeddings with natural language descriptions.</p>

<p>Given an initial frame \(I_0\), a future frame \(I_t\), and a language instruction \(l\), a scoring function \(G_\theta\) predicts whether the transition from \(I_0\) to \(I_t\) completes the described activity. Training uses a contrastive objective:</p>

\[\mathcal{L}_{\text{Lang}} =
- \sum_{b \in \mathcal{B}}
\log
\frac{\exp(G_\theta(z_0^b, z_t^b, l^b))}
{\exp(G_\theta(z_0^b, z_t^b, l^b)) + \exp(G_\theta(z_0^b, z_i^b, l^b)) + \exp(G_\theta(z_0', z_t', l^b))}\]

<p>This objective encourages embeddings to capture objects and relations referenced by language, which are directly relevant to manipulation.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team50/r3m_learning.png" alt="R3M Overview" /></p>
<p><em>Fig. 4. R3M pretraining. R3M is trained on Ego4D video–language pairs using temporal contrastive learning and video–language alignment to produce semantically grounded, temporally coherent visual embeddings for robotic manipulation.</em> [2]</p>

<h4 id="sparsity-and-compactness">Sparsity and Compactness</h4>

<p>R3M additionally encourages compact embeddings through explicit regularization. Sparse representations reduce sensitivity to background clutter and improve robustness in low-data imitation settings.</p>

<p>This is enforced via L1 and L2 penalties:</p>

\[\mathcal{L}_{\text{Reg}} = \lambda_3 \| z \|_1 + \lambda_4 \| z \|_2\]

<h3 id="full-training-objective">Full Training Objective</h3>

<p>The complete pre-training objective combines all components:</p>

\[\mathcal{L}(\phi, \theta) =
\mathbb{E}_{I \sim \mathcal{D}}
\left[
\lambda_1 \mathcal{L}_{\text{TCN}}
+ \lambda_2 \mathcal{L}_{\text{Lang}}
+ \lambda_3 \|F_\phi(I)\|_1
+ \lambda_4 \|F_\phi(I)\|_2
\right]\]

<p>The encoder \(F_\phi\) is implemented as a ResNet backbone and trained on Ego4D. After training, the encoder is frozen and reused without further adaptation.</p>

<h3 id="downstream-policy-learning">Downstream Policy Learning</h3>

<p>For downstream tasks, R3M embeddings are concatenated with robot proprioceptive state and used as input to a policy trained via behavior cloning:</p>

\[\mathcal{L}_{\text{BC}} = \| a_t - \pi([z_t, p_t]) \|_2^2\]

<p>where \(z_t = F_\phi(I_t)\) and \(p_t\) denotes proprioceptive features.</p>

<h3 id="empirical-results-and-implications">Empirical Results and Implications</h3>

<p>Across manipulation benchmarks such as MetaWorld, Franka Kitchen, and Adroit, R3M substantially improves data efficiency relative to training from scratch and outperforms prior visual backbones such as CLIP and ImageNet-pretrained models. These gains hold despite R3M never observing robot data during pre-training.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team50/r3m_results.png" alt="R3M Overview" /></p>
<p><em>Fig. 5. Data-efficient imitation learning. Success rates on 12 unseen manipulation tasks show that R3M consistently outperforms MoCo (PVR), CLIP, supervised ImageNet features, and training from scratch, with standard error bars reported.</em> [2]</p>

<p>In real-world experiments on a Franka Panda robot, R3M enables complex tasks such as towel folding and object placement in clutter with significantly fewer demonstrations. The results show that large-scale human video can serve as a strong perceptual prior for robotics.</p>

<p>However, the representation remains static. It encodes the current scene but does not explicitly model future evolution or action-conditioned dynamics. This limitation motivates subsequent work on predictive visual representations, such as Track2Act, which explicitly reason about how scenes change over time to support generalizable robot manipulation.</p>

<h2 id="context-expansion">Context Expansion</h2>

<p>The progression from VideoMimic to R3M reflects a shift from direct imitation toward reusable visual representations. However, both approaches primarily reason about the present: VideoMimic reconstructs a demonstrated trajectory, and R3M encodes the current scene into a static embedding. Neither explicitly models how a scene will evolve under interaction. Track2Act addresses this limitation by introducing predictive, object-centric representations that enable generalizable robot manipulation.</p>

<p>Rather than predicting actions or generating RGB video, Track2Act learns to predict future point trajectories that describe how objects move when manipulated. This abstraction captures the geometric and temporal structure necessary for manipulation while avoiding the brittleness of full video synthesis. As a result, Track2Act occupies an intermediate position between static visual embeddings and full world models: it anticipates future states without modeling pixels directly.</p>

<h3 id="predicting-object-motion-via-point-tracks">Predicting Object Motion via Point Tracks</h3>

<p>Track2Act takes as input an initial image \(I_0\), a goal image \(I_G\), and a set of two-dimensional points specified on the object in the initial frame. Given \(p\) points and a prediction horizon \(H\), the model outputs a trajectory for each point, producing a set of object motion tracks:</p>

\[\tau_{\text{obj}} =
\left\{
(x_t^i, y_t^i)
\;\middle|\;
i = 1,\ldots,p,\;
t = 1,\ldots,H
\right\}\]

<p>These tracks form a correspondence-preserving representation of how the object should move to satisfy the goal. Because supervision is obtained from passive web videos using off-the-shelf tracking algorithms, the model scales naturally with diverse, uncurated data. By predicting motion instead of appearance, the representation abstracts away texture and lighting while retaining physically meaningful dynamics.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team50/t2a_predict.png" alt="Track2Act Point Tracks" /></p>
<p><em>Fig. 6. Predicted point trajectories between an initial and goal image.</em> [3]</p>

<h3 id="from-2d-tracks-to-3d-rigid-motion">From 2D Tracks to 3D Rigid Motion</h3>

<p>To convert predicted tracks into an executable manipulation plan, Track2Act lifts the two-dimensional motion into three dimensions. Given a set of 3D object points from the first frame \(P_0^{3D}\) and camera intrinsics \(K\), the method solves for a sequence of rigid transforms \(\{T_t\}_{t=1}^H\) such that:</p>

\[K \, T_t \, P_0^{3D}
\;\approx\;
\{(x_t^i, y_t^i)\}_{i=1}^p\]

<p>This optimization can be solved using standard Perspective-n-Point solvers and is well-constrained because a single rigid transform must explain the motion of multiple points. The resulting transforms describe how the object should move in the scene, independent of any specific robot embodiment.</p>

<h3 id="open-loop-execution">Open-Loop Execution</h3>

<p>After bringing the robot end-effector into contact with the object and executing a grasp, the predicted rigid transforms are converted into an open-loop end-effector trajectory. The nominal action at time step \(t\) is defined as:</p>

\[\bar{a}_t = T_t \, e_1\]

<p>where \(e_1\) denotes the end-effector pose at the moment of grasp. This produces a complete manipulation trajectory without requiring any robot-specific training data. However, open-loop execution is sensitive to prediction errors, imperfect grasps, and unmodeled contact dynamics.</p>

<h3 id="residual-policies-for-closed-loop-correction">Residual Policies for Closed-Loop Correction</h3>

<p>To improve robustness, Track2Act introduces a residual policy that corrects the open-loop plan during execution. Instead of predicting full actions, the policy outputs a correction term added to the nominal action:</p>

\[\hat{a}_t = \bar{a}_t + \Delta a_t\]

<p>where the residual action is given by:</p>

\[\Delta a_t =
\pi_{\text{res}}
\left(
I_t,\;
G,\;
\tau_{\text{obj}},\;
[\bar{a}]_{1:H}
\right)\]

<p>The residual policy is trained using behavior cloning on a small amount of embodiment-specific data. Because it only learns to correct deviations from a predicted plan, the policy generalizes effectively across unseen objects, scenes, and task configurations.</p>

<p style="width: 800px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team50/t2a_residual_correction.png" alt="Residual Policy Execution" /></p>
<p><em>Fig. 7. Open-loop execution from predicted rigid transforms with closed-loop residual correction.</em> [3]</p>

<h2 id="conclusion">Conclusion</h2>

<p>Across these approaches, a clear progression emerges in how vision supports robot control. VideoMimic demonstrates that richly structured behaviors can be transferred directly from monocular human videos when reconstruction and physics-aware learning are tightly integrated. R3M shows that large-scale video can instead be used to pretrain perceptual representations that generalize broadly across tasks, improving data efficiency without entangling vision with action learning. Track2Act bridges these paradigms by introducing prediction as a first-class component, using object-centric motion forecasts to guide manipulation while remaining agnostic to embodiment.</p>

<p>Taken together, these methods suggest that generalization arises not from a single monolithic model, but from carefully chosen abstractions aligned with physical interaction. Future progress will likely come from unifying these abstractions into systems that reason about uncertainty, contact, and long-horizon consequences while remaining deployable on real robots.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>We thank the TAs of the Computer Vision this course for providing the class with relevant materials to supplement all of our chosen topics for the final project.</p>

<h2 id="reference">Reference</h2>

<p>[1] Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa: “Visual Imitation Enables Contextual Humanoid Control”, 2025; [https://arxiv.org/abs/2505.03729 arXiv:2505.03729].</p>

<p>[2] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta: “R3M: A Universal Visual Representation for Robot Manipulation”, 2022; [https://arxiv.org/abs/2203.12601 arXiv:2203.12601].</p>

<p>[3] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, Shubham Tulsiani: “Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation”, 2024; [https://arxiv.org/abs/2405.01527 arXiv:2405.01527].</p>

<hr />]]></content><author><name>Anirudh Kannan</name></author><summary type="html"><![CDATA[Vision-based learning has become a central paradigm for enabling robots to operate in complex, unstructured environments. Rather than relying on hand-engineered perception pipelines or task-specific supervision, recent work increasingly leverages large-scale video data to learn transferable visual representations and predictive models for control. This survey reviews a sequence of recent approaches that illustrate this progression: learning control directly from video demonstrations, pretraining universal visual representations, incorporating predictive dynamics through visual point tracking, and augmenting learning with synthetic visual data. Together, these works highlight how representation learning and prediction from video are enabling increasingly generalizable robot manipulation capabilities.]]></summary></entry><entry><title type="html">Human Pose Estimation</title><link href="/CS163-Projects-2025Fall/2025/12/12/team31-human-pose-estimation.html" rel="alternate" type="text/html" title="Human Pose Estimation" /><published>2025-12-12T00:00:00+00:00</published><updated>2025-12-12T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/12/team31-human-pose-estimation</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/12/team31-human-pose-estimation.html"><![CDATA[<blockquote>
  <p>In this paper, I will be discussing the fundamentals and workings of deep learning for human pose estimation. I believe that there has been a lot of research and breakthroughs, especially recently, on technology that relates to this, and I hope that this deep dive will bring some clarity and new information to how it works!</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#why-does-human-pose-estimation-work-now" id="markdown-toc-why-does-human-pose-estimation-work-now">Why Does Human Pose Estimation Work Now?</a></li>
  <li><a href="#problem-formulation-of-human-pose-estimation" id="markdown-toc-problem-formulation-of-human-pose-estimation">Problem Formulation of Human Pose Estimation</a>    <ul>
      <li><a href="#single-person-and-multi-person-pose-estimation" id="markdown-toc-single-person-and-multi-person-pose-estimation">Single-Person and Multi-Person Pose Estimation</a></li>
      <li><a href="#deep-learning-models-for-pose-estimation" id="markdown-toc-deep-learning-models-for-pose-estimation">Deep Learning Models for Pose Estimation</a></li>
    </ul>
  </li>
  <li><a href="#transformer-based-pose-models" id="markdown-toc-transformer-based-pose-models">Transformer-Based Pose Models</a></li>
  <li><a href="#3d-human-pose-estimation" id="markdown-toc-3d-human-pose-estimation">3D Human Pose Estimation</a></li>
  <li><a href="#implementation-example-with-mmpose" id="markdown-toc-implementation-example-with-mmpose">Implementation Example with MMPose</a></li>
  <li><a href="#applications-of-human-pose-estimation" id="markdown-toc-applications-of-human-pose-estimation">Applications of Human Pose Estimation</a></li>
  <li><a href="#related-work" id="markdown-toc-related-work">Related Work</a>    <ul>
      <li><a href="#srcnn-super-resolution-convolutional-neural-network" id="markdown-toc-srcnn-super-resolution-convolutional-neural-network">SRCNN (Super-Resolution Convolutional Neural Network)</a></li>
      <li><a href="#implementation-of-srcnn" id="markdown-toc-implementation-of-srcnn">Implementation of SRCNN</a></li>
      <li><a href="#vitpose-vision-transformer-for-pose-estimation" id="markdown-toc-vitpose-vision-transformer-for-pose-estimation">ViTPose (Vision Transformer for Pose Estimation)</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>Human pose estimation is the computational task of detecting and localizing predefined keypoints, such as joints or landmarks, on one or more objects in images or videos. These keypoints are typically represented as 2D or 3D coordinates (e.g., [x, y] or [x, y, visibility]), often accompanied by confidence scores indicating the model’s certainty for each point. By capturing the spatial arrangement of the body parts, pose estimation enables fine-grained applications such as motion analysis, gesture recognition, animation, biomechanics, surveillance, and human-computer interaction.   </p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/print.png" alt="Graphical Skeleton" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 1. Graphical Skeleton. (Image source: <a href="https://www.mdpi.com/2071-1050/15/18/13363">https://www.mdpi.com/2071-1050/15/18/13363</a>)</em></p>
</div>

<p>The field of pose estimation has grown in the past few years, driven by advances in deep learning, the availability of large annotated datasets, and the development of flexible, production toolkits. Out of all of these, MMPose stands out as an open-source and extensible framework build on PyTorch that supports a wide array of tasks. Some of these tasks include 2D multi-person human pose estimation, hand keypoint detection, face landmarks, full-body pose estimation including body, hands, face, and feet, animal pose estimations, and so much more.</p>

<p>The advantages of MMPose is its comprehensive “model zoo” that includes both accuracy-oriented and real-time lightweight architectures, pertained weights on strandard datasets, and configurable pipelines for dataset loading, data augmentations, and evaluation. This versatility makes MMPose suitable for both academic research and real-world production systems, whether the task is single-person pose detection, multi-person tracking, or whole-body landmark detection.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/map.png" alt="YOLO UMAP" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 2. UMAP of datasets with root subtraction. (Image source: <a href="https://wangzheallen.github.io/cross-dataset-generalization">https://wangzheallen.github.io/cross-dataset-generalization</a>)</em></p>
</div>

<p>Accompanying the implementation is a curated collection of seminal and research papers, datasets, benchmark tasks, and open-source implementations that cover 2D and 3D pose estimation, human mesh construction, pose-based action recognition, and video pose tracking. These sources provide researchers and engineers with a structured overview of the theoretical foundations, methodological advances, and practical tools in the domain. In combiningthe  implementation toolkit with a comprehensive research source, one obtains both the useful means to build pose-estimation systems and the theoretical grounding to understand trade-offs. In this paper, we leverage the idea that we adopt the MMPose framwork for our pose estimation tasks, while consulting the literature summarized by several resources to choose appropraite architectures, training strategies, and evaluation protocols. The goal is to demonstrate accurate pose detection in both 2D and 3D, under diverse conditions, and to assess how well modern models generalize beyond standard benchmark datasets.</p>

<h2 id="why-does-human-pose-estimation-work-now">Why Does Human Pose Estimation Work Now?</h2>
<p>Although pose estimation has been studied for decades, its recent success can largely be attributed to three converging facts. These factors are data, computation, and model design. Modern pose estimation models rely on large-scale annotated datasets such as COCO, MPII, Human3.6M, and 3DPW, which provide diverse human poses across different viewpoints, environments, and levels of occlusion. Without these datasets, learning representations of human articulation would not be possible.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/3DPose.png" alt="3D Pose Estimation" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 3. Examples of 3D pose estimation for Human3.6M. (Image source: <a href="https://www.researchgate.net/figure/Examples-of-3D-pose-estimation-for-Human36M-top-row-and-MPII-middle-and-bottom">ResearchGate</a>)</em></p>
</div>

<p>Just as important is the availability of powerful computational resources. Training deep neural networks for pose estimation involves optimizing millions of parameters and processing high-resolution feature maps. GPUs and specialized accelerators make it possible to train models efficiently and deploy them in real-time systems.</p>

<p>Finally, deep learning architectures are designed to learning spatial dependencies between joints. Unlike traditional hand-crafted approaches, deep models can automatically learn hierarchical representations that encode both local joint appearance and global body structure. This allows pose estimation systems to scale effectively as data size increases, improving performance rather than saturating.</p>

<h2 id="problem-formulation-of-human-pose-estimation">Problem Formulation of Human Pose Estimation</h2>
<p>At its core, human pose estimation can be formulated as a structured prediction problem. Given an input Image \(I\), the goal is to predict a set of keypoints:</p>

\[\mathbf{P} = \{(x_i, y_i, c_i)\}_{i=1}^{K}
])\]

<p>where \(K\) is the number of keypoints, \((x_i, y_i)\) denotes the spatial location of the \(i\) -th joint, and \(c_i\) represents either a confidence score or a visibility flag.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/Joint_heatmap.png" alt="Joint Heatmaps" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 4. Examples of generated joint heatmap, limb heatmap, and joint–limb heatmap. (Image source: <a href="https://www.researchgate.net/publication/378907335/figure/extraction-and-representation.png">ResearchGate</a>)</em></p>
</div>

<p>Most modern approaches model pose estimation as a heatmap regression problem. For each joint \(i\), the network predicts a heat map (shown  below), where each pixel value represents the probability of that joint appearing at that location. The final keypoint location is obtained by:</p>

\[(x_i, y_i) = \arg\max_{(x, y)} H_i(x, y), \quad H_i \in \mathbb{R}^{H \times W}\]

<p>This formulation is particularly effective because it preserves spatial uncertainty and allows the network to express ambiguity when joints are occluded or visually similar.</p>

<h3 id="single-person-and-multi-person-pose-estimation">Single-Person and Multi-Person Pose Estimation</h3>
<p>Single-person pose estimation assumes the presence of one dominant subject in the image. The model focuses entirely on accurately localizing all keypoints of that individual, often after a preprocessing step that crops the person from the background. This setup allows for high precision and is commonly used in controlled environments such as motion capturing or sports analysis.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/single.png" alt="YOLO UMAP" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 5. Single-person 3D HPE frameworks.  [5].</em></p>
</div>

<p>Multi-person pose estimation, on the other hand, introduces the challenge of associating detected keypoints with the correct individuals. Top-down approaches first detect bounding boxes for each person and then apply a single-person pose estimator to each crop. Bottom-up approaches detect all keypoints in the image simultaneously and then group them into individual skeletons.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/multi.jpg" alt="CMU-Pose vs SE-ResNet-OKHM-CMU-Pose Results" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 6. Results of CMU-Pose (a–c) and SE-ResNet-OKHM-CMU-Pose (Ours) (d–f). The joint points in the red circle were not recognized by CMU-Pose. (Image source: <a href="https://www.researchgate.net/figure/Single-person-3D-HPE-frameworks-a-Direct-estimation-approaches-directly-estimate-the_fig5_347881067">ResearchGate</a>)</em></p>
</div>

<p>Building on the challenges inherent in multi-person pose estimation recent research has explored architectural improvements to enhance robustness in crowded and complex scenes. Figure 18 illustrates a qualitative comparison of multi-person 2D pose estimation results from the research project Improved Multi-Person 2D Human Pose Estimation Using Attention Mechanisms and Hard Example Mining. The first row presents results produced by the baseline CMU Pose network, while the second row shows outputs from the proposed SE-ResNet-OKHM-CMU-Pose model.</p>

<p>In the first set of images, the CMU Pose network detects only 17 joint keypoints for the two individuals on the right, whereas the SE-ResNet-OKHM-CMU-Pose model successfully identifies all 18 joints. The missing joints, highlighted by red circles, demonstrate the baseline model’s difficulty in localizing harder or partially occluded joints. Similarly, in the second image set, only the proposed model correctly detects all 18 joints for the third person in the scene, while the baseline fails to fully recover the pose. In the third set of images, the left ankle of the first individual is accurately detected only by the SE-ResNet-OKHM-CMU-Pose network, further emphasizing its improved sensitivity to challenging and flexible joints.</p>

<p>These qualitative results demonstrate that incorporating attention mechanisms and online hard keypoint mining significantly improves joint localization accuracy, particularly in crowded scenes with overlapping bodies or visually ambiguous joints. By explicitly focusing on difficult examples during training, the proposed method addresses common failure cases present in the COCO dataset. Overall, these findings validate the effectiveness of the approach in improving multi-person pose estimation performance under complex real-world conditions.</p>

<h3 id="deep-learning-models-for-pose-estimation">Deep Learning Models for Pose Estimation</h3>
<p>Human pose estimation aims to infer the spatial configuration of human body parts from visual input such as images or videos. Depending on the target representation, pose estimation can be formulated as either a 2D or 3D prediction problem. In 2D pose estimation, the task is to localize body joints on the image plane, while 3D pose estimation further recovers depth information to reconstruct the full skeletal structure in three-dimensional space. As a result, human pose estimation has become a dominant and foundational research topic within the computer vision community.</p>

<p>Accurate pose estimation plays a critical role in a wide range of applications. For example, sports activity recognition relies on precise skeletal joint localization to analyze movement patterns, evaluate performance, and prevent injuries. Similarly, pose estimation enables downstream tasks such as action recognition, gesture classification, and posture assessment, although these tasks are conceptually distinct. While action recognition focuses on identifying motion patterns over time, pose estimation provides the structural representation required to support such high-level reasoning. As pose estimation models improve, they increasingly serve as a backbone for action understanding systems, suggesting that future advances in pose estimation may significantly enhance action recognition performance.</p>

<p>Convolutional neural networks form the backbone of most pose estimation systems. CNNs exploit local spatial correlations in images and progressively build higher-level features through stacked convolutional layers. Early layers capture low-level patterns such as edges, while deeper layers encode semantic concepts like limbs and body parts.</p>

<p>A typical pose estimation network predicts a stack of heatmaps using a fully convolutional architecture:</p>

\[\hat{\mathbf{H}}_k = f_\theta(\mathbf{I})\]

<p>where \(f_\theta\) is a CNN parameterized by \(theta\). The loss function is usually defined as the mean squared error between predicted and ground-truth heatmaps:</p>

\[\text{L} = \frac{1}{K} \sum_{i=1}^{K} \left\| H_i - \hat{H}_i \right\|_2^2\]

<p>Beyond research settings, human pose estimation has become indispensable in real-time systems across multiple domains. In healthcare, pose estimation supports rehabilitation monitoring, fall detection, and posture grading systems that assess whether patients perform movements correctly. In human–machine interaction, pose-based interfaces allow users to control systems using body movements rather than traditional input devices. Additional applications include surveillance, animation, virtual reality, and intelligent monitoring systems. These use cases demand models that are not only accurate but also robust, efficient, and capable of operating in unconstrained environments.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/intelligent-monitoring-system.ppm" alt="Intelligent Monitoring System" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 7. Intelligent monitoring system. (Image source: <a href="https://www.semanticscholar.org/paper/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Li-Zhou/ca97a086da0306927ee89944e58e5758d0706b2d">Semantic Scholar</a>)</em></p>
</div>

<p>To overcome the spatial resolution limitations of traditional CNNs, later architectures introduced multi-scale feature fusion and high-resolution representations. Models such as HRNet maintain high-resolution feature maps throughout the network and continuously exchange information across resolutions. This design significantly improves joint localization accuracy, especially for fine-grained body parts like wrists and ankles.</p>

<h2 id="transformer-based-pose-models">Transformer-Based Pose Models</h2>
<p>More recently, transformer-based architectures have been introduced to human pose estimation. Unlike CNNs, transformers rely on self-attention mechanisms to capture long-range dependencies and global context. This capability is particularly beneficial in scenarios involving occlusion, crowded scenes, or unusual body configurations. Transformer-based pose models, such as ViTPose, treat pose estimation as a sequence modeling problem.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/hmr2.jpg" alt="YOLO UMAP" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 8. Examples of Berkeley's HMR 2.0 system, which uses Transformers to more effectively and simply extract pose information that can be superimposed onto a CGI-based SMPL human template.  [8].</em></p>
</div>

<p>Transformers introduce self-attention mechanisms that allow each joint representation to attend to all others. This is particularly useful for modeling long-range dependencies, such as symmetric limbs or full-body constraints. Instead of relying solely on local convolutions, transformers explicitly reason about global body structure.</p>

<p>Self-attention is written as:</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d}}\right) V\]

<p>where \(Q\), \(K\), and \(V\), are query, key, and value matrices derived from feature embeddings. By inforporating attention, pose models can better handle occlusions and complex interactions between joints.</p>

<h2 id="3d-human-pose-estimation">3D Human Pose Estimation</h2>
<p>3D pose estimation extends the 2D formulation by predicting depth information for each joint and predicting the image-plan coordinates (x,y) of each joing, as well as estimating the depth \(z\), thereby reconstructing the full three-dimensinoal structure of the human body. The task is inherently ill-posed because multiple 3D poses can project to the same 2D configuration. Many approaches therefore, rely on multi-view supervision, temporal constraints, or learned priors over human anatomy.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/3D.webp" alt="3D Human Pose Estimation Baseline" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 9. A baseline model for 3D human pose estimation by Martinez et al. (Image source: <a href="https://link.springer.com/chapter/10.1007/978-3-030-98457-1_10">Springer</a>)</em></p>
</div>

<p>A common formulation predicts 3D joints (\(x_i\), \(y_i\), \(z_i\)) from 2D detections:</p>

\[P_\text{3D} = g_\phi(P_\text{2D})\]

<p>where \(g_\phi\) is a regression network. The loss is typically defined as the mean per-joint position error (MPJPE):</p>

\[\text{MPJPE} = \frac{1}{K} \sum_{i=1}^{K} \| \hat{p}^i - p^i \|_2\]

<h2 id="implementation-example-with-mmpose">Implementation Example with MMPose</h2>
<p>Below is a simplified example of running inference using a pretrained MMPose model:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from mmpose.apis import init_pose_model, inference_top_down_pose_model
from mmdet.apis import init_detector

detector = init_detector(det_config, det_checkpoint)
pose_model = init_pose_model(pose_config, pose_checkpoint)

person_results = inference_detector(detector, image)
pose_results, _ = inference_top_down_pose_model(
    pose_model,
    image,
    person_results
)
</code></pre></div></div>
<p>This pipeline demonstrates how detection and pose estimation are decoupled in a top-down framework, enabling flexible experimentation and modular design.</p>

<h2 id="applications-of-human-pose-estimation">Applications of Human Pose Estimation</h2>
<p>Human pose estimation enables a wide range of applications beyond academic benchmarks. In healthcare, pose estimation supports rehabilitation monitoring and gait analysis. In sports, it enables fine-grained motion analysis for performance optimization. In entertainment and AR/VR, pose estimation allows realistic avatar animation and immersive interaction.</p>

<p>Three-dimensional human pose estimation extends the traditional 2D pose estimation problem by predicting the spatial coordinates of body joints in three-dimensional space rather than only their image-plane locations. While 2D pose estimation outputs pixel coordinates of joints projected onto the camera sensor, 3D pose estimation seeks to recover depth information, enabling a more faithful representation of human body geometry and motion. This added dimensionality is crucial for applications that require accurate modeling of posture, joint angles, and movement dynamics, such as biomechanics analysis, robotics, animation, and clinical assessment. However, 3D human pose estimation is inherently an ill-posed problem. A fundamental challenge arises from the fact that multiple distinct 3D body configurations can produce identical 2D projections when viewed from a single camera. As a result, depth information cannot be uniquely inferred from monocular images alone without additional assumptions or constraints. This ambiguity is further exacerbated by factors such as self-occlusion, variations in body shape, loose clothing, and complex camera viewpoints.</p>

<p>To address these challenges, many approaches incorporate additional sources of information to constrain the solution space. Multi-view methods leverage synchronized images from multiple cameras to triangulate joint positions in 3D space, significantly reducing depth ambiguity and improving accuracy. While effective, these approaches often require careful calibration and controlled setups, limiting their scalability in real-world environments. Temporal models exploit motion continuity across video frames, enforcing smoothness and physical plausibility in predicted poses by assuming that human motion evolves gradually over time. Learned priors over human anatomy and kinematics further constrain predictions by embedding structural knowledge, such as limb length consistency and joint angle limits, into the model.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/multiview.jpg" alt="YOLO UMAP" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 10. The framework of deep multi-view concept learning method (DMCL).  [10].</em></p>
</div>

<p>Recent deep learning approaches have shown remarkable success by learning implicit 3D pose priors from large annotated datasets. These models can infer plausible 3D poses even from single images by exploiting statistical regularities in human motion and body structure. Nevertheless, despite substantial progress, 3D pose estimation remains an active area of research due to persistent challenges in generalization, robustness to occlusion, and performance in unconstrained, in-the-wild settings.</p>

<p>The emergence of human pose estimation algorithms represents a paradigm shift in the quantitative analysis of human movement. Powered by advances in computer vision and deep learning, modern pose estimation methods enable automatic tracking of human body joints from simple video recordings captured using widely available, low-cost devices such as smartphones, tablets, and laptop cameras. This accessibility has dramatically lowered the barriers to motion analysis, making pose-based measurement feasible in everyday environments rather than restricted laboratory settings. One of the most promising application areas is human health and performance. In clinical contexts, pose estimation allows clinicians to conduct quantitative motor assessments remotely, potentially within a patient’s home, without the need for expensive motion capture systems or wearable sensors. Similarly, researchers without access to specialized laboratory equipment can analyze movement kinematics using consumer-grade video data, while coaches and trainers can evaluate athletic performance directly on the field. These capabilities offer significant advantages in terms of cost, scalability, and ecological validity.</p>

<h2 id="related-work">Related Work</h2>
<p>Human pose estimation has evolved considerably over the last decade, largely driven by advances in deep learning architectures. In this section, we compare some of the most influential models and frameworks, including SRCNN and ViTPose, highlighting their design and limitations.</p>

<h3 id="srcnn-super-resolution-convolutional-neural-network">SRCNN (Super-Resolution Convolutional Neural Network)</h3>
<p>Single-image super-resolution (SR) is a classical and well-studied problem in computer vision that aims to recover a high-resolution image from a single low-resolution input. This task is fundamentally ill-posed, as multiple high-resolution images can correspond to the same low-resolution observation. In other words, SR is an underdetermined inverse problem, where the solution space is large and non-unique. To mitigate this ambiguity, most super-resolution approaches constrain the solution space by imposing strong prior information learned from data.</p>

<p>SRCNN introduced a key conceptual shift by showing that this traditional pipeline can be interpreted as a deep convolutional neural network, enabling end-to-end learning of the super-resolution mapping. Rather than explicitly learning dictionaries or manifolds for patch representation, SRCNN implicitly captures these priors through hidden convolutional layers. Patch extraction, non-linear mapping, and reconstruction are all reformulated as convolution operations, allowing the entire process to be optimized jointly using backpropagation.</p>

<p>Although SRCNN was originally designed for image super-resolution, its underlying convolutional architecture has inspired early approaches to pose estimation through feature extraction and heatmap regression. SRCNN consists of three layers of convolution:</p>

\[F_1(Y) = \max\left(0, W_1 * Y + B_1\right)\]

\[F_2(Y) = \max\left(0, W_2 * F_1(Y) + B_2\right)\]

\[F(Y) = W_3 * F_2(Y) + B_3\]

<p>Where \(Y\) is the input image, \(*\) denotes convolution, \(W_i\) and \(B_i\) are the weights and biases of the \(i\)-th layer, and max (0,⋅) represents the ReLU activation. SRCNN’s simplicity allows fast training and easy integration into pipelines where low-resolution keypoint heatmaps are upscaled to higher resolution for finer localization.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/SRCNN.png" alt="YOLO UMAP" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 11. Architecture of SRCNN.  [11].</em></p>
</div>

<p>While SRCNN was originally designed for image super-resolution, its principles have influenced pose estimation pipelines—particularly in heatmap-based keypoint localization. In pose estimation, models often predict low-resolution heatmaps for each joint, which must then be upsampled to higher resolutions for precise localization. SRCNN-like architectures can be used to refine or super-resolve these heatmaps, improving joint accuracy without substantially increasing computational cost. In this context, SRCNN-style models serve as post-processing or refinement modules, learning a mapping from coarse joint confidence maps to sharper, more spatially precise outputs. The ill-posed nature of heatmap super-resolution mirrors that of image SR, as multiple high-resolution joint configurations can correspond to the same low-resolution heatmap.</p>

<h3 id="implementation-of-srcnn">Implementation of SRCNN</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class SRCNN(nn.Module):
    def __init__(self):
        super(SRCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)
        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)
        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = self.conv3(x)
        return x
</code></pre></div></div>
<p>While SRCNN provides a foundation for feature extraction, it suffers in hgih complexity scenes due to its shallow structure and limited capacity to capture long range dependencies.</p>

<h3 id="vitpose-vision-transformer-for-pose-estimation">ViTPose (Vision Transformer for Pose Estimation)</h3>
<p>ViTPose is a transformer-based approach to human pose estimation that leverages the global modeling capabilities of Vision Transformers (ViTs) to address limitations inherent in convolutional neural networks. Unlike CNN-based methods, which rely on local receptive fields and hierarchical feature aggregation, ViTPose models long-range spatial dependencies directly through self-attention. This property is particularly advantageous for pose estimation in crowded or complex scenes, where joints may be spatially distant, heavily occluded, or visually ambiguous.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/assets/images/905972224/vitpose.png" alt="YOLO UMAP" style="width: 600px; max-width: 100%;" />
  <p><em>Fig 12. Architecture of the ViTPose model. It consists of a Transformer encoder and two different kinds of decoders.  [12].</em></p>
</div>

<p>In ViTPose, the input image is first divided into fixed-size patches that are linearly embedded and processed by a transformer encoder. Through multi-head self-attention, the model captures global contextual relationships among different body parts, enabling more coherent reasoning about human structure and inter-joint constraints. This global awareness allows ViTPose to better disambiguate challenging poses, such as overlapping limbs or interactions between multiple individuals, which are common failure cases for purely convolutional architectures.</p>

<p>The input image is split into patches, embedded, and passed through a series of self-attention layers:</p>

\[Z_0 = X_p + E_{\text{pos}}\]

\[Z_l' = \text{MSA}(\text{LN}(Z_{l-1})) + Z_{l-1}\]

\[Z_l = \text{MLP}(\text{LN}(Z_l')) + Z_l'\]

<p>Where \(X_p\) is the patch embedding, \(E_{\text{pos}}\) is positional encoding, MSA is multi-head self-attention, and LN is layer normalization. ViTPose produces heatmaps for keypoints using transformer output, enabling it to capture spatial and contextual dependencies across the whole image, even in crowded scenes.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
import torch.nn as nn

class PatchEmbed(nn.Module):
    def __init__(self, img_size=256, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)
        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)
        return x
</code></pre></div></div>
<p>Example of patch embedding layer:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
import torch.nn as nn

class PatchEmbed(nn.Module):
    def __init__(self, img_size=256, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        self.proj = nn.Conv2d(
            in_chans,
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )

    def forward(self, x):
        # x: (B, C, H, W)
        x = self.proj(x)                  # (B, embed_dim, H/P, W/P)
        x = x.flatten(2).transpose(1, 2)  # (B, N, embed_dim)
        return x
</code></pre></div></div>
<p>Self-attention allows every patch to attend to every other patch:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim=768, num_heads=12):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scale = self.head_dim ** -0.5

        self.qkv = nn.Linear(embed_dim, embed_dim * 3)
        self.proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(
            B, N, 3, self.num_heads, self.head_dim
        ).permute(2, 0, 3, 1, 4)

        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)

        out = (attn @ v).transpose(1, 2).reshape(B, N, C)
        out = self.proj(out)
        return out
</code></pre></div></div>
<p>In modern pose estimation pipelines, ViTPose is often integrated with convolutional components to combine the strengths of both paradigms. Convolutional backbones are frequently used for early-stage feature extraction due to their efficiency and inductive bias toward local spatial patterns, while transformer modules are employed at later stages to perform global reasoning and joint refinement. This hybrid design enables accurate, fine-grained keypoint localization while retaining the transformer’s ability to model long-range dependencies and holistic body structure. Overall, ViTPose represents a shift toward attention-based modeling in human pose estimation, demonstrating that transformers can serve as powerful alternatives—or complements—to convolutional networks. Its success highlights the importance of global context in understanding human body configurations and has influenced a growing number of transformer-based and hybrid architectures in both 2D and 3D pose estimation research.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Human pose estimation has emerged as a fundamental problem in modern computer vision, driven by deep learning, large datasets, and powerful toolkits such as MMPose. Through structured representations of the human body, pose estimation enables machines to reason about motion, posture, and interaction at a fine-grained level. As models continue to evolve toward 3D, whole-body, and real-time systems, pose estimation will remain a critical component of intelligent visual understanding.</p>

<h2 id="reference">Reference</h2>
<p>Please make sure to cite properly in your work, for example:</p>

<p>[1] Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. You only look once: Unified, real-time object detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</p>

<p>[2] Dong, C., Loy, C. C., He, K., &amp; Tang, X. Image super-resolution using deep convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(2), 295–307, 2016.
(Architecture figure source: ResearchGate)
https://www.researchgate.net/publication/378907335/figure/fig1/AS:11431281252590961@1718783506447/Architecture-of-SRCNN-SRCNN-consists-of-feature-block-extraction-and-representation.png</p>

<p>[3] Dong, C., Loy, C. C., He, K., &amp; Tang, X. Learning a deep convolutional network for image super-resolution. arXiv preprint arXiv:1501.00092, 2015.
https://arxiv.org/pdf/1501.00092v3</p>

<p>[4] Cao, Z., Hidalgo, G., Simon, T., Wei, S. E., &amp; Sheikh, Y. OpenPose: Realtime multi-person 2D pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1), 172–186, 2021.
(Heatmap visualization example)
https://www.researchgate.net/figure/The-examples-of-generated-joint-heatmap-limb-heatmap-and-joint-limb-heatmap_fig3_368320282</p>

<p>[5] Toshev, A., &amp; Szegedy, C. DeepPose: Human pose estimation via deep neural networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</p>

<p>[6] Sun, Y., Wang, W., Tang, X., &amp; Liu, X. Human pose estimation in the wild: A survey. Neurocomputing, 2021.
https://www.sciencedirect.com/science/article/pii/S0925231221004768</p>

<p>[7] Kocabas, M., Athanasiou, N., &amp; Black, M. J. HMR 2.0: Advances in human mesh recovery. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.
(Example visualization)
https://blog.metaphysic.ai/wp-content/uploads/2023/06/hmr2-examples-1024x397.jpg</p>

<p>[8] Ashfaq, N., et al. Intelligent monitoring systems using human pose estimation. Sensors, 2022.
(Figure source)
https://www.researchgate.net/profile/Niaz-Ashfaq/publication/366703501/figure/fig3/AS:11431281111105083@1672881081850/ntelligent-monitoring-system.ppm</p>

<p>[9] Kim, J., et al. Vision-based human activity recognition: A comprehensive review. Sustainability, 15(18), 13363, 2023.
https://www.mdpi.com/2071-1050/15/18/13363</p>

<p>[10] Mathis, A., et al. DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning. Nature Neuroscience, 21(9), 1281–1289, 2018.
(Application and clinical context)
https://pmc.ncbi.nlm.nih.gov/articles/PMC8588262/</p>

<p>[11] Xu, Y., Zhang, J., &amp; Zhang, Y. Vision Transformer-based pose estimation. arXiv preprint, 2022.
(ViTPose architecture visualization)
https://debuggercafe.com/wp-content/uploads/2025/02/vitpose-architecture.png</p>

<hr />]]></content><author><name>Lina Lee</name></author><summary type="html"><![CDATA[In this paper, I will be discussing the fundamentals and workings of deep learning for human pose estimation. I believe that there has been a lot of research and breakthroughs, especially recently, on technology that relates to this, and I hope that this deep dive will bring some clarity and new information to how it works!]]></summary></entry><entry><title type="html">Camera Pose Estimation</title><link href="/CS163-Projects-2025Fall/2025/12/07/team28-camera-pose-estimation.html" rel="alternate" type="text/html" title="Camera Pose Estimation" /><published>2025-12-07T00:00:00+00:00</published><updated>2025-12-07T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2025/12/07/team28-camera-pose-estimation</id><content type="html" xml:base="/CS163-Projects-2025Fall/2025/12/07/team28-camera-pose-estimation.html"><![CDATA[<blockquote>
  <p>Camera pose estimation is a fundamental Computer Vision task that aims to determine the position and orientation of a camera relative to a scene using image or video data. Our project evaluates three camera pose estimation methods, COLMAP, VGGSfM, and depth-based pose estimation with ICP.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ul>
      <li><a href="#camera-pose-estimation" id="markdown-toc-camera-pose-estimation">Camera Pose Estimation</a></li>
    </ul>
  </li>
  <li><a href="#camera-pose-estimation-methods" id="markdown-toc-camera-pose-estimation-methods">Camera Pose Estimation Methods</a>    <ul>
      <li><a href="#colmap" id="markdown-toc-colmap">COLMAP</a></li>
      <li><a href="#vggsfm-visual-geometry-grounded-deep-structure-from-motion" id="markdown-toc-vggsfm-visual-geometry-grounded-deep-structure-from-motion">VGGSfM (Visual Geometry Grounded Deep Structure From Motion)</a></li>
      <li><a href="#depth-based-estimation-with-icp-iterative-closest-point" id="markdown-toc-depth-based-estimation-with-icp-iterative-closest-point">Depth-based Estimation with ICP (Iterative Closest Point)</a></li>
    </ul>
  </li>
  <li><a href="#metrics" id="markdown-toc-metrics">Metrics</a>    <ul>
      <li><a href="#absolute-trajectory-error-ate" id="markdown-toc-absolute-trajectory-error-ate">Absolute Trajectory Error (ATE)</a></li>
      <li><a href="#relative-translation-error-rle" id="markdown-toc-relative-translation-error-rle">Relative Translation Error (RLE)</a></li>
      <li><a href="#relative-orientation-error-roe" id="markdown-toc-relative-orientation-error-roe">Relative Orientation Error (ROE)</a></li>
    </ul>
  </li>
  <li><a href="#findings-and-analysis" id="markdown-toc-findings-and-analysis">Findings and Analysis</a>    <ul>
      <li><a href="#table" id="markdown-toc-table">Table</a></li>
    </ul>
  </li>
  <li><a href="#analysis" id="markdown-toc-analysis">Analysis</a>    <ul>
      <li><a href="#absolute-trajectory-error-ate-1" id="markdown-toc-absolute-trajectory-error-ate-1">Absolute Trajectory Error (ATE)</a></li>
      <li><a href="#relative-translation-error-rte" id="markdown-toc-relative-translation-error-rte">Relative Translation Error (RTE)</a></li>
      <li><a href="#relative-orientation-error-roe-1" id="markdown-toc-relative-orientation-error-roe-1">Relative Orientation Error (ROE)</a></li>
      <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>Camera pose estimation has become a fundamental task in Computer Vision 
that aims to determine the position (translation) and orientation (rotation) of a camera relative to a scene using image or extracted video data. Accurately estimating the absolute pose of a camera has widespread applications in 3D reconstruction, world models, and augmented reality.</p>

<h3 id="camera-pose-estimation">Camera Pose Estimation</h3>
<p>For camera pose estimation, there are 3D-2D correspondences between a 3D point in the world (scene geometry) and the 2D pixel location where that point appears in the image or video frame.</p>

<p>Camera pose estimation predicts the pose of a camera with these two components:</p>
<ul>
  <li>A translation vector: which describes where the camera is in the world coordinate system</li>
  <li>A rotation: which describes the camera’s orientation relative to the world</li>
</ul>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team28/pose_estimation.PNG" alt="YOLO" /></p>
<p><em>Fig 1. Overview of camera pose estimation.</em> [8].</p>

<h2 id="camera-pose-estimation-methods">Camera Pose Estimation Methods</h2>
<p>Here is an overview of the three camera pose estimation methods that we are evaluating: COLMAP, VGGSfM, and depth-based estimation with ICP.</p>

<h3 id="colmap">COLMAP</h3>

<p style="width: 400px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team28/colmap.PNG" alt="COLMAP" /></p>
<p><em>Fig 1. Example: COLMAP used for 3D scene reconstruction</em> [2].</p>

<p>COLMAP is an end-to-end 3D reconstruction pipeline that estimates both scene geometry and camera poses from images. It uses Structure-from-Motion (SfM) to recover a sparse representation of the scene and camera poses of the input images. This is then fed into Multi-View Stereo (MVS) which recovers a dense representation of the scene.</p>

<p>The process of SfM consists of these key stages after taking in images as input:</p>
<ul>
  <li>Performing feature detection and extraction</li>
  <li>Feature matching and geometric verification</li>
  <li>Structure and motion reconstruction</li>
</ul>

<p>For the feature matching and geometric verification, we employ sequential matching, which is best for images that were acquired in sequential order, such as video data. Since frames have visual overlap, it is not required to use exhaustive matching. In this process, consecutive images and matched against each other.</p>

<p>After SfM, Multi-View Stereo (MVS) then takes that output to compute depth and/or normal information of every pixel in the image, and then it uses the depth and normal maps to create a dense point cloud of the scene. This sparse reconstruction process loads the extracted data from the database and incrementally extends the reconstruction from an initial image pair by registering new image and triangulating new points.</p>

<div style="display: flex; gap: 20px; justify-content: center;">
  <div style="text-align: center;">
    <img src="/CS163-Projects-2025Fall/assets/images/team28/colmap_demo.gif" style="width: 400px; max-width: 100%;" />
    <p><em>(a) COLMAP sparse reconstruction on freiburg1_plant</em></p>
  </div>

  <div style="text-align: center;">
    <img src="/CS163-Projects-2025Fall/assets/images/team28/colmap_demo2.gif" style="width: 400px; max-width: 100%;" />
    <p><em>(b) Alternate view of camera trajectory and sparse points</em></p>
  </div>
</div>
<p style="text-align: center;">
<em>Fig. 2. COLMAP sparse 3D points and estimated camera poses (red frustums) on the freiburg1_plant sequence</em>
</p>

<h3 id="vggsfm-visual-geometry-grounded-deep-structure-from-motion">VGGSfM (Visual Geometry Grounded Deep Structure From Motion)</h3>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team28/vggsfm.png" alt="VGGSfM" /></p>
<p><em>Fig 3. Overview of VGGSfM pipeline.</em> [9].</p>

<p>VGGSfM is a fully differentiable, learning-based SfM (Structure-from-Motion) pipeline that jointly estimates camera poses and 3D scene reconstruction. Unlike classical SfM frameworks, which uses non-differentiable components and incremental reconstruction, VGGSfM is fully differentiable, and therefore can be trained end-to-end.</p>

<p>The pipeline works by:</p>
<ul>
  <li>Extracting 2D tracks from input images</li>
  <li>Reconstructing cameras using image and track features</li>
  <li>Initializing a point cloud based on those tracks and camera parameters</li>
  <li>Applies a bundle adjustment layer for reconstruction refinement</li>
</ul>

<h3 id="depth-based-estimation-with-icp-iterative-closest-point">Depth-based Estimation with ICP (Iterative Closest Point)</h3>

<p>Unlike COLMAP and VGGSfM, this method does not directly operate on images/video data to estimate absolute camera positions. Instead, it uses a geometric approach that aligns two 3D points clouds and estimates the transformation (rotation and translation) between them. This transformation represents the relative camera motion between frames, which can be accumulated to form a camera trajectory.</p>

<p>Given the depth images from our dataset, each depth image can be projected back into a 3D point cloud using the known camera intrinsics. Iterative Closest Point (ICP) is then applied to each pair of consecutive point clouds by estimating the transformation that minimizes the spatial discrepances/sum of square errors.</p>

<p>This estimated transformation represents the relative camera motion between frames, and accumulating these relative motions forms the camera trajectory. This process is related to RGB-Dodometry, which produces a sequence of relative poses instead of globally optimized absolute reconstruction.</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team28/icp.svg" alt="YOLO" /></p>
<p><em>Fig 4. Overview of how ICP works.</em> [4].</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team28/icp2.gif" alt="YOLO" /></p>
<p><em>Fig 5. Overview of how ICP works.</em> [5].</p>

<p>Here is our code for going from the depth images to the point cloud:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def depth_to_pointcloud(depth_path, intrinsics):
    depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH)
    h, w = depth.shape
    fx, fy, cx, cy = intrinsics

    u, v = np.meshgrid(np.arange(w), np.arange(h))
    z = depth.astype(float) / 5000.0
    x = (u - cx) * z / fx
    y = (v - cy) * z / fy

    valid = z &gt; 0
    points = np.stack([x[valid], y[valid], z[valid]], axis=1)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points)
    return pcd.voxel_down_sample(0.02)
</code></pre></div></div>

<p>The transformation matrix from ICP encodes the relative camera motion between the consecutive frames, and accumulates them the estimated camera trajectory.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>reg = o3d.pipelines.registration.registration_icp(
    source_pcd, target_pcd,
    max_correspondence_distance=0.05,
    init=np.eye(4),
    estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint()
)

relative_transform = reg.transformation
</code></pre></div></div>

<h2 id="metrics">Metrics</h2>
<p>Now we will be going over the three key metrics that we used and what each of them are calculating.</p>

<h3 id="absolute-trajectory-error-ate">Absolute Trajectory Error (ATE)</h3>

<p>Absolute trajectory error (ATE) is a metric that is used to evaluate the accuracy of the estimated camera trajectory compared to the ground-truth trajectory. It measures the difference between the points of the true and estimated trajectory.</p>

<p style="width: 600px; max-width: 100%;"><img src="/CS163-Projects-2025Fall/assets/images/team28/ate.png" alt="YOLO" /></p>
<p><em>Fig 6. Example of absolute trajectory error.</em> [10].</p>

<h3 id="relative-translation-error-rle">Relative Translation Error (RLE)</h3>

<p>Relative translation error (RTE) is a metric that measures the accuracy of the frame-to-frame translational motion of the camera. It is a local motion accuracy metric that evaluates if the camera moved the correct distance and direction between consecutive frames, independent of global drift.</p>

<h3 id="relative-orientation-error-roe">Relative Orientation Error (ROE)</h3>

<p>Relative orientation error (ROE) is a local rotation accuracy metric, which checks if the camera rotated by the correct amount and in the correct direction between two consecutive frames. It looks at the relative rotation in the estimated trajectory compared to the ground-truth relative rotation.</p>

<h2 id="findings-and-analysis">Findings and Analysis</h2>

<h3 id="table">Table</h3>
<p>Here is a table comparing the three methods using the metrics above.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>ATE</th>
      <th>RTE</th>
      <th>ROE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>VGG</td>
      <td>0.0214</td>
      <td>0.00493</td>
      <td>0.3298</td>
    </tr>
    <tr>
      <td>ICP</td>
      <td>0.0842</td>
      <td>0.0172</td>
      <td>1.5116</td>
    </tr>
    <tr>
      <td>COLMAP</td>
      <td>0.0477 m</td>
      <td>0.0073 m/frame</td>
      <td>0.9050 °/frame</td>
    </tr>
  </tbody>
</table>

<h2 id="analysis">Analysis</h2>

<h3 id="absolute-trajectory-error-ate-1">Absolute Trajectory Error (ATE)</h3>
<p>ATE captures how accurately each method reconstructs the global camera trajectory, which is critical for long web videos where drift can accumulate. VGGSfM achieves the lowest ATE, showing strong global consistency due to its end-to-end optimization of camera poses and scene structure. COLMAP performs moderately well but is more sensitive to unreliable feature matches that commonly occur in web videos. ICP performs worst, as it only estimates relative motion and lacks global optimization, causing small errors to compound over time.</p>

<h3 id="relative-translation-error-rte">Relative Translation Error (RTE)</h3>
<p>RTE measures local frame-to-frame translation accuracy and reflects how well each method handles short-term camera motion. VGGSfM again performs best, suggesting that learned motion representations help stabilize local translation estimates under challenging visual conditions. COLMAP’s performance depends heavily on feature quality and overlap between frames, leading to higher error when these assumptions break down. ICP shows the largest error, likely due to depth noise and incomplete point cloud overlap between consecutive frames.</p>

<h3 id="relative-orientation-error-roe-1">Relative Orientation Error (ROE)</h3>
<p>ROE evaluates the accuracy of relative camera rotations, which is especially important in videos with abrupt or irregular motion. VGGSfM achieves the lowest rotational error, indicating stable orientation tracking across diverse scenes. COLMAP shows moderate rotational error, which can arise from weak geometric constraints in feature-based matching. ICP performs worst, as small rotational errors in each alignment step accumulate without any global correction mechanism.</p>

<h3 id="summary">Summary</h3>
<p>Overall, the results support the project’s motivation that pose estimation accuracy varies significantly across methods when applied to web videos. VGGSfM consistently performs best due to its learning-based and globally optimized design, COLMAP provides a strong but scene-sensitive classical baseline, and ICP struggles with accumulated drift over long sequences. These differences highlight the importance of choosing pose estimation methods that are well-suited to the variability and noise present in real-world video data.</p>

<h2 id="reference">Reference</h2>

<p>[1] Schönberger, J. (n.d.). COLMAP - Structure-from-Motion and Multi-View Stereo. https://demuc.de/colmap/</p>

<p>[2] Tutorial — COLMAP 3.14.0.dev0, 5b9a079a (2025-11-14) documentation. (n.d.). https://colmap.github.io/tutorial.html</p>

<p>[3] VGGSFM: Visual Geometry Grounded deep structure from motion. (n.d.). https://vggsfm.github.io/</p>

<p>[4] By Biggerj1 - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=88265436</p>

<p>[5] Jaykumaran, &amp; Jaykumaran. (2025, May 10). Iterative Closest Point (ICP) for 3D Explained with Code. LearnOpenCV – Learn OpenCV, PyTorch, Keras, Tensorflow With Code, &amp; Tutorials. https://learnopencv.com/iterative-closest-point-icp-explained/</p>

<p>[6] Computer Vision Group - Useful tools for the RGB-D benchmark. (n.d.). https://cvg.cit.tum.de/data/datasets/rgbd-dataset/tools</p>

<p>[7] System Evaluation » Filter Evaluation Metrics OpenVINS. (n.d.). https://docs.openvins.com/eval-metrics.html</p>

<p>[8] Kitani, K. &amp; Carnegie Mellon University. (n.d.). Pose estimation [Lecture notes]. In 16-385 Computer Vision. https://www.cs.cmu.edu/~16385/s17/Slides/11.3_Pose_Estimation.pdf</p>

<p>[9] Wang, J., Karaev, N., Rupprecht, C., &amp; Novotny, D. (2023, December 7). Visual geometry grounded deep structure from motion. arXiv.org. https://arxiv.org/abs/2312.04563</p>

<p>[10] Zhang, Z., Scaramuzza, D., Robotics and Perception Group, University of Zürich, &amp; University of Zürich and ETH Zürich. (2021). A tutorial on Quantitative Trajectory Evaluation for Visual(-Inertial) odometry. Robotics and Perception Group. https://www.ifi.uzh.ch/dam/jcr:89d3db14-37b1-431d-94c3-8be9f37466d3/IROS18_Zhang.pdf</p>

<hr />]]></content><author><name>Group 28</name></author><summary type="html"><![CDATA[Camera pose estimation is a fundamental Computer Vision task that aims to determine the position and orientation of a camera relative to a scene using image or video data. Our project evaluates three camera pose estimation methods, COLMAP, VGGSfM, and depth-based pose estimation with ICP.]]></summary></entry><entry><title type="html">Post Template</title><link href="/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html" rel="alternate" type="text/html" title="Post Template" /><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post</id><content type="html" xml:base="/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html"><![CDATA[<blockquote>
  <p>[Project Track: Project N] This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#main-content" id="markdown-toc-main-content">Main Content</a></li>
  <li><a href="#basic-syntax" id="markdown-toc-basic-syntax">Basic Syntax</a>    <ul>
      <li><a href="#image" id="markdown-toc-image">Image</a></li>
      <li><a href="#table" id="markdown-toc-table">Table</a></li>
      <li><a href="#code-block" id="markdown-toc-code-block">Code Block</a></li>
      <li><a href="#formula" id="markdown-toc-formula">Formula</a></li>
      <li><a href="#more-markdown-syntax" id="markdown-toc-more-markdown-syntax">More Markdown Syntax</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="main-content">Main Content</h2>
<p>Your survey starts here. You can refer to the <a href="https://github.com/lilianweng/lil-log/tree/master/_posts">source code</a> of <a href="https://lilianweng.github.io/lil-log/">lil’s blogs</a> for article structure ideas or Markdown syntax. We’ve provided a <a href="https://ucladeepvision.github.io/CS188-Projects-2022Winter/2017/06/21/an-overview-of-deep-learning.html">sample post</a> from Lilian Weng and you can find the source code <a href="https://raw.githubusercontent.com/UCLAdeepvision/CS188-Projects-2022Winter/main/_posts/2017-06-21-an-overview-of-deep-learning.md">here</a></p>

<h2 id="basic-syntax">Basic Syntax</h2>
<h3 id="image">Image</h3>
<p>Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.</p>

<p style="width: 400px; max-width: 100%;">You can add an image to your survey like this:
<img src="/CS163-Projects-2025Fall/assets/images/UCLAdeepvision/object_detection.png" alt="YOLO" /></p>
<p><em>Fig 1. YOLO: An object detection method in computer vision</em> [1].</p>

<p>Please cite the image if it is taken from other people’s work.</p>

<h3 id="table">Table</h3>
<p>Here is an example for creating tables, including alignment syntax.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">column 1</th>
      <th style="text-align: right">column 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">row1</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
    <tr>
      <td style="text-align: left">row2</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
  </tbody>
</table>

<h3 id="code-block">Code Block</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample code block
import torch
print (torch.__version__)
</code></pre></div></div>

<h3 id="formula">Formula</h3>
<p>Please use latex to generate formulas, such as:</p>

\[\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}\]

<p>or you can write in-text formula \(y = wx + b\).</p>

<h3 id="more-markdown-syntax">More Markdown Syntax</h3>
<p>You can find more Markdown syntax at <a href="https://www.markdownguide.org/basic-syntax/">this page</a>.</p>

<h2 id="reference">Reference</h2>
<p>Please make sure to cite properly in your work, for example:</p>

<p>[1] Redmon, Joseph, et al. “You only look once: Unified, real-time object detection.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>

<hr />]]></content><author><name>UCLAdeepvision</name></author><summary type="html"><![CDATA[[Project Track: Project N] This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.]]></summary></entry></feed>